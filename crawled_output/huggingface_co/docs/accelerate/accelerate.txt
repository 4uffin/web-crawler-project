Accelerate
Hugging Face
Models
Datasets
Spaces
Community
Docs
Enterprise
Pricing
Log In
Sign Up
Accelerate documentation
Accelerate
Accelerate
üè° View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersKernelsLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm
Search documentation
mainv1.10.1v1.9.0v1.8.1v1.7.0v1.6.0v1.5.2v1.4.0v1.3.0v1.2.1v1.1.0v1.0.1v0.34.2v0.33.0v0.32.0v0.31.0v0.30.1v0.29.3v0.28.0v0.27.2v0.26.1v0.25.0v0.24.0v0.23.0v0.22.0v0.21.0v0.20.3v0.19.0v0.18.0v0.17.1v0.16.0v0.15.0v0.14.0v0.13.2v0.12.0v0.11.0v0.10.0v0.9.0v0.8.0v0.7.1v0.6.0v0.5.1v0.4.0v0.3.0v0.2.1v0.1.0
EN
Getting started
ü§ó Accelerate
Installation
Quicktour
Tutorials
Overview
Add Accelerate to your code
Execution process
TPU training
Launching Accelerate scripts
Launching distributed training from Jupyter Notebooks
How to guides
Accelerate
Start Here!
Model memory estimator
Model quantization
Experiment trackers
Profiler
Checkpointing
Troubleshoot
Example Zoo
Training
Gradient accumulation
Local SGD
Low precision (FP8) training
DeepSpeed
Using multiple models with DeepSpeed
DDP Communication Hooks
Fully Sharded Data Parallel
Megatron-LM
Amazon SageMaker
Apple M1 GPUs
Intel CPU
Intel Gaudi
Compilation
Inference
Big Model Inference
Distributed inference
Concepts and fundamentals
Accelerate's internal mechanism
Loading big models into memory
Comparing performance across distributed setups
Executing and deferring jobs
Gradient synchronization
FSDP vs DeepSpeed
FSDP1 vs FSDP2
Context parallelism
Low precision training methods
Training on TPUs
Reference
Accelerator
Stateful classes
The Command Line
DataLoaders, Optimizers, Schedulers
Experiment trackers
Launchers
DeepSpeed utilities
Logging
Working with large models
Pipeline parallelism
Kwargs handlers
FP8
Utility functions and classes
Megatron-LM utilities
Fully Sharded Data Parallel utilities
Join the Hugging Face community
and get access to the augmented documentation experience
Collaborate on models, datasets and Spaces
Faster examples with accelerated inference
Switch between documentation themes
Sign Up
to get started
Accelerate Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code! In short, training and inference at scale made simple, efficient and adaptable.
Copied + from accelerate import Accelerator
+ accelerator = Accelerator()
+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(
+
model, optimizer, training_dataloader, scheduler
+ )
for batch in training_dataloader:
optimizer.zero_grad()
inputs, targets = batch
inputs = inputs.to(device)
targets = targets.to(device)
outputs = model(inputs)
loss = loss_function(outputs, targets)
+
accelerator.backward(loss)
optimizer.step()
scheduler.step() Built on torch_xla and torch.distributed, Accelerate takes care of the heavy lifting, so you don‚Äôt have to write any custom code to adapt to these platforms.
Convert existing codebases to utilize DeepSpeed, perform fully sharded data parallelism, and have automatic support for mixed-precision training! To get a better idea of this process, make sure to check out the Tutorials! This code can then be launched on any system through Accelerate‚Äôs CLI interface:
Copied accelerate launch {my_script.py} Tutorials Learn the basics and become familiar with using Accelerate. Start here if you are using Accelerate for the first time! How-to guides Practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use Accelerate to solve real-world problems. Conceptual guides High-level explanations for building a better understanding of important topics such as avoiding subtle nuances and pitfalls in distributed training and DeepSpeed. Reference Technical descriptions of how Accelerate classes and methods work. < > Update on GitHub
Installation‚Üí
Accelerate