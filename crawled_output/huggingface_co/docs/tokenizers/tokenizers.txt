Tokenizers
Hugging Face
Models
Datasets
Spaces
Community
Docs
Enterprise
Pricing
Log In
Sign Up
Tokenizers documentation
Tokenizers
Tokenizers
üè° View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersKernelsLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm
Search documentation
mainv0.20.3v0.13.4.rc2v0.10.0v0.9.4
EN
Getting started
ü§ó Tokenizers
Quicktour
Installation
The tokenization pipeline
Components
Training from memory
API
Input Sequences
Encode Inputs
Tokenizer
Encoding
Added Tokens
Models
Normalizers
Pre-tokenizers
Post-processors
Trainers
Decoders
Visualizer
Join the Hugging Face community
and get access to the augmented documentation experience
Collaborate on models, datasets and Spaces
Faster examples with accelerated inference
Switch between documentation themes
Sign Up
to get started
Tokenizers Fast State-of-the-art tokenizers, optimized for both research and
production ü§ó Tokenizers provides an
implementation of today‚Äôs most used tokenizers, with a focus on
performance and versatility. These tokenizers are also used in ü§ó Transformers.
Main features: Train new vocabularies and tokenize, using today‚Äôs most used tokenizers. Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server‚Äôs CPU. Easy to use, but also extremely versatile. Designed for both research and production. Full alignment tracking. Even with destructive normalization, it‚Äôs always possible to get the part of the original sentence that corresponds to any token. Does all the pre-processing: Truncation, Padding, add the special tokens your model needs. < > Update on GitHub
Quicktour‚Üí
Tokenizers