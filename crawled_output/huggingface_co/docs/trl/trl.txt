TRL - Transformer Reinforcement Learning
Hugging Face
Models
Datasets
Spaces
Community
Docs
Enterprise
Pricing
Log In
Sign Up
TRL documentation
TRL - Transformer Reinforcement Learning
TRL
üè° View all docsAWS Trainium & InferentiaAccelerateArgillaAutoTrainBitsandbytesChat UIDataset viewerDatasetsDeploying on AWSDiffusersDistilabelEvaluateGradioHubHub Python LibraryHuggingface.jsInference Endpoints (dedicated)Inference ProvidersKernelsLeRobotLeaderboardsLightevalMicrosoft AzureOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTrackioTransformersTransformers.jssmolagentstimm
Search documentation
mainv0.23.0v0.22.2v0.21.0v0.20.0v0.19.1v0.18.1v0.17.0v0.16.1v0.15.2v0.14.0v0.13.0v0.12.2v0.11.4v0.10.1v0.9.6v0.8.6v0.7.11v0.6.0v0.5.0v0.4.7v0.3.1v0.2.1v0.1.1
EN
Getting started
TRL
Installation
Quickstart
Conceptual Guides
Dataset Formats
Paper Index
Training FAQ
Understanding Logs
How-to guides
Command Line Interface (CLI)
Training using Jobs
Customizing the Training
Reducing Memory Usage
Speeding Up Training
Distributing Training
Using Trained Models
Integrations
DeepSpeed
Kernels Hub
Liger Kernel
PEFT
Trackio
Unsloth
vLLM
Examples
Example Overview
Community Tutorials
Sentiment Tuning
Training StackLlama
Detoxifying a Language Model
Multi Adapter RLHF
API
Trainers
AlignProp
BCO
CPO
DDPO
DPO
Online DPO
GKD
GRPO
KTO
Nash-MD
ORPO
PPO
PRM
Reward
RLOO
SFT
Iterative SFT
XPO
Model Classes
Model Utilities
Best of N Sampling
Judges
Callbacks
Data Utilities
Reward Functions
Script Utilities
Others
Join the Hugging Face community
and get access to the augmented documentation experience
Collaborate on models, datasets and Spaces
Faster examples with accelerated inference
Switch between documentation themes
Sign Up
to get started
TRL - Transformer Reinforcement Learning TRL is a full stack library where we provide a set of tools to train transformer language models with methods like Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO), Direct Preference Optimization (DPO), Reward Modeling, and more.
The library is integrated with ü§ó transformers.
üéâ What‚Äôs New ‚ú® OpenAI GPT OSS Support: TRL now fully supports fine-tuning the latest OpenAI GPT OSS models! Check out the: OpenAI Cookbook GPT OSS recipes Our example script You can also explore TRL-related models, datasets, and demos in the TRL Hugging Face organization.
Learn Learn post-training with TRL and other libraries in ü§ó smol course.
Contents The documentation is organized into the following sections: Getting Started: installation and quickstart guide. Conceptual Guides: dataset formats, training FAQ, and understanding logs. How-to Guides: reducing memory usage, speeding up training, distributing training, etc. Integrations: DeepSpeed, Liger Kernel, PEFT, etc. Examples: example overview, community tutorials, etc. API: trainers, utils, etc.
Blog posts
Published on August 7, 2025 Vision Language Model Alignment in TRL ‚ö°Ô∏è
Published on June 3, 2025 NO GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL
Published on May 25, 2025 üêØ Liger GRPO meets TRL
Published on January 28, 2025 Open-R1: a fully open reproduction of DeepSeek-R1
Published on July 10, 2024 Preference Optimization for Vision Language Models with TRL
Published on June 12, 2024 Putting RL back in RLHF
Published on September 29, 2023 Finetune Stable Diffusion Models with DDPO via TRL
Published on August 8, 2023 Fine-tune Llama 2 with DPO
Published on April 5, 2023 StackLLaMA: A hands-on guide to train LLaMA with RLHF
Published on March 9, 2023 Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU
Published on December 9, 2022 Illustrating Reinforcement Learning from Human Feedback < > Update on GitHub
Installation‚Üí
TRL - Transformer Reinforcement Learning
üéâ What‚Äôs New
Learn
Contents
Blog posts