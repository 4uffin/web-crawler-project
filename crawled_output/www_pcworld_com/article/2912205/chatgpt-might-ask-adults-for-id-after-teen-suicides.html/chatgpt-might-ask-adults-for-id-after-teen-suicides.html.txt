ChatGPT might ask adults for ID after teen suicides | PCWorld
News
All News
Accessories
Business
Entertainment
Gaming
Laptops
Mobile
Monitors
PCs & Components
Security
Smart Home Security
Software
Storage
Streaming
Wi-Fi & Networks
Windows
Best Picks
All Best Picks
Accessories
Business
Entertainment
Gaming
Laptops
Mobile
Monitors
PCs & Components
Security
Smart Home Security
Software
Storage
Streaming
Wi-Fi & Networks
Windows
Reviews
All Reviews
Accessories
Business
Entertainment
Gaming
Laptops
Mobile
Monitors
PCs & Components
Security
Smart Home Security
Software
Storage
Streaming
Wi-Fi & Networks
Windows
How-To
All How Tos
Accessories
Business
Entertainment
Gaming
Laptops
Mobile
Monitors
PCs & Components
Security
Smart Home Security
Software
Storage
Streaming
Wi-Fi & Networks
Windows
Deals
Laptops
Windows
Security
TechHive
TechHive Home
Smart Home Security
Streaming
Cord-Cutter Confidential
More
Accessories
Business
Entertainment
Gaming
Mobile
Monitors
PCs & Components
Software
Storage
Wi-Fi & Networks
Newsletters
Digital Magazine – Subscribe
Digital Magazine – Info
Smart Answers
Skip to content
Menu
NewsExpand News
All News
Accessories
Business
Entertainment
Gaming
Laptops
Mobile
Monitors
PCs & Components
Security
Smart Home Security
Software
Storage
Streaming
Wi-Fi & Networks
Windows
Best PicksExpand Best Picks
All Best Picks
Accessories
Business
Entertainment
Gaming
Laptops
Mobile
Monitors
PCs & Components
Security
Smart Home Security
Software
Storage
Streaming
Wi-Fi & Networks
Windows
ReviewsExpand Reviews
All Reviews
Accessories
Business
Entertainment
Gaming
Laptops
Mobile
Monitors
PCs & Components
Security
Smart Home Security
Software
Storage
Streaming
Wi-Fi & Networks
Windows
How-ToExpand How-To
All How Tos
Accessories
Business
Entertainment
Gaming
Laptops
Mobile
Monitors
PCs & Components
Security
Smart Home Security
Software
Storage
Streaming
Wi-Fi & Networks
Windows
Deals
Laptops
Windows
Security
TechHiveExpand TechHive
TechHive Home
Smart Home Security
Streaming
Cord-Cutter Confidential
MoreExpand More
Accessories
Business
Entertainment
Gaming
Mobile
Monitors
PCs & Components
Software
Storage
Wi-Fi & Networks
Newsletters
Digital Magazine – Subscribe
Digital Magazine – Info
Smart Answers
Hot Topics
Best laptops
Best VPN
Best antivirus
Best SSDs
Best monitors
Laptop deals
Desktop PC deals
AI PCs
Newsletters
When you purchase through links in our articles, we may earn a small commission. This doesn't affect our editorial independence.
Home
News
Software News
News
ChatGPT might ask adults for ID after teen suicides
The large language model is putting in new security features after high-profile cases of teen suicides, and may require that adults verify with scanned identification in some countries.
By Michael Crider
Staff Writer, PCWorld
Sep 17, 2025 9:18 am PDT
Image: Pixabay
Cases of “AI psychosis” are apparently on the rise, and multiple people have committed suicide after conversing with the ChatGPT large language model. That’s pretty horrible. Representatives of ChatGPT maker OpenAI are testifying before the US congress in response, and the company is announcing new methods of detecting users’ age. According to the CEO, that may include ID verification.
New age detection systems are being implemented in ChatGPT, and where the automated system can’t verify (to itself, at least) that a user is an adult, it will default to the more locked-down “under 18” experience that blocks sexual content and, “potentially involving law enforcement to ensure safety.” In a separate blog post spotted by Ars Technica, OpenAI CEO Sam Altman said that in some countries the system may also ask for an ID to verify the user’s age.
“We know this is a privacy compromise for adults but believe it is a worthy tradeoff,” Altman wrote. ChatGPT’s official policy is that users under the age of 13 are not allowed, but OpenAI claims that it’s building an experience that’s appropriate for children aged 13 to 17.
Altman also talked up the privacy angle, a serious concern in countries and states that are now requiring ID verification before adults can access pornography or other controversial content. “We are developing advanced security features to ensure your data is private, even from OpenAI employees,” Altman wrote. But exceptions will be made, apparently at the discretion of ChatGPT’s systems and OpenAI. “Potential serious misuse,” including threats to someone’s life or plans to harm others, or “a potential massive cybersecurity incident,” could be viewed and reviewed by human moderators.
As ChatGPT and other large language model services become more ubiquitous, their use has become more scrutinized from just about every angle. “AI psychosis” appears to be a phenomenon where users communicate with an LLM like a person, and the generally obliging nature of LLM design indulges them into a repeating, digressing cycle of delusion and potential harm. Last month parents of a California 16-year-old who committed suicide filed a wrongful death lawsuit against OpenAI. The teen had conversed with ChatGPT, and logs of the conversations that have been confirmed as genuine include instructions for tying a noose and what appear to be encouragement and support for the decision to kill himself.
It’s only the latest in a continuing series of mental health crises and suicides, which appear to be either directly inspired or aggravated by chatting with “artificial intelligence” products like ChatGPT and Character.AI. Both the parents in the case above and OpenAI representatives testified before the United States Senate earlier this week in an inquiry into chat systems, and the Federal Trade Commission is looking into OpenAI, Character.AI, Meta, Google, and xAI (now the official owner of X, formerly Twitter, under Elon Musk) for potential dangers of AI chatbots.
As more than a trillion US dollars are invested into various AI industries, and countries strive to make sure they have a piece of that pie, questions keep emerging about the dangers of LLM systems. But with all that money flying around, a “move fast and break things” approach seems to have been the default position up to now. Safeguards are emerging, but balancing them with user privacy won’t be easy. “We realize that these principles are in conflict and not everyone will agree with how we are resolving that conflict,” wrote Altman.
Author: Michael Crider, Staff Writer, PCWorld
Michael is a 10-year veteran of technology journalism, covering everything from Apple to ZTE. On PCWorld he's the resident keyboard nut, always using a new one for a review and building a new mechanical board or expanding his desktop "battlestation" in his off hours. Michael's previous bylines include Android Police, Digital Trends, Wired, Lifehacker, and How-To Geek, and he's covered events like CES and Mobile World Congress live. Michael lives in Pennsylvania where he's always looking forward to his next kayaking trip.
Recent stories by Michael Crider:
Google won’t be forced to sell Chrome after all
Perplexity’s AI browser is a sucker for blatant scams and prompt hijacks
Steam’s new performance monitor beats Task Manager, says Valve
PCWorld helps you navigate the PC ecosystem to find the products you want and the advice you need to get the job done.
PoliciesExpand Policies
Privacy Policy
Cookie Policy
Copyright Notice
European Privacy Settings
Member Preferences
Editorial independence
Licensing & Eprints
California: Do not Sell my Personal Info
AboutExpand About
About Us
Advertise
Ad Choice
Contact Us
Foundry Careers
GamePro
Smart Answers
PCWorld Categories
Expand submenu for PCWorld Categories
Business
Laptop
Mobile
PC Hardware
Storage
Deals
TechHive
Subscribe
Expand submenu for Subscribe
Digital Magazine - Subscribe
Digital Magazine - Info
Gift Subscription
Newsletters
Copyright © 2025 IDG Communications, Inc.
Explore the Foundry Network +
Tech AdvisorMacworld
Top Of Page
Do Not Sell or Share My Personal Information
Privacy Settings