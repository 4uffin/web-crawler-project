<![CDATA[ Latest from Tom's Hardware ]]>
https://www.tomshardware.com
All the latest content from the Tom's Hardware team
Thu, 18 Sep 2025 20:41:38 +0000
en
<![CDATA[ Teams at Nvidia and Intel have been working in secret on jointly developed processors for a year — 'The Trump administration has no involvement in this partnership at all' ]]>
<p>Intel and Nvidia have been working on the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal">jointly developed processors for client and data center products</a> for about a year now as both companies see huge opportunities behind their Intel x86 RTX SoCs and custom Nvidia x86 data center processors. Although the Nvidia CEO Jensen Huang said in a press call that the Trump administration was pleased with the collaboration between two leading U.S. companies, it had nothing to do with it.</p><h2 id="trump-not-involved-2">Trump not involved</h2><p>"The Trump administration had had no involvement in this partnership at all," said Nvidia's Huang said, during the joint press conference with Nvidia on Thursday. "They would have been very supportive, of course. Today I had the opportunity to tell Secretary [of Commerce Howard] Lutnick and he was very excited and very supportive of seeing two American technology companies working together."</p><p>The work began around a year ago, and preliminary agreements were reached by Intel's then-CEO Pat Gelsinger and Nvidia's Jensen Huang even before that. (A year ago, Joe Biden was president, though no one suggested his administration was involved, either.) Intel and Nvidia are working on custom data center CPUs that Nvidia will integrate into its AI platforms as well as GPU tiles that Intel will integrate into its upcoming client processors. In both cases CPUs and GPUs will use Nvidia's NVLink technology as an I/O interface. By now, there are three teams working together on the joint projects.</p><h2 id="the-work-is-ongoing-2">The work is ongoing</h2><p> "The two technology teams have been discussing and architecting solutions now for probably coming out to a year," said Jensen Huang, chief executive of Nvidia. "The two architecture teams… Well, it is three architecture teams are working across... the CPU architecture, as well as product lines for server and PCs. The architecture work is fairly extensive, and the teams are really excited about the new architecture. The teams have been working for a while and we are excited about the announcement today."</p><p>As Huang mentioned teams working on a CPU architecture as well as client and data center product lines, we figure out that Nvidia wants rather deep customizations of Intel's Xeons to meet the needs of its AI platforms.</p><p>The involvement of a CPU architecture team highlights the depth of the partnership between Intel and Nvidia as well as indicates that the CPU company is implementing rather deep optimizations required by next-generation AI platforms. Given Nvidia's history with Grace and Vera CPUs (custom Arm) and the high bandwidth needs of its next-gen GPUs (e.g. Rubin, Feynman, post-Feynman, etc.), it is reasonable to expect tailored cache structures, memory IO, and coherency protocols on these x86 CPUs.<br><br>Such a deep collaboration probably means that custom Intel processors will be used by Nvidia sometimes in the post-Vera Rubin platform era. We would certainly expect Nvidia's data center GPU team to work with Intel as well, but Huang never mentioned one during the call, probably because Feynman GPUs have already been defined by now.</p><p>Yet, he mentioned that there are two more teams working on product lines for server and PC products, which probably points to data center system level architecture team on Nvidia's side as well as client CPU/system level architecture team on Intel's side.</p><p>While the collaboration between Intel and Nvidia on the data center front is a multi-faceted cross-organizational effort, the timing to its fruition is tied to emergence of Intel's custom CPUs for Nvidia.</p><p>As for the joint work on client project (or projects), developing an Intel CPU with Nvidia GPU chiplet will take at least three to four years from drawing board to volume production. The collaboration requires deep integration across SoC fabrics, dimensions, performance/power consumption targets, packaging technologies (Foveros, EMIB), and software stacks from both companies. The collaboration likely began in 2024, so the first products could hit the market in late 2027 or early 2028.</p><h2 id="hundreds-of-millions-of-pcs-2">Hundreds of millions of PCs</h2><p>While we do not know for sure when Intel and Nvidia plan to come up with jointly developed products, it looks like they intend to address a broad range of applications. At least, Jensen Huang said that that the two companies plan to build CPUs that could address the vast majority of notebooks, which points to hundreds of millions of devices.</p><p>"Just the notebook market is 150 million notebooks sold each year," said Huang. "So that kind of gives you a sense of the scale of the work that we are going to do here. We are going to address the consumer market, we are going to address a vast majority of that consumer PC market, consumer PC notebook market."</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/cpus/teams-at-nvidia-and-intel-have-been-working-in-secret-on-jointly-developed-processors-for-a-year-the-trump-administration-has-no-involvement-in-this-partnership-at-all
Intel and Nvidia have quietly spent the past year co-developing custom x86 processors and SoCs for data center and client PCs with deep architectural collaboration across three joint teams.
GshJWp2g4cucSdaspaEZfa
Thu, 18 Sep 2025 20:41:38 +0000 CPUs
PC Components
ashilov@gmail.com (Anton Shilov)
Anton Shilov
intel, Nvidia
Intel, Nvidia
Intel, Nvidia
<![CDATA[ xAI's new gas turbine facility gets halfway to Elon Musk's 1-gigawatt 'AI factory' goal ]]>
<p>xAI is moving faster than anyone expected on its <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/elon-musk-xai-power-plant-overseas-to-power-1-million-gpus"><u>power strategy</u></a>. According to a new report from <a data-analytics-id="inline-link" href="https://semianalysis.com/2025/09/16/xais-colossus-2-first-gigawatt-datacenter/" target="_blank"><u><em>SemiAnalysis</em></u></a>, Elon Musk’s AI startup already has 460MW of natural gas generation either installed or under construction, split between its Memphis campus and a new site across the border in Southaven, Mississippi.</p><p>The numbers check out against state filings and local reporting. Shelby County granted xAI a permit in July for 15 stationary gas turbines at its Paul R. Lowry Road facility in Memphis after months of wrangling with environmental groups, which alleged that dozens of turbines had been running without proper approval. In Mississippi, regulators issued a 12-month authorization to operate gas turbines at 2875 Stanton Road, a property xAI acquired from Duke Energy this summer, while the company builds out a permanent plant.</p><p>Equipment lists emerging from legal disclosures align with SemiAnalysis’s reporting of 12 SMT-130 turbines, rated at roughly 16MW apiece, on the Memphis side, and seven Titan 350 units in Southaven, each capable of more than 35MW. Together, that brings xAI’s on-site capacity close to half a gigawatt, or roughly the output of a midsize utility plant, stood up in less than a year.</p><div class="see-more see-more--clipped"><blockquote class="twitter-tweet hawk-ignore" data-lang="en"><p lang="en" dir="ltr">xAI now has 460 MW of natural gas turbines installed and either operating or under construction. This includes 12 SMT-130 turbines at Colossus-1 and 7 Titan-350 turbines in Mississippi, located right across from Colossus-2. @elonmusk and @BrentM_SpaceX chose Mississippi due to… pic.twitter.com/dCTYjfK7oQ<a href="https://twitter.com/cantworkitout/status/1968420084214571512">September 17, 2025</a></p></blockquote><div class="see-more__filter"></div></div><p>A single Nvidia GB200 NVL72 rack is modeled at around 120-1302kW. Even after factoring in cooling and overhead, 460MW of generation translates to headroom for nearly 3,000 NVL72 racks, which is more than 200,000 GPUs in total. If xAI succeeds in doubling that to a full gigawatt, it would dwarf most hyperscale campuses in terms of concentrated GPU capacity.</p><p>xAI’s choice of turbine suggests a sense of urgency behind the project. Solar’s SMT-130 and Titan 350 packages are containerized modules designed for rapid deployment, essentially acting as bridge power while xAI transitions to the larger Southaven site. That speed helps sidestep the years-long queue for new grid interconnects, but also explains the geographic shuffle — Tennessee pushback slowed the Memphis approvals, so xAI pivoted to Mississippi, where regulators moved faster.</p><p>Compared with Microsoft and Amazon, which are experimenting with 100-200MW on-site projects, xAI’s goal of leaping straight to a gigawatt sets its power strategy apart from anything else. None of this means xAI is home free. The Memphis permit is already under appeal, and the Southaven authorization is temporary. Critics argue the company is prioritizing speed over compliance, which is a familiar criticism for Musk’s ventures.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em> </em><u><em>Tom's Hardware on Google News</em></u></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em> </em><u><em>add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/xai-pushes-power-strategy-towards-1gw-ai-factory
According to a new report from SemiAnalysis, Elon Musk’s AI startup already has 460MW of natural gas generation either installed or under construction.
2QteaApzqNubxMRupgrKp9
Thu, 18 Sep 2025 18:23:23 +0000 Tech Industry
lukejamesalden@gmail.com (Luke James)
Luke James
Getty Images
Elon Musk&#039;s face imposed over a phone displaying xAI&#039;s logo.
Elon Musk&#039;s face imposed over a phone displaying xAI&#039;s logo.
<![CDATA[ I managed to snag a Core i5 CPU for $10, because someone scammed Amazon out of an i7-14700 ]]>
<p>The other day, I stopped into a local Amazon returns store on my lunch break. You probably know the type: chaotically overflowing other people's returned orders, with half-open boxes scattered about in huge bins. It's like some kind of post-apocalyptic ball pit game show, where you might find something worthwhile if you wade through enough discarded shelving kits, no-name iPhone cases, and shoe insoles. This particular store is only a few months old, and I'd visited a few times without finding much (other than a pair of insoles, which I needed because I walk 9-10 miles a day).</p><p>On this trip, the first day after a weekly restock, when everything in the bins costs $10, I managed to find a roll of Creality 3D printer PLA filament. That's not a huge discount over its typical Amazon sale price, but I happened to need a new spool for my Anycubic printer, and I was a few blocks from home, so this saved me the hassle of ordering. After a few more minutes pawing through returns, I hadn't found anything else and went up to pay. But there was a line, and I wound up waiting at the corner of one of the closest bins to the register. Killing time, I idly dug around while I waited, and soon spotted the familiar blue of an Intel CPU box. I flipped it over and saw an i7-14700 sticker!<br><br>Could I really have just found the frequency-locked version of Intel's last-generation flagship for $10? And if so, had someone returned it because of the<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/raptor-lake-instability-saga-continues-as-intel-releases-0x12f-update-to-fix-vmin-instability"> <u>notorious instability issues</u></a>? Something else? I could see the CPU in its plastic clamshell through the cardboard window. The back looked OK, but the top was covered in thermal paste.</p><p>I was suspicious, but by this time, I was next in line, curious, and decided to gamble $10 on Intel. That’s maybe not the smartest wager I could make in 2025, but I was curious, and figured this would at least be more interesting than wasting money on a scratch-off ticket. I checked out with three items: the filament, the CPU, and another pair of shoe insoles – seriously, I wear those things out and can never have enough.</p><p>After paying my $32.25 after tax, I stepped outside, wishing I had a napkin to immediately wipe the thermal paste off with. Instead, I slacked my coworkers about what I had found, while I marched back to my apartment. When I got home, I immediately opened the CPU box, grabbed a paper towel, and wiped the used thermal paste off the CPU's IHS, to be met with immediate disappointment. This wasn't a 14th Gen Core i7 after all!</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:4000px;"><p class="vanilla-image-block" style="padding-top:56.30%;"><img id="ekkPrJ5PeuWEPmFpbVhhGY" name="Intel Core i5 return clamshell" alt="Tom's Hardware" src="https://cdn.mos.cms.futurecdn.net/ekkPrJ5PeuWEPmFpbVhhGY.jpg" mos="" align="middle" fullscreen="" width="4000" height="2252" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p>But it was a 13th Gen Core i5 – a<a data-analytics-id="inline-link" href="https://www.intel.com/content/www/us/en/products/sku/230580/intel-core-i513500-processor-24m-cache-up-to-4-80-ghz/specifications.html"> <u>Core i5 13500</u></a>, to be specific. Not quite one of the<a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-cpus,3986.html"><u> best CPUs</u></a>, and a generation older than what the box promised, but still a very usable chip, with 14 cores, 20 threads, and a Turbo Frequency of 4.8 GHz. It's not the fastest chip, but it currently sells for $264 at Newegg – not a bad pickup for $10. If it works, anyway.</p><p>So why was a 13th Gen Core i5 returned in a Core i7-14700 box? For those who haven't already connected the dots, it's likely that someone scammed Amazon by buying a new, higher-end chip than what they had, put the old one back in the box (helpfully obscured by thermal paste), and returned it for a refund. And Amazon, dealing as it does with millions of packages a day, seemingly accepted the return without checking that the returned product was actually what was returned, eventually selling it as part of a lot of liquidated returns.</p><p>I have no way to verify any of this, of course, but it seems the most likely scenario. And it's certainly unsurprising that Amazon would just accept a return without paying someone to open the box, wipe off the thermal paste, and confirm they had received the Core i7-14700 the customer had ordered. There's no way Amazon could continue to run its business if it had to do something like that with even half of its returns.</p><p>The only lingering question I had was whether my $10 13th Gen Core i5 CPU actually works. So I grabbed my trusty Hoto screwdriver, removed the AIO waterblock on the system that previously served as our external SSD storage testbed, and removed the 12th Gen Core i5 CPU that previously resided in the LGA 1700 socket. I then dropped my 13th Gen Core i5 into the motherboard, applied<a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/apply-thermal-paste-to-your-cpu"> <u>five small drops of thermal paste</u></a>, re-attached the cooler, and plugged the system back in.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3120px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="U76oWNfzfjLJE5jSZm7cSW" name="Intel Core i5 return in socket" alt="Tom's Hardware" src="https://cdn.mos.cms.futurecdn.net/U76oWNfzfjLJE5jSZm7cSW.jpg" mos="" align="middle" fullscreen="" width="3120" height="1755" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p>I pressed the power button and stared at the blackness of my test bench monitor for what felt like too many seconds, but eventually I saw the spinning circle and soon the familiar Windows 11 login screen. The old system booted up without an issue, and after running a few benchmarks, it looks like my $10 chip performs as expected.</p><p>Now the only question is, what should I do with it? I don't need another gaming rig – I'm writing this on an <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/amd-ryzen-9-7950x-ryzen-5-7600x-cpu-review"><u>AMD Ryzen 7950X</u></a> / <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/nvidia-geforce-rtx-4090-review"><u>Nvidia RTX 4090</u></a> PC I built back in 2023, and I already have a few other systems and CPUs for testing PC cases and accessories. Maybe I'll build a system for a family member or friend.</p><p>All I know is, while it didn't turn out to be a 14th Gen Core i7 promised on the box, I'm happy with the results of my $10 CPU gamble, and I wonder what I'll find at the returns store next week. I don't really need any more PC hardware, but if I could pass up enticing tech that I don't really need, I probably wouldn't have gotten into this crazy business in the first place.</p>
https://www.tomshardware.com/pc-components/cpus/i-managed-to-snag-a-core-i5-cpu-for-usd10-because-someone-scammed-amazon-out-of-an-i7-14700
Someone scammed Amazon out of a Core i7-14700, but I got a 13th Gen Core i5 for $10 as a result.
ec7uNwubEj6hmkGR89oy3T
Thu, 18 Sep 2025 18:23:22 +0000 CPUs
PC Components
Matt Safford
Tom&#039;s Hardware
An Intel Core i7-14700 CPU box, along with a cleaned-off version of the Core i5-13500 that was inside, next to a syringe of thermal paste
An Intel Core i7-14700 CPU box, along with a cleaned-off version of the Core i5-13500 that was inside, next to a syringe of thermal paste
<![CDATA[ Snapdragon 8 Elite Gen 5 shows up in Geekbench with a score of 3,831 — upcoming chip catches Apple's just-launched A19 Pro, beats desktop chips on single-core perf ]]>
<p>The upcoming Snapdragon 8 Elite Gen 5 was formally announced a few days ago, but now Geekbench leaks are rolling in. <br><br>The keen-eyed X leaker <a data-analytics-id="inline-link" href="https://x.com/never_released/status/1968418182961496131?s=31" target="_blank">Longhorn</a> noticed an unnamed Xiaomi 25113PN0EC device (possibly a Xiaomi 17 Pro) with the Qualcomm SoC inside, posting a <a data-analytics-id="inline-link" href="https://browser.geekbench.com/v6/cpu/13864869" target="_blank">whopping 3,831-point</a> single-threaded score, a value that should put it head-to-head with<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/apples-a19-pro-beats-ryzen-9-9950x-in-single-thread-geekbench-tests-iphone-17-pro-chip-packs-11-12-percent-cpu-performance-bump-gpu-performance-up-37-percent-over-predecessor"> Apple's A19 Pro SoC</a> inside the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/apple-debuts-a19-and-a19-pro-processors-for-iphone-17-iphone-air-and-iphone-17-pro">iPhone 17 Pro</a>.</p><p>If that figure is reflective of shipping products, that would be quite the leap for Qualcomm's chips. The company's SoCs have historically trailed Apple's designs by some margin in both performance and efficiency, so catching up would be quite the feat. The A19 Pro <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/apples-a19-pro-beats-ryzen-9-9950x-in-single-thread-geekbench-tests-iphone-17-pro-chip-packs-11-12-percent-cpu-performance-bump-gpu-performance-up-37-percent-over-predecessor">rings in at close to 3,900</a> points in Geekbench. To put this into perspective, even the mighty Ryzen 7 9800X3D and Ryzen 9 9950X3D post scores of about 3,400 and 3,500, respectively. That's by no means an ultimate measure of real-world performance, but it does display the might of contemporary Arm-based chips, at least in power-constrained scenarios.</p><p>The 3,831-point figure for the Snapdragon 8 Elite Gen 5 might sound a little too good to be true — particularly as it would mean a generational uplift of over 34% — but it is at least consistent with leaks that showed a purported Samsung handset <a data-analytics-id="inline-link" href="https://hothardware.com/news/snapdragon-8-elite-gen-2-impresses-in-early-benchmarks-running-at-474ghz" target="_blank">displaying a score of nearly 3,400</a> at only a 4 GHz boost clock speed. Per Qualcomm's recent announcement, the chip uses two performance cores and six efficiency cores, with the performance cores hitting 4.61 GHz in the standard configuration, or 4.74 GHz in a Samsung Galaxy-specific flavor. This makes 3,831 points at least plausible, as the recent score post shows 4.6 GHz for the performance cores. The new SoC is manufactured in <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/tsmcs-3nm-update-n3p-in-production-n3x-on-track.">TSMC's 3-nm N3P node</a>, an evolution of the previous N3E.</p><div ><table><caption>Geekbench scores</caption><tbody><tr><td class="firstcol " ><p>Snapdragon 8 Elite Gen 5 @ 4.6 GHz</p></td><td
><p>3,831 (unconfirmed)</p></td></tr><tr><td class="firstcol " ><p>Apple A19 Pro</p></td><td
><p>3,895 (unconfirmed)</p></td></tr><tr><td class="firstcol " ><p>Ryzen 9 9950X3D</p></td><td
><p>~3,500</p></td></tr><tr><td class="firstcol " ><p>Ryzen 7 9800X3D</p></td><td
><p>~3,400</p></td></tr><tr><td class="firstcol " ><p>Snapdragon 8 Elite</p></td><td
><p>~2,850</p></td></tr></tbody></table></div><p>That's not the only noteworthy difference, though. Longhorn points out in their X post that "SVE2 and SME say hello", implying that the new chip ought to support the newer versions of Arm's Scalable Vector Instructions and Scalable Matrix Instructions. Both
of these CPU instruction sets are called "SIMD" (Single Instruction Multiple Operation), making it easy for developers to efficiently process chunks of data at a time with few instructions.</p><p>That means that applications that can make use of those instructions should see quite a significant speed boost. The original SVE was designed for AI-related data processing, but <a data-analytics-id="inline-link" href="https://developer.arm.com/documentation/102340/0100/Introducing-SVE2" target="_blank">Arm says that SVE2</a> should cover more broad uses cases, and calls out general-purpose software, multimedia, computer vision, and in-memory databases. <a data-analytics-id="inline-link" href="https://www.geekbench.com/blog/2024/04/geekbench-63/" target="_blank">Geekbench does use SME</a> (which in turn apparently needs a subset of SVE2), so the posted scores should reflect the use of these optimizations.</p><p>By the way, if the "Gen 5" name in this report is throwing you off, know that you're not alone. Many people thought the new Snapdragon 8 Elite SoC would be called "Gen 2", but Qualcomm has decided that the "Gen" suffix now applies to its series of Snapdragon products, making this chip the fifth generation, across Snapdragon 8 Gen 1, Gen 2, Gen 3, with the original Snapdragon 8 Elite counting as "Gen 4".</p><p>Of course, consider that these recently-posted figures originate from leakers around the globe and may not reflect production silicon, clock-speed targets, or power envelopes of their final devices. Second, although Geekbench single-core results mostly track with general application performance, that may not be true of every scenario. Regardless, even if figures for production Snapdragon 8 Elite Gen 5 devices are somewhat lower, that would still be an impressive showing.</p>
https://www.tomshardware.com/phones/snapdragon-8-elite-gen-5-shows-up-in-geekbench-with-a-score-of-3-831-upcoming-chip-catches-to-apples-just-launched-a19-pro-beats-desktop-chips-on-single-core-perf
A Xiaomi device in Geekbench packing a Snapdragon 8 Elite Gen 5 chip posted a single-thread score of 3831 points.
Y5D5pgbEnMUqCNao2Qmkwk
Thu, 18 Sep 2025 18:01:41 +0000 CPUs
PC Components
Bruno Ferreira
Qualcomm
Snapdragon 8 Elite handset
Snapdragon 8 Elite handset
<![CDATA[ Silverstone's retro PC FLP02 case launches
— throw-back 5.25-inch expansion bays meet modern 360mm radiator support, likely to be $240 in the US ]]>
<p><strong>09/18/25 update: </strong><em>A few hours after initial publication, a representative from Silverstone reached out to say that the U.S. price for the FLP02 had not yet been set, but that it will likely be around
$240. We changed the headline and removed a reference to a $220 U.S. MSRP at the end of the article.</em><br><br><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/pc-cases/silverstone-reveals-the-flp02-late-80s-style-tower-pc-case-proudly-beige-but-thoroughly-modern-inside">Initially presented</a> at Computex 2025, Silverstone has launched its new FLP02 (SST-FLP02), a retro-inspired PC case designed to house modern hardware components. The FLP02 is the second old-school-looking case introduced by Silverstone, following the successful <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/pc-cases/retro-pc-case-flaunts-floppy-disk-style-bay-cover-silverstone-flp01-will-sell-for-around-usd130">FLP01</a>.</p><p>Silverstone manufactures the FLP02 using steel and plastic, and it is finished with the characteristic beige color reminiscent of the good old days. The case dimensions are 9.13 x 19.45 x 18.58 inches (232 x 494 x 472mm), a moderate size by today's standards. With a weight of 21.6 pounds (9.79 kg), the FLP02 is not overly heavy, allowing it to be conveniently placed on a desk, which is an appropriate location for display purposes.</p><p>The FLP02 accommodates mini-ATX, microATX, ATX, and SSI-CEB motherboards. Considering the dimensions of the FLP02, a mini-ATX motherboard may appear disproportionate within the case; however, individual preferences vary. The FLP02 is equipped with a total of seven expansion slots, with an additional two slots available should you choose to install the graphics card in a vertical orientation.</p><p>Regarding the dimensions of graphics cards, the maximum length is 15.2 inches (386 mm) when the front 3.5-inch/2.5-inch combo bay drive cage is not installed, and 11 inches (279 mm) when it is installed. In terms of width, the maximum is 7.4 inches (188 mm) without the GPU brace, and 6 inches with it installed. Consequently, even large graphics cards such as the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5090-review">GeForce RTX 5090</a> are compatible with the FLP02 case. The case accepts power supplies with a length of up to 9.8 inches (250mm), so even monstrous power supplies, such as <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/power-supplies/corsair-launches-gargantuan-3-000w-power-supply-for-usd599-99-comes-with-four-native-12v-2x6-600w-gpu-cables">Corsair's 3,000W unit,</a> can fit easily into the FLP02.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="LKmBRft6E6LxSVG3NhXUJ4" name="flp02-1" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/LKmBRft6E6LxSVG3NhXUJ4.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="H3TW6ari6WPFuCiA5hZcHC" name="flp02-26" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/H3TW6ari6WPFuCiA5hZcHC.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="frNc5GMW7Vd6D77aCj6WZB" name="flp02-28" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/frNc5GMW7Vd6D77aCj6WZB.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="vtUXch3ZRSyGuAynJv6tvE" name="flp02-4" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/vtUXch3ZRSyGuAynJv6tvE.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div></div></div><p>The FLP02 provides a variety of cooling options. The case comes with two front 120mm fan mounts and one rear fan mount accommodating either a 120mm or 140mm fan. Silverstone supplies three black cooling fans with the FLP02. The top fan mounts offer greater flexibility, allowing for configurations of either three 120mm fans, two 140mm fans, or two 160mm fans. Regarding radiator support, 120mm and 140mm radiators are compatible at the front, while the top accommodates radiators ranging from 120mm to 360mm in size.</p><p>If you prefer air cooling, the FLP02 has you covered as well. The case measures 9.13 inches wide, with clearance space for CPU air coolers up to a maximum height of 7.2 inches (182mm). You'll have no issues slipping something like a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/air-cooling/noctua-nh-d15-g2-review">Noctua NH-D15 G2</a> or a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/air-cooling/jiushark-jf15k-review">Jiushark JF15K </a>in the FLP02.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="fRYvybdFLw3JNUkZ9zyxVb" name="flp02-10" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/fRYvybdFLw3JNUkZ9zyxVb.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="PSVhRFUGqjMYvddtuuDnrW" name="flp02-8" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/PSVhRFUGqjMYvddtuuDnrW.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="aM2CRwaNxbt364EzWSa2Vh" name="flp02-19" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/aM2CRwaNxbt364EzWSa2Vh.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="zUBtYZjzV4MLp3TtRhahZQ" name="flp02-20" alt="Silverstone FLP02" src="https://cdn.mos.cms.futurecdn.net/zUBtYZjzV4MLp3TtRhahZQ.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1600" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Silverstone)</span></figcaption></figure></div></div></div><p>The FLP02 features three external 5.25-inch expansion bays, which are suitable for optical drives, fan controllers, USB hubs, or hot-swap bays. Regarding storage options, it provides one 3.5-inch bay and one 2.5-inch bay, each capable of housing up to two drives of their respective sizes. Additionally, there is a third bay that supports either two 3.5-inch or two 2.5-inch drives.</p><p>The front panel of the FLP02 houses a flickable power switch, a reset button, and a turbo button. There's even a special key lock that prevents accidental actuations on the power and reset buttons. Silverstone has incorporated a fan controller on the FLP02, where you can connect up to eight PWM fans. The turbo button essentially cranks these fans up to full speed. There's a cable that connects to one of your motherboard's PWM fan headers, allowing you to fine-tune the fans.</p><p>As for indicators, you'll find a green power LED, an orange turbo LED, a red hard drive activity LED, and a large digital display that shows the duty cycle for the fans that are connected to the fan controller. Front I/O ports include one USB Type-C port, two USB 3.0 Type-A ports, and a combo 3.5mm audio port.</p><p>The FLP02 will launch in the U.S. market in the fourth quarter of the year.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/pc-cases/silverstones-retro-pc-flp02-case-launches-for-usd220-throw-back-5-25-inch-expansion-bays-meet-modern-360mm-radiator-support
Silverstone has launched the FLP02, a retro-inspired PC case with high-performance cooling and expansion support.
Tr9ZTqTJxLWoGBi5qzpv5B
Thu, 18 Sep 2025 16:44:44 +0000 PC Cases
PC Components
Zhiye Liu
Silverstone
Silverstone FLP02
Silverstone FLP02
<![CDATA[ Gigabyte's upgradeable Aero X16 gaming laptop is on sale for just $1,349 on Best Buy — Get $300 off on this device featuring an RTX 5070, Ryzen 7 AI CPU, and 32 GB RAM ]]>
<p>If you're looking for a performant laptop that doesn't cost an arm and a leg, looks sleek, and has all the latest features — we've got the perfect deal for you. It's not easy to find Nvidia's 50-series machines for a low price, especially if you're after a slightly upscale device that doesn't scream "gamer." Fortunately, <a data-analytics-id="inline-link" href="https://www.bestbuy.com/product/gigabyte-aero-x16-copilot-pc-16-25601600-wqxga-amd-ryzen-al-7-350-1tb-ssd-32gb-ddr5-ram-geforce-rtx-5070-space-gray/J3GWPQCCFK/sku/6632266" target="_blank">Gigabyte's Aero X16 is on sale for just $1,349 on Best Buy</a> right now, offering you an insane $300 discount from its list price of $1,649. It's a great deal on a laptop that can work and play comfortably.</p><ul><li><a href="https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&qp=category_facet%3DLaptops%7Eabcat0502000&st=RTX+5070+" target="_blank">Check out RTX 5070 gaming laptops on Best Buy</a></li></ul><div class="product star-deal"><a data-dimension112="4bc04008-5581-4d94-9974-8d731d86ffba" data-action="Star Deal Block" data-label="Featuring an RTX 5070 with a Ryzen 7 processor and 32 GB memory, there is no task this bad boy can't handle. Whether you're a student looking for something that can game on the side, or a video editor trying to manage multiple 4K streams in one timeline, the Aero X16 is a great laptop—made even better at its sale price." data-dimension48="Featuring an RTX 5070 with a Ryzen 7 processor and 32 GB memory, there is no task this bad boy can't handle. Whether you're a student looking for something that can game on the side, or a video editor trying to manage multiple 4K streams in one timeline, the Aero X16 is a great laptop—made even better at its sale price." data-dimension25="$1349" href="https://www.bestbuy.com/product/gigabyte-aero-x16-copilot-pc-16-25601600-wqxga-amd-ryzen-al-7-350-1tb-ssd-32gb-ddr5-ram-geforce-rtx-5070-space-gray/J3GWPQCCFK/sku/6632266" target="_blank" rel="nofollow"><figure class="van-image-figure "
><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:771px;"><p class="vanilla-image-block" style="padding-top:62.91%;"><img id="Vdt5RgtC5Ucxa2RSSJ5ohb" name="Gigabyte Aero X16" caption="" alt="" src="https://cdn.mos.cms.futurecdn.net/Vdt5RgtC5Ucxa2RSSJ5ohb.jpg" mos="" align="middle" fullscreen="" width="771" height="485" attribution="" endorsement="" credit="" class=""></p></div></div></figure></a><p>Featuring an RTX 5070 with a Ryzen 7 processor and 32 GB memory, there is no task this bad boy can't handle. Whether you're a student looking for something that can game on the side, or a video editor trying to manage multiple 4K streams in one timeline, the Aero X16 is a great laptop—made even better at its sale price.<a class="view-deal button" href="https://www.bestbuy.com/product/gigabyte-aero-x16-copilot-pc-16-25601600-wqxga-amd-ryzen-al-7-350-1tb-ssd-32gb-ddr5-ram-geforce-rtx-5070-space-gray/J3GWPQCCFK/sku/6632266" target="_blank" rel="nofollow" data-dimension112="4bc04008-5581-4d94-9974-8d731d86ffba" data-action="Star Deal Block" data-label="Featuring an RTX 5070 with a Ryzen 7 processor and 32 GB memory, there is no task this bad boy can't handle. Whether you're a student looking for something that can game on the side, or a video editor trying to manage multiple 4K streams in one timeline, the Aero X16 is a great laptop—made even better at its sale price." data-dimension48="Featuring an RTX 5070 with a Ryzen 7 processor and 32 GB memory, there is no task this bad boy can't handle. Whether you're a student looking for something that can game on the side, or a video editor trying to manage multiple 4K streams in one timeline, the Aero X16 is a great laptop—made even better at its sale price." data-dimension25="$1349">View Deal</a></p></div><p>The Aero X16 we're taking a look at today is the RTX 5070 variant, and it's paired up with an 8-core Ryzen Al 7 350 processor. You get an 85W TDP on that 5070, offering you a decent balance between battery and power. More importantly, this model comes with 32 GB of DDR5 RAM — which is plenty already — but you can upgrade the vacant SODIMM slots to add even more. The same goes for the storage, you get 1TB PCIe 4.0 storage that can be expanded with the extra M.2 NVMe slot. In terms of flexibility, this is a clear standout among current-gen mainstream offerings.</p><p>As the name suggests, the Aero X16 has a gorgeous 16" screen, featuring a 165 Hz IPS panel with 100% coverage of the sRGB color space. It's no OLED, but the 16:10 aspect ratio offers a bit more screen real estate, and the 1440p resolution keeps things looking sharp no matter what. That screen is accompanied by a solid RGB-lit keyboard, trackpad, and 1080p webcam with Windows Hello. The Aero X16 is healthy in terms of connectivity, too, offering multiple USB-C and USB-A ports with fast speeds, along with HDMI 2.1, headphone jack, and a gigabit Ethernet port.</p><p><em>If you're looking for more savings, check out our </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-deals-on-tech"><em>Best PC Hardware deals</em></a><em> for a range of products, or dive deeper into our specialized </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-deals-on-ssds"><em>SSD and Storage Deals,</em></a><em> </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/best-hard-drive-deals"><em>Hard Drive Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-computer-monitor-deals"><em>Gaming Monitor Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-graphics-card-deals-now"><em>Graphics Card Deals</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-cpu-deals"><em>CPU Deals</em></a><em> pages.</em></p>
https://www.tomshardware.com/pc-components/gigabytes-ultra-upgradeable-aero-x16-gaming-laptop-is-on-sale-for-just-usd1-349-on-best-buy-get-usd300-off-on-this-device-featuring-an-rtx-5070-ryzen-7-ai-cpu-and-32-gb-ram
Gigabyte is a reliable name when it comes to PC hardware, and this laptop deal is no different. The Aero X16 features excellent specs, like an RTX 5070 and Ryzen AI 7 350 CPU, along with 32 GB of memory and 1 TB SSD — all while costing less than $1,500. It features a clean design and weighs under 2KG.
3BStm2XEwurDDwE6KnS9Vk
Thu, 18 Sep 2025 16:36:42 +0000 PC Components
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
Tom&#039;s Hardware / Gigabyte
Gigabyte Aero X16
Gigabyte Aero X16
<![CDATA[ Microsoft announces 'world's most powerful' AI data center — 315-acre site to house 'hundreds of thousands' of Nvidia GPUs and enough fiber to circle the Earth 4.5 times ]]>
<p>Microsoft is planning to bring the "world's most powerful"
AI datacenter online in early 2026, the <a data-analytics-id="inline-link" href="https://blogs.microsoft.com/blog/2025/09/18/inside-the-worlds-most-powerful-ai-datacenter/" target="_blank">company announced</a> today. The Pleasantville, Wisconsin-based datacenter,
dubbed Fairwater, is. meant specifically for training AI models as well as running large-scale models. The datacenter will be housed on 315 acres of land, with 1.2 million square feet in three buildings to house "hundreds of thousands" of Nvidia GB200 and GB300 GPUs.</p><div class="see-more see-more--clipped"><blockquote class="twitter-tweet hawk-ignore" data-lang="en"><p lang="en" dir="ltr">If intelligence is the log of compute… it starts with a lot of compute! And that’s why we’re scaling our GPU fleet faster than anyone else.Just last year, we added over 2 gigawatts of new capacity – roughly the output of 2 nuclear power plants.And today we’re going further,… pic.twitter.com/cZJ3pdN1rX<a href="https://twitter.com/cantworkitout/status/1968677244861379012">September 18, 2025</a></p></blockquote><div class="see-more__filter"></div></div><p>On X, Microsoft CEO Satya Nadella wrote that these GPUs will be "connected by enough fiber to circle the Earth 4.5 times" and said that they will deliver ten times more performance than today's fastest supercomputer. This is likely a comparison to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/musks-colossus-is-fully-operational-with-200-000-gpus-backed-by-tesla-batteries-phase-2-to-consume-300-mw-enough-to-power-300-000-homes">xAI's Colossus</a>, which uses over 200,000 GPUs and 300 megawatts of power. Microsoft didn't specify its exact number of GPUs nor the expected power consumption.<br><br>Fairwater uses closed-loop water cooling, which the company suggests will have "zero water waste," with all of the water supplied once, at construction. In fact, Microsoft says it's the second-largest water-cooled chiller plant on Earth. Hot water will be sent out to cooling fins on each side of Fairwater, and then cooled with 172 20-foot fans before being sent back in to cool the GPUs again.<br><br>The other 10% will be traditional servers using outside air for cooling, and will move to water "only during the hottest days."<br><br>In a <a data-analytics-id="inline-link" href="https://blogs.microsoft.com/on-the-issues/2025/09/18/made-in-wisconsin-the-worlds-most-powerful-ai-datacenter/">separate blog post</a>, Microsoft president Brad Smith wrote that the company is working to avoid driving up electricity costs for surrounding communities.<br><br>The construction sounds immense. Executive vice president of Cloud and AI, Scott Guthrie, wrote that the new datacenter uses "46.6 miles of deep foundation piles, 26.5 million pounds of structural steel, 120 miles of medium-voltage underground cable and 72.6 miles of mechanical piping." The datacenter's storage systems alone are "five football fields" long. <br><br>Beyond the Mount Pleasant facility, Guthrie adds that several identical Fairwater data centers are under construction elsewhere in the United States.<br></p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/tech-industry/artificial-intelligence/microsoft-announces-worlds-most-powerful-ai-data-center-315-acre-site-to-house-hundreds-of-thousands-of-nvidia-gpus-and-enough-fiber-to-circle-the-earth-4-5-times
Microsoft's new datacenter in Wisconsin will use "hundreds of thousands of NVIDIA GB200s, connected by enough fiber to circle the Earth 4.5 times," CEO Satya Nadella wrote.
GARcWuzpDDF55cKiUmcdgi
Thu, 18 Sep 2025 16:26:37 +0000 Artificial Intelligence
Tech Industry
Andrew E. Freedman
Microsoft
Microsoft Wisconsin data center
Microsoft Wisconsin data center
<![CDATA[ My favorite SSD stick, SK hynix's speedy 2TB Tube T31, is down to $118, an all-time low price ]]>
<p>My personal favorite flash drive, and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-flash-drives" target="_blank" rel="nofollow">best flash drive</a> we've tested, SK hynix's Tube T31, is down to <a data-analytics-id="inline-link" href="https://www.amazon.com/SK-hynix-1000MB-External-Compatible/dp/B0DT3NJT16/?th=1">a new low price of $118 at Amazon</a>. It's one of the fastest USB-A flash drives we've ever tested and a great choice for people looking for a convenient storage solution that is compact and plugs right into any USB Type-A port, while delivering great performance. It's technically a tiny external SSD, but the Tube T31 keeps the familiar (and handily cable-free) USB flash drive form factor.</p><p>The<a data-analytics-id="inline-link" href="https://www.amazon.com/SK-hynix-1000MB-External-Compatible/dp/B0DT3NJT16/?th=1" target="_blank" rel="nofollow"> Tube T31 2TB flash drive is 26% off, taking it down to a new low price of $118</a>. And if you want to spend less and don't need 2TB, the <a data-analytics-id="inline-link" href="https://www.amazon.com/SK-hynix-1000MB-External-Compatible/dp/B0CQZCWHTQ/?th=1">1TB Tube T31 is also down to $59, </a>although that's not a new low price.
<br><br>While some will prefer USB-C, the Tube T31's ultra-prevalent USB Type-A connector makes it easy to use with a host of everyday computing devices, as well as consoles, and it's a great backup or offline data transfer tool.</p><div class="product star-deal"><a data-dimension112="9433ea31-d205-4fb6-9253-99b12e9a2c43" data-action="Star Deal Block" data-label="A super-compact SSD on a stick, the Tube T31 brings together a 2TB drive with a USB-A 3.2 Gen 2 connector that offers speeds of up to 1,000MB/s (10Gbps). This drive is compatible with PS4, PS5, Xbox, Windows PC &amp; Mac. Perfect for your portable storage needs." data-dimension48="A super-compact SSD on a stick, the Tube T31 brings together a 2TB drive with a USB-A 3.2 Gen 2 connector that offers speeds of up to 1,000MB/s (10Gbps). This drive is compatible with PS4, PS5, Xbox, Windows PC &amp; Mac. Perfect for your portable storage needs." data-dimension25="$118" href="https://www.amazon.com/SK-hynix-1000MB-External-Compatible/dp/B0DT3NJT16/?th=1" target="_blank" rel="nofollow"><figure class="van-image-figure "
><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:209px;"><p class="vanilla-image-block" style="padding-top:208.13%;"><img id="C7F2SwynPwDj6ZBz8ucQpD" name="SK hynix Tube T31 1TB.PNG" caption="" alt="" src="https://cdn.mos.cms.futurecdn.net/C7F2SwynPwDj6ZBz8ucQpD.png" mos="" align="middle" fullscreen="" width="209" height="435" attribution="" endorsement="" credit="" class=""></p></div></div></figure></a><p>A super-compact SSD on a stick, the Tube T31 brings together a 2TB drive with a USB-A 3.2 Gen 2 connector that offers speeds of up to 1,000MB/s (10Gbps). This drive is compatible with PS4, PS5, Xbox, Windows PC & Mac. Perfect for your portable storage needs.<a class="view-deal button" href="https://www.amazon.com/SK-hynix-1000MB-External-Compatible/dp/B0DT3NJT16/?th=1" target="_blank" rel="nofollow" data-dimension112="9433ea31-d205-4fb6-9253-99b12e9a2c43" data-action="Star Deal Block" data-label="A super-compact SSD on a stick, the Tube T31 brings together a 2TB drive with a USB-A 3.2 Gen 2 connector that offers speeds of up to 1,000MB/s (10Gbps). This drive is compatible with PS4, PS5, Xbox, Windows PC &amp; Mac. Perfect for your portable storage needs." data-dimension48="A super-compact SSD on a stick, the Tube T31 brings together a 2TB drive with a USB-A 3.2 Gen 2 connector that offers speeds of up to 1,000MB/s (10Gbps). This drive is compatible with PS4, PS5, Xbox, Windows PC &amp; Mac. Perfect for your portable storage needs." data-dimension25="$118">View Deal</a></p></div><p>I <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/external-ssds/sk-hynix-tube-t31-review">reviewed the 1TB Tube T31</a> in 2024 and gave it an Editor's Choice award for its performance. It's a little bulky for a flash drive, and could block neighboring USB ports, particularly on a desktop. But most laptops should have adjacent USB-C ports spaced far enough apart. And the Tube T31 did deliver class-leading performance compared to its peers and, turning in some of the fastest transfer speeds available for a 10 Gbps USB-A drive.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3643px;"><p class="vanilla-image-block" style="padding-top:56.27%;"><img id="djePcXx4bEXoUaxS9PGfyM" name="SK hynix Tube T31 Comparison" alt="The SK hynix Tube T31 is a bit bulky compared to competing drives from Kingston and Teamgroup." src="https://cdn.mos.cms.futurecdn.net/djePcXx4bEXoUaxS9PGfyM.jpg" mos="" align="middle" fullscreen="" width="3643" height="2050" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>The SK hynix Tube T31 2TB uses its impressive performance to blur the lines between external SSDs and flash drives in terms of both speed and capacity. It uses a USB 3.2 Gen 2 Type-A connector, making it compatible with basically any recent device with a USB-A port, and utilizes the full 10 Gbps data bandwidth available, providing speeds of up to 1,000MB/s when transferring your data. <br><br><em>If you're looking for more savings, check out our </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-deals-on-tech"><em>Best PC Hardware deals</em></a><em> for a range of products, or dive deeper into our specialized </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/best-ssd-deals"><em>SSD and Storage Deals,</em></a><em> </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/hdds/best-hard-drive-deals-amazon-prime-day-2025"><em>Hard Drive Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/monitors/best-computer-monitor-deals"><u><em>Gaming Monitor Deals</em></u></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-graphics-card-deals-now"><em>Graphics Card Deals</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-cpu-deals"><em>CPU Deals</em></a><em> pages.</em></p>
https://www.tomshardware.com/pc-components/usb-flash-drives/my-favorite-ssd-stick-sk-hynixs-speedy-2tb-tube-t31-is-down-to-usd118-an-all-time-low-price
The Tube T31 is an M.2 SSD on a USB-A stick, with the best performance we've seen from a USB-A drive.
LdJsm6U8bXL9ZymMUSuqai
Thu, 18 Sep 2025 15:48:23 +0000 USB Flash Drives
PC Components
Storage
Matt Safford
Tom&#039;s Hardware
SK hynix Tube T31 in hand
SK hynix Tube T31 in hand
<![CDATA[ Valve to drop Steam support for 32-bit Windows versions next year — says it's no longer compatible with core client features, only 0.01% of players actually used it ]]>
<p>Valve is dropping support for Steam running on 32-bit versions of Windows, <a data-analytics-id="inline-link" href="https://help.steampowered.com/en/faqs/view/49A1-B944-48B8-FF00" target="_blank">starting January 1, 2026</a>. Steam has been available on Windows for more than two decades and, therefore, was built with 32-bit systems in mind. Today, every modern computer is 64-bit, with compatibility layers built in to support older 32-bit apps. So, even though 32-bit apps have carried forward, there's really no place for 32-bit operating systems anymore — which is why Valve is axing support for them.</p><p>It's important to understand the distinction between 32-bit apps and operating systems. Steam itself is 32-bit, partly because it's from that era, but mostly because it doesn't need to be updated to a 64-bit instruction set, given its lightweight nature. A lot of games on Steam are also 32-bit. None of that will be affected by the sunsetting of 32-bit Windows support, since it's only support for the operating system itself that's being phased out. Windows 10 32-bit is the only version Steam currently supports anyways, and Valve says just 0.01% of players are still using it today.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3000px;"><p class="vanilla-image-block" style="padding-top:61.70%;"><img id="wcQFM25CBLa32ErQkMP8jP" name="GettyImages-1169550" alt="Bill Gates on stage announcing Windows XP 64-bit" src="https://cdn.mos.cms.futurecdn.net/wcQFM25CBLa32ErQkMP8jP.jpg" mos="" align="middle" fullscreen="" width="3000" height="1851" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">What once was. </span><span class="credit" itemprop="copyrightHolder">(Image credit: Getty Images)</span></figcaption></figure><p>Windows 11 is exclusively 64-bit, on the other hand, and now holds more than 60% of the OS share, according to <a data-analytics-id="inline-link" href="https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam" target="_blank">Steam's August 2025 Hardware Survey</a>. Valve has made it clear that 32-bit Windows is no longer compatible with drivers and libraries required for the core features of the Steam client, rendering continued support for it unfeasible.</p><p>Come January 1st, the client itself will still work for a while, but will stop receiving security updates, and Valve won't entertain support requests for it. The company advises gamers to upgrade to 64-bit Windows to keep receiving timely updates and assistance. This move somewhat aligns with Microsoft's own plans for Windows 10, which will completely lose official support next month. Steam should still continue to run on 64-bit versions of Windows 10, however, which makes up 35% of all Steam users right now.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/video-games/pc-gaming/valve-to-drop-steam-support-for-32-bit-windows-versions-next-year-says-its-no-longer-compatible-with-core-client-features-only-0-01-percent-of-players-actually-used-it
Steam will stop supporting 32-bit versions of Windows from next year. Valve says the libraries and drivers needed for core client features are no longer compatible with 32-bit architecture, and therefore need to be left behind. Upgrade to Windows 10 64-bit, or Windows 11, to keep receiving timely updates and assistance.
b2Y5WmQSxWSDonz3va7X5R
Thu, 18 Sep 2025 15:42:37 +0000 PC Gaming
Video Games
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
Future
Steam Library shot
Steam Library shot
<![CDATA[ A wireless device exploit uncovered 11 years ago still hasn't been fixed by some manufacturers — six vendors and 24 devices found harbouring vulnerable firmware across routers, range extenders, and more ]]>
<p>NetRise has <a data-analytics-id="inline-link" href="https://www.netrise.io/hubfs/Pixie-Dust-Report.pdf" target="_blank">revealed</a> (PDF) that wireless devices from several manufacturers remain vulnerable to the Pixie Dust exploit disclosed in 2014, even though companies have had over a decade to harden their products against the well-known security flaw.</p><p>"Across six vendors, we found 24 devices, including routers, range extenders, access points, and hybrid Wi-Fi/powerline products, with firmware that was released vulnerable to Pixie Dust," NetRise said. "The oldest vulnerable firmware in the set dates to Sept. 2017, nearly three years after public disclosure of the Pixie Dust exploit. On average, vulnerable releases occurred 7.7 years after the exploit was first published."</p><p><em>SecurityWeek </em><a data-analytics-id="inline-link" href="https://www.securityweek.com/decade-old-pixie-dust-wi-fi-hack-still-impacts-many-devices/" target="_blank">reported</a> that Pixie Dust can be "exploited to obtain a router’s [Wi-Fi Protected Setup] PIN and connect to the targeted wireless network without needing its password." All someone has to do to take advantage of this exploit is make sure they're within range of the network they want to access, capture the initial WPS handshake between the network and a client device, and then crack the PIN offline.</p><p>Pixie Dust is so well-known that numerous resources use it to demonstrate introductory wireless network hacking techniques. Researchers have also developed <a data-analytics-id="inline-link" href="https://github.com/wiire-a/pixiewps" target="_blank">several</a> open source <a data-analytics-id="inline-link" href="https://github.com/t6x/reaver-wps-fork-t6x" target="_blank">tools</a> capable of exploiting Pixie Dust—one of which is <a data-analytics-id="inline-link" href="https://www.kali.org/tools/reaver/#reaver-1" target="_blank">highlighted</a> by the security-focused Kali Linux distribution—so manufacturers can't really feign ignorance about the ease with which vulnerable devices can be hacked.</p><p>An exploit this old remaining viable on dated hardware wouldn't necessarily come as a surprise; most companies release enough products each year that it would be unreasonable to expect all of them to be fully supported in perpetuity. (Even if there <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/linux/windows-10-support-is-ending-but-end-of-10-wants-you-to-switch-to-linux" target="_blank">are many people</a> who don't want to upgrade to a newer gizmo.) But that doesn't seem to be what's happening with the devices NetRise scrutinized for its report.</p><p>"Of the 24 devices, only four were ever patched, and these patches arrived late," NetRise said. "As of this writing, thirteen devices remain actively supported but unpatched. Another seven reached end of life without ever receiving fixes. In some cases, vendors described fixes vaguely in changelogs as, 'Fixed some security vulnerability,' with no acknowledgement of Pixie Dust."</p><p>This means six manufacturers released products with known vulnerabilities and, in many cases, have neglected to update the relevant firmware even though their customers have been assured the products are still being supported. Even the products that received patches did so long after the fact—NetRise said on average Pixie Dust patches arrived 9.6 years after the exploit's public disclosure.</p><p>"The Pixie Dust exploit is not an isolated case but a symptom of systemic issues in firmware supply chains, from weak cryptography and poor entropy generation to opaque vendor patch practices," NetRise said. "The lesson is clear: without consistent visibility into firmware, organizations cannot assume that old exploits are gone."</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><u><em>Tom's Hardware on Google News</em></u></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><u><em>add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/cyber-security/a-wireless-device-exploit-uncovered-11-years-ago-still-hasnt-been-fixed-by-some-manufacturers-six-vendors-and-24-devices-found-harbouring-vulnerable-firmware-across-routers-range-extenders-and-more
NetRise has revealed that wireless devices from several manufacturers remain vulnerable to the Pixie Dust exploit disclosed in 2014, even though companies have had over a decade to harden their products against the well-known security flaw.
fasgjLhZwm8j8tBJfqjovK
Thu, 18 Sep 2025 13:49:43 +0000 Cybersecurity
Tech Industry
Nathaniel Mott
Shutterstock
A broken lock on a PCB.
A broken lock on a PCB.
<![CDATA[ China foes get worse results using DeepSeek, research suggests — CrowdStrike finds nearly twice as many flaws in AI-generated code for IS, Falun Gong, Tibet, and Taiwan ]]>
<p>Research suggests that your DeepSeek AI results can be of drastically lower quality if you trigger topics that are geopolitically sensitive or banned in China. During tests undertaken by U.S. security firm CrowdStrike, it was observed that code generated for a professed Islamic State militant group computer system contained nearly twice as many flaws as it would otherwise have had. Other potential topics included: Falun Gong, Tibet, and Taiwan, according to a new <a data-analytics-id="inline-link" href="https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/" target="_blank">Washington Post</a> report.</p><p>One of the key findings, highlighted by the source, is that DeepSeek AI-generated code for a program to run an industrial control system would typically result in 22.8% of the code featuring flaws. If requested on behalf of an Islamic State project, a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/microsoft-and-open-ai-investigate-whether-deepseek-illicitly-obtained-data-from-chatgpt">DeepSeek user</a> could see that the flaw percentage rises sharply, to 42.1%.</p><p>Rather than delivering faulty code, DeepSeek would sometimes refuse to generate code for the likes of professed Islamic State backers or devotees of the spiritual movement Falun Gong. Refusals to aid those groups would occur 61% and 45% of the time, respectively. Notably, both movements are banned in China.</p><p>However, DeepSeek’s perceived reduction of the quality of code, when it is generated for such organizations and others, has surprised some. “That is something people have worried about — largely without evidence,” Helen Toner, from the Center for Security and Emerging Technology at Georgetown University, told the Washington Post.</p><p>DeepSeek’s reasons behind the downgrading of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/linux/linux-distros-ban-tainted-ai-generated-code">AI-generated code</a> for purported use in places like Tibet and Taiwan may be less clear-cut. But such code was also less flawed than that generated for the Islamic State, for example.</p><h2 id="what-is-happening-a-few-theories-2">What is happening? A few theories.</h2><p>The Washington Post has sought comment from the makers of DeepSeek regarding <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/windows/crowdstrikes-market-cap-falls-dollar125-billion-in-wake-of-global-outage">CrowdStrike</a>’s research findings, but has yet to get a response. It has a few theories about what might be happening, though…</p><p>One of the possibilities the source muses over is that sneakily producing <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/firmware-flaw-affects-numerous-generations-of-intel-cpus-uefi-code-execution-vulnerability-found-for-intel-cpus-from-14th-gen-raptor-lake-to-6th-gen-skylake-cpus">flawed code</a> is a less obvious sabotage technique, used to blunt the energies of foes. It could also provide a wider attack surface for subsequent hacking.</p><p>Another possibility is that, as the most secure code found during testing was for projects destined for American clients, DeepSeek is trying harder to penetrate this market.</p><p>The source also ponders whether code quality is impacted by its target market due to training on the existing regional material. It expects many more relevant training resources for coders working in the U.S. than in Tibet, for example.</p><p>Last but not least, perhaps DeepSeek is working ‘off its own initiative’ to supply more error-prone code to entities and regions governed by what it has learned are ‘rebels.’ All of these are mere hypotheticals, but the AI outfit is not without attachments to Beijing. In August, it was reported that <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseek-reportedly-urged-by-chinese-authorities-to-train-new-model-on-huawei-hardware-after-multiple-failures-r2-training-to-switch-back-to-nvidia-hardware-while-ascend-gpus-handle-inference">DeepSeek switched to training its models on Huawei hardware instead of Nvidia at the behest of China</a>, leading to delays caused by hardware failures.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/artificial-intelligence/china-foes-get-worse-results-using-deepseek-research-suggests-crowdstrike-finds-nearly-twice-as-many-flaws-in-ai-generated-code-for-is-falun-gong-tibet-and-taiwan
Research suggests that your DeepSeek AI results can be of drastically lower quality if you trigger China’s geopolitically sensitive tripwires.
idLipufznFPQ8EcLWgYK8d
Thu, 18 Sep 2025 12:51:06 +0000 Artificial Intelligence
Tech Industry
Mark Tyson
Getty / Herstockart
Deepseek logo on an iPhone
Deepseek logo on an iPhone
<![CDATA[ Huawei reveals long-range Ascend chip roadmap — three-year plan includes ambitious provision for in-house HBM with up to 1.6 TB/s bandwidth ]]>
<p>Huawei’s AI silicon roadmap is no longer a state secret. Speaking at the Huawei Connect conference on September 18, rotating chairman Xu Zhijun outlined the company’s first official long-range <a data-analytics-id="inline-link" href="https://www.reuters.com/business/media-telecom/chinas-huawei-hypes-up-chip-computing-power-plans-fresh-challenge-nvidia-2025-09-18">Ascend chip strategy</a>, with four new parts scheduled across the next three years: Ascend 950PR and 950DT in early 2026, followed by Ascend 960 and 970 in 2027 and 2028, respectively.</p><p>Huawei says its upcoming 950PR chip will ship in Q1 next year with in-house HBM designed to compete with the likes of SK hynix and Samsung. That’s a pretty bold claim considering HBM supply and factors like packaging and bandwidth efficiency have arguably become the single biggest constraint on AI accelerator performance at scale.</p><p>According to Huawei, the 950PR will feature 128GB of its in-house HBM delivering up to 1.6 TB/s of bandwidth, while the 950DT increases those figures to 144GB and 4 TB/s, but Huawei hasn’t disclosed how its in-house HBM is manufactured, what packaging is used, or which foundry is producing the chip itself.</p><p>Under U.S. sanction rules, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/taiwan-bans-chip-exports-to-huawei-smic-ban-comes-after-huawei-tricked-tsmc-into-making-one-million-ai-processors-despite-us-restrictions">Huawei is barred from accessing TSMC’s advanced nodes</a> and CoWoS packaging lines, both of which Nvidia uses to stack HBM around its top-end Hopper and Blackwell GPUs. If Huawei is working with SMIC or other domestic fabs, yields and bandwidth may prove to be hugely limiting factors.</p><p>That hasn’t stopped the company from talking scale, though. Alongside its chip roadmap, Huawei teased new so-called “supernodes” that will house thousands of Ascend chips. The Atlas 950 and 960 systems are positioned as next-gen AI compute clusters that, on paper, rival Nvidia’s GB200 NVL72 configurations in deployment scale, with up to 15,488 Ascend accelerators in a single system. Huawei says Atlas 950 will debut in Q4 this year.</p><p>But big numbers don’t necessarily translate into performance. Nvidia’s big advantage isn’t just its silicon but NVLink and a tightly optimized software stack that keeps its clusters saturated across large model workloads. To challenge that, Huawei is going to need more than a boastful chip roadmap — a roadmap that has landed conveniently alongside demands from the Chinese government to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/china-mandates-domestic-firms-source-50-percent-of-chips-from-chinese-producers-beijing-continues-to-squeeze-companies-over-reliance-on-foreign-semiconductors">produce more domestic silicon</a> and a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/china-bans-its-biggest-tech-companies-from-acquiring-nvidia-chips-says-report-beijing-claims-its-homegrown-ai-processors-now-match-h20-and-rtx-pro-6000d">ban on procuring Nvidia parts</a>.</p><p>Huawei will need a proven end-to-end platform that can match Nvidia in training, efficiency, and model throughput for its roadmap to succeed. Right now, it doesn’t, and plans alone don’t break bottlenecks.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/semiconductors/huawei-unveils-ascend-roadmap-backed-by-in-house-hbm
Speaking at the Huawei Connect conference on September 18, rotating chairman Xu Zhijun outlined the company’s first official long-range Ascend chip strategy.
zZAnWKX8VdPmt3aqKAK95Q
Thu, 18 Sep 2025 12:33:07 +0000 Semiconductors
Tech Industry
Manufacturing
lukejamesalden@gmail.com (Luke James)
Luke James
Shutterstock
Huawei
Huawei
<![CDATA[ Save $700 on the LG Ultragear 39-Inch OLED curved ultrawide monitor — now available at its cheapest price of $899, with a massive 44% off ]]>
<p>A premium ultrawide monitor not only gives you more screen real estate but also makes multitasking and productivity easier without switching between multiple displays. Additionally, it enhances your gaming or movie-watching experience with immersive visuals. One such offering is the 39-inch LG Ultragear OLED, which is currently on sale and is available at its <a data-analytics-id="inline-link" href="https://www.amazon.com/dp/B0F15C7JL2?th=1" target="_blank">lowest price ever on Amazon</a>.</p><p>Launched earlier this year in April, the LG Ultragear 39GX90SA is currently priced at $899, which is a significant saving considering its launch price of $1,599. While the monitor has seen price drops in recent months, this is the lowest we’ve ever seen it go.</p><ul><li><a href="https://www.amazon.com/dp/B0F15C7JL2?th=1">Check out this deal on Amazon</a></li></ul><div class="product star-deal"><a data-dimension112="4bc04008-5581-4d94-9974-8d731d86ffba" data-action="Star Deal Block" data-label="All-time low price The LG Ultragear 39GX90SA OLED curved ultrawide monitor is packed with features making it suitable for gaming, everyday productivity, and binge watching movies or TV shows." data-dimension48="All-time low price The LG Ultragear 39GX90SA OLED curved ultrawide monitor is packed with features making it suitable for gaming, everyday productivity, and binge watching movies or TV shows." data-dimension25="$899.99" href="https://www.amazon.com/dp/B0F15C7JL2?th=1" target="_blank" rel="nofollow"><figure class="van-image-figure "
><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1143px;"><p class="vanilla-image-block" style="padding-top:69.29%;"><img id="ssx8ttEFP2QKfp3xYctoMU" name="LG-39GX90SA Ultrawide OLED monitor" caption="" alt="" src="https://cdn.mos.cms.futurecdn.net/ssx8ttEFP2QKfp3xYctoMU.jpg" mos="" align="middle" fullscreen="" width="1143" height="792" attribution="" endorsement="" credit="" class=""></p></div></div></figure></a><p><em>All-time low price </em></p><p>The LG Ultragear 39GX90SA OLED curved ultrawide monitor is packed with features making it suitable for gaming, everyday productivity, and binge watching movies or TV shows. <a class="view-deal button" href="https://www.amazon.com/dp/B0F15C7JL2?th=1" target="_blank" rel="nofollow" data-dimension112="4bc04008-5581-4d94-9974-8d731d86ffba" data-action="Star Deal Block" data-label="All-time low price The LG Ultragear 39GX90SA OLED curved ultrawide monitor is packed with features making it suitable for gaming, everyday productivity, and binge watching movies or TV shows." data-dimension48="All-time low price The LG Ultragear 39GX90SA OLED curved ultrawide monitor is packed with features making it suitable for gaming, everyday productivity, and binge watching movies or TV shows." data-dimension25="$899.99">View Deal</a></p></div><p>The monitor features a 39-inch curved WOLED panel built around an 800R curvature with a resolution of 3440 x 1440. According to LG, the monitor offers an ultra-fast response time of 0.03ms (grey-to-grey) along with a 240 Hz refresh rate. The panel is rated to offer up to 1300 nits peak brightness and has been tuned to deliver standard luminosity of 275 nits in SDR at a 100% APL (Average Picture Level). It also supports 10-bit colour depth and a wide colour gamut covering 98.5% DCI-P3. Additionally, it is VESA DisplayHDR 400 True Black certified for an enhanced HDR experience along with high contrast and deep blacks.</p><p>The LG Ultragear 39 OLED also doubles as an entertainment hub, as it comes with LG’s WebOS interface, which is usually seen on its smart TV range. You get access to video streaming apps like Netflix, Prime Video, Hulu, Disney+, over 300 free LG Channels, as well as access to cloud gaming services like Nvidia GeForce Now, Amazon Luna, and Blacknut.</p><p>As for ports, the monitor comes with a USB Type-C with DisplayPort Alt mode and 65W power delivery, DisplayPort 1.4, two HDMI 2., two USB Type-A, Ethernet jack, and a 3.5mm headphone jack. The monitor also comes with built-in stereo speakers, each rated at 7W. For the ones who care about their gamer aesthetics, the
LG Ultragear 39GX90SA-W also features RGB lighting at the back in a hexagonal pattern.</p><p><em>If you're looking for more savings, check out our </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-deals-on-tech"><em>Best PC Hardware deals</em></a><em> for a range of products, or dive deeper into our specialized </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-deals-on-ssds"><em>SSD and Storage Deals,</em></a><em> </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/best-hard-drive-deals"><em>Hard Drive Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-computer-monitor-deals"><em>Gaming Monitor Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-graphics-card-deals-now"><em>Graphics Card Deals</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-cpu-deals"><em>CPU Deals</em></a><em> pages.</em></p>
https://www.tomshardware.com/pc-components/save-usd700-on-the-lg-ultragear-39-inch-oled-curved-ultrawide-monitor-now-available-at-its-cheapest-price-of-usd899-with-a-massive-44-percent-off
The 39-inch curved WOLED monitor is great for gamers and comes with smart TV features courtesy of LG's WebOS interface.
riVG6jcgn8gJiuKW9f3DWh
Thu, 18 Sep 2025 12:00:07 +0000 PC Components
editors@tomshardware.com (Kunal Khullar)
Kunal Khullar
Tom&#039;s Hardware / LG
LG monitor
LG monitor
<![CDATA[ Kioxia Exceria Plus G4 2TB SSD Review: A safe but unexceptional drive ]]>
<p>The Kioxia Exceria Plus G4 is a solid all-around SSD. Kioxia is probably best known for its OEM and enterprise drives, and to some extent, its Exceria line of consumer drives has flown under the radar. This is a misjustice because these drives have a solid reputation for reliability, with generally few downsides with the hardware. Performance and power efficiency are acceptable to good, and there’s no unusual switching of the type of flash, unlike what we see with some other vendors. You can pick up one of these, including the Plus G4, and expect a straightforward experience. What’s not to love?</p><p>If there’s a cost for this experience, it’s probably found in the limited capacity range, middle-of-the-road performance, and, to some extent, availability and pricing concerns. This isn’t the drive for maximum performance or power efficiency; it’s not going to solve your needs for a super small drive or a large drive, and it might not always be a sensible option economically. But there are sales, and perhaps more importantly, some regions of the world have fewer reliable drive choices, and Kioxia’s Exceria drives might be more competitive. The Plus G4, in particular, also demonstrates what’s good about this class of drives – they can be used for any purpose while delivering a decent experience.</p><p>This makes it a safe drive to pick up if you’re just trying to put the last-minute final touches on a build. Maybe you’re not sure what to get, or maybe this drive catches your eye on a sale. Whatever the case, its greatest strength is that you can buy it without worry. Peace of mind is a value of its own. Kioxia’s SSDs are not fancy, and that, in our opinion, is to their benefit. We believe Crucial has a stronger hold in this market segment with the P510, but the Plus G4 is a good alternative, and it surpasses the P510 in enough areas to remain competitive.</p><h2 id="kioxia-exceria-plus-g4-specifications-2">Kioxia Exceria Plus G4 Specifications</h2><div ><table><thead><tr><th class="firstcol " ><p>Product</p></th><th
><p>1TB</p></th><th
><p>2TB</p></th></tr></thead><tbody><tr><td class="firstcol " ><p><strong>Pricing</strong></p></td><td
><p><a href="https://www.amazon.com/KIOXIA-EXCERIA-PLUS-NVMe-Gen5/dp/B0DW52LDPD">$142.99</a></p></td><td
><p><a href="https://www.amazon.com/KIOXIA-EXCERIA-PLUS-NVMe-Gen5/dp/B0DW52LDPD">$209.99</a></p></td></tr><tr><td class="firstcol " ><p><strong>Form Factor</strong></p></td><td
><p>M.2 2280</p></td><td
><p>M.2 2280</p></td></tr><tr><td class="firstcol " ><p><strong>Interface / Protocol</strong></p></td><td
><p>PCIe 5.0 x4
</p><p>NVMe 2.0c</p></td><td
><p>PCIe 5.0 x4
</p><p>NVMe 2.0c</p></td></tr><tr><td class="firstcol " ><p><strong>Controller</strong></p></td><td
><p>Phison E31T</p></td><td
><p>Phison E31T</p></td></tr><tr><td class="firstcol " ><p><strong>DRAM</strong></p></td><td
><p>N/A (HMB)</p></td><td
><p>N/A (HMB)</p></td></tr><tr><td class="firstcol " ><p><strong>Flash Memory</strong></p></td><td
><p>Kioxia 218-Layer (BiCS8) TLC</p></td><td
><p>Kioxia 218-Layer (BiCS8) TLC</p></td></tr><tr><td class="firstcol " ><p><strong>Sequential Read</strong></p></td><td
><p>10,000 MB/s</p></td><td
><p>10,000 MB/s</p></td></tr><tr><td class="firstcol " ><p><strong>Sequential Write</strong></p></td><td
><p>7,900 MB/s</p></td><td
><p>8,200 MB/s</p></td></tr><tr><td class="firstcol " ><p><strong>Random Read (IOPS)</strong></p></td><td
><p>1,300K</p></td><td
><p>1,300K</p></td></tr><tr><td class="firstcol " ><p><strong>Random Write (IOPS)</strong></p></td><td
><p>1,400K</p></td><td
><p>1,400K</p></td></tr><tr><td class="firstcol " ><p><strong>Security</strong></p></td><td
><p>N/A</p></td><td
><p>N/A</p></td></tr><tr><td class="firstcol " ><p><strong>Endurance (TBW)</strong></p></td><td
><p>600TB</p></td><td
><p>1,200TB</p></td></tr><tr><td class="firstcol " ><p><strong>Part Number</strong></p></td><td
><p>LVD10Z001TG8</p></td><td
><p>LVD10Z002TG8</p></td></tr><tr><td class="firstcol " ><p><strong>Warranty</strong></p></td><td
><p>5-Year</p></td><td
><p>5-Year</p></td></tr></tbody></table></div><p>The Kioxia Exceria Plus G4 is only available in 1TB and 2TB capacities, which might sound crazy to some. There is a market for smaller drives, especially 512GB, and larger drives of 4TB or more. However, the statistics don’t lie – 1TB and 2TB remain the most popular capacities, and these provide plenty of space for most people. Crucial’s <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/the-crucial-p510-2tb-ssd-review"><u>P510</u></a> has the same capacities on offer for good reason. The Exceria Plus G4 directly competes with that drive, so we can’t act too surprised by this turn of events. With costs getting tighter in the NAND flash and SSD storage markets, it’s safer to focus on high-volume SKUs.</p><p>Currently, the Exceria Plus G4, or Plus G4 for short, sells at $142.99 and $209.99 on Amazon. This is way too high for the 1TB, and the 2TB is more expensive than the competition, including the P510. However, the Plus G4 is likely to be more widely available in other regions and probably at a more competitive price. The drive has modest performance levels of up to 10,000 / 8,200 MB/s for sequential reads and writes with up to 1,300 K / 1,400K random read and write IOPS. The warranty is standard at five years, with up to 600TB of write endurance per TB of capacity</p><h2 id="kioxia-exceria-plus-g4-software-and-accessories-2">Kioxia Exceria Plus G4 Software and Accessories</h2><p>Kioxia offers its <a data-analytics-id="inline-link" href="https://apac.kioxia.com/en-apac/personal/software/ssd-utility.html"><u>SSD Utility management software</u></a> for its SSDs. This is an SSD toolbox that gives a health summary of the drive and also lets you monitor the SSD in real time. The application also helps with firmware updates, password protection, and enables functions such as secure erase. The program works for Windows 10 and up, and it works on all of Kioxia’s recent SSDs.</p><p>It’s nice to see such software being offered for what are essentially client or OEM drives, but Kioxia has been pushing deeper into the retail space with its Plus line of drives. Most users are on Windows, and the software covers the most common functions, so it’s a respectable attempt.</p><h2 id="kioxia-exceria-plus-g4-a-closer-look-2">Kioxia Exceria Plus G4: A Closer Look</h2><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="3WY3PhbjGr95kiogiATJvH" name="02" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/3WY3PhbjGr95kiogiATJvH.jpg" mos="" link="" align="" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="Xm22ZjL2ZyeRrCKsLqMCmH" name="03" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/Xm22ZjL2ZyeRrCKsLqMCmH.jpg" mos="" link="" align="" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>Even without removing the top label, which Kioxia states does help spread and dissipate heat, we can tell this is a Phison drive from the power management IC (PMIC). The label states it’s a PCIe 5.0 drive, so that narrows things down considerably.</p><p>This is a single-sided drive at all capacities, so the back has no components. The drive lists its Physical Security ID (PSID), which means this drive supports TCG Opal. Phison controllers can and do support hardware encryption, but that feature likely adds to the manufacturer’s cost, which is why many brands omit SED support. It’s more common as at least an option on client and OEM drives due to business requirements, although typically you will have two separate SKUs for it, as is common with Micron drives.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="Ew2mHdrtJNsEniGiPNfisS" name="04" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/Ew2mHdrtJNsEniGiPNfisS.jpg" mos="" link="" align="" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="CLN6oGrmmpt34pZPPzTRsS" name="06" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/CLN6oGrmmpt34pZPPzTRsS.jpg" mos="" link="" align="" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="8M5RSBpTSth2mfJM8pPzeS" name="05" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/8M5RSBpTSth2mfJM8pPzeS.jpg" mos="" link="" align="" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The Plus G4 uses Phison’s E31T controller. For more details, see our <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/phison-e31t-es-2tb-review"><u>E31T preview</u></a>. For a brief reminder of the <a data-analytics-id="inline-link" href="https://www.phison.com/images/products_datasheet/ProductBrochure_Consumer_PS5031-E31T_040825.pdf"><u>specifications</u></a>: this is a four-channel DRAM-less PCIe 5.0 solution that can support an I/O rate up to 3,600 MT/s per NAND channel. 3,600 MT/s can be assumed to be 3,600 MB/s as consumer NAND flash transfers 8 bits, or one byte, at a time. There is overhead on these transfers, so the maximum bandwidth will be less than the channel count times this number. In this case, Phison rates the E31T for up to 10,600 MB/s. Eventually, these drives will be surpassed by 4,800 MT/s capable controllers and flash.</p><p>With four chip enable (CE) signals per channel, this drive can normally handle up to 32 dies without a problem, which is 4TB with current flash, although 2Tb dies would bump this up to 8TB. This is unlikely to ever happen, and for the most part, we’ve really only seen drives up to 2TB with this controller. This is something that frustrates the storage community, who see no reason for 4TB not to be commonplace. The reality is that it’s not cost-effective to run fast flash at that capacity when most of the market is selling smaller drives. 4TB drives can be found in other segments – on higher-end drives or with YMTC flash – and sticking to 2TB or less streamlines the production process for the third-party vendors. Flash availability is also a direct influence here, as QLC is in high demand in the enterprise.</p><p>Kioxia has an easier time with that since it manufactures its own flash. The NAND flash packages here are labeled TH58LKT3T488A8S, which are still using the old Toshiba coding. We already know these are 1TB packages with eight 1Tb dies each, using 218-Layer BiCS8 TLC flash. We’ve only had good results with this flash – see the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/sandisk-wd-black-sn8100-2tb-ssd-review"><u>Sandisk WD_Black SN8100</u></a> – and it’s proven to be power-efficient with low 4K latency. Combined with Kioxia’s usually reliable custom firmware, we expect only good things.</p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-ssds,3891.html"><strong>Best SSDs</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-external-hard-drive-ssd,5987.html"><strong>Best External SSDs</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ssd-for-steam-deck"><strong>Best SSD for the Steam Deck</strong></a></p><h2 id="comparison-products-2">Comparison Products</h2><p>The Kioxia Exceria Plus G4 is directly positioned to compete with mid-range PCIe 5.0 SSDs, so we arranged our test pool accordingly. Some popular ones include the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/corsair-mp700-elite-ssd-review"><u>Corsair MP700 Elite</u></a>, which uses the same hardware, and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/the-crucial-p510-2tb-ssd-review"><u>Crucial P510</u></a>, which has the same controller but Micron rather than Kioxia TLC flash. This performance line was once fulfilled by early E26-based drives like the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/corsair-mp700-ssd-review"><u>Corsair MP700</u></a>. Those have eight channels and DRAM, but older flash.</p><p>Higher-end options include the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/sandisk-wd-black-sn8100-2tb-ssd-review"><u>Sandisk WD_Black SN8100</u></a>, which is the best of the best right now, and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/acer-predator-gm9000-2tb-ssd-review"><u>Acer Predator GM9000</u></a>, a drive that represents less expensive high-end options that cut cost by using older flash or, in the case of the Biwin Black Opal X570, no DRAM.</p><p>We are also comparing the three musketeers of high-end Gen 4 DRAM-less: the Maxio MAP1602-based <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/silicon-power-us75-2tb-review-a-practical-choice-for-the-everyday-gamer"><u>Silicon Power US75</u></a>, the Phison E27T-based <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/sabrent-rocket-4-2tb-ssd-review"><u>Sabrent Rocket 4</u></a>, and the SMI SM2268XT2-based <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/kingston-nv3-ssd-review"><u>Kingston NV3</u></a>. The NV3 and US75 are known for hardware revisions but we want to cover all potential competitors. If you’re looking at a drive like the Plus G4, then there’s the sparkle of some money saved by dropping down to PCIe 4.0, especially given that you might be running your next drive at least temporarily at that speed.</p><h2 id="trace-testing-3dmark-storage-benchmark-2">Trace Testing — 3DMark Storage Benchmark</h2><p>Built for gamers, 3DMark’s Storage Benchmark focuses on real-world gaming performance. Each round in this benchmark stresses storage based on gaming activities including loading games, saving progress, installing game files, and recording gameplay video streams. Future gaming benchmarks will be DirectStorage-inclusive and we also include notes about which drives may be future-proofed.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="reukbhkaM66GSUCNebbN9b" name="ALLSSD-3DMMBps" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/reukbhkaM66GSUCNebbN9b.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="342Tz2K9WwBeemdhihQR8b" name="ALLSSD-3DMLatency" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/342Tz2K9WwBeemdhihQR8b.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="QYfadk83we7Vz5Z62HVL8b" name="ALLSSD-3DMPoints" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/QYfadk83we7Vz5Z62HVL8b.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>Hey, the Plus G4 scores pretty well here! It edges out the Micron-fuelled P510 and lines up nicely with the MP700 Elite. The drive is performing exactly as expected, which, for games, is exceptional. High-end PCIe 5.0 drives are still better, but the Plus G4 is more than fast enough for a primary drive where you also keep all of your games. The only downside is that it only goes up to 2TB.</p><h2 id="trace-testing-pcmark-10-storage-benchmark-2">Trace Testing — PCMark 10 Storage Benchmark</h2><p>PCMark 10 is a trace-based benchmark that uses a wide-ranging set of real-world traces from popular applications and everyday tasks to measure the performance of storage devices. The results are particularly useful when analyzing drives for their use as primary/boot storage devices and in work environments.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="PtV6HB3ggnMQimEY9Yfopi" name="ALLSSD-PCM10Latency" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/PtV6HB3ggnMQimEY9Yfopi.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="uVBxXZBtui39RdUwc6Nmpi" name="ALLSSD-PCM10BW" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/uVBxXZBtui39RdUwc6Nmpi.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="4Yzis4gmpin6EQxmgfdkpi" name="ALLSSD-PCM10Score" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/4Yzis4gmpin6EQxmgfdkpi.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The Plus G4 also has relatively good application performance, but here it is closer to the P510, and it falls behind the similarly equipped MP700 Elite. Why is this so? Well, PCMark is one of those benchmarks that we know some manufacturers have optimized for in firmware. This also goes the other way in that optimized firmware could hurt a drive in this benchmark.</p><p>Client drives, with one application being for use in standardized prebuilt PCs for small businesses, have different requirements than retail. Getting fully specced for Dell or HP is actually a long and potentially grueling process. Client drives usually have a tighter performance envelope based on thermals, and reliability is a higher priority. This is one reason Kioxia drives have proven to be more reliable than analogous retail drives, even with spotty controllers like the InnoGrit IG5236 on the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/kioxia-xg8-review"><u>XG8</u></a>.</p><p>We’re pointing that out because a lot of the time, SSD buyers have a single priority in mind: reliability. This is a very difficult thing to quantify. Most of the time, it comes down to a battle of anecdotes. Well, Kioxia drives have a decent track record for reliability, and if a slight decrease in PCMark 10 performance isn’t concerning to you, then you should consider a drive like the Plus G4 if you’re weighing various options. Kioxia makes the flash on this drive, which gives them a leg up on understanding how to optimize for a consistent, reliable experience.</p><h2 id="console-testing-playstation-5-transfers-2">Console Testing — PlayStation 5 Transfers</h2><p>The PlayStation 5 is capable of taking one additional PCIe 4.0 or faster SSD for extra game storage. While any 4.0 drive will technically work, Sony recommends drives that can deliver at least 5,500 MB/s of sequential read bandwidth for optimal performance. In our testing, PCIe 5.0 SSDs don’t bring much to the table and generally shouldn’t be used in the PS5, especially as they may require additional cooling. Check our <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ps5-ssds"><u>Best PS5 SSDs</u></a> article for more information.</p><p>Our testing utilizes the PS5’s internal storage test and manual read/write tests with over 192GB of data both from and to the internal storage. Throttling is prevented where possible to see how each drive operates under ideal conditions. While game load times should not deviate much from drive to drive, our results can indicate which drives may be more responsive in long-term use.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:88.54%;"><img id="yx4MnxoRC6seU92eEAEU95" name="PS5E28-PS5ReadTest" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/yx4MnxoRC6seU92eEAEU95.png" mos="" link="" align="" fullscreen="" width="1920" height="1700" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:88.54%;"><img id="zCWUrRZ2jKAeSeAeZiUZ85" name="PS5E28-CopyToMBps" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/zCWUrRZ2jKAeSeAeZiUZ85.png" mos="" link="" align="" fullscreen="" width="1920" height="1700" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:88.54%;"><img id="ZqjfHo8gKHWMzv6k79rY85" name="PS5E28-CopyFromMBps" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/ZqjfHo8gKHWMzv6k79rY85.png" mos="" link="" align="" fullscreen="" width="1920" height="1700" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>We don’t see any reason to particularly recommend the Plus G4 for the PS5. It <em>could</em> be a good choice if you want something that might last a long time in a predictable role, but usually it’s better to go with something less expensive that has a full five-year warranty from a known name brand. If such a drive goes bad, you can often get an equivalent or superior replacement. In some regions, this is more difficult, and Kioxia drives can be a safer bet than alternatives with unknown hardware.</p><h2 id="transfer-rates-diskbench-2">Transfer Rates — DiskBench</h2><p>We use the DiskBench storage benchmarking tool to test file transfer performance with a custom, 50GB dataset. We write 31,227 files of various types, such as pictures, PDFs, and videos to the test drive, then make a copy of that data to a new folder, and follow up with a reading test of a newly-written 6.5GB zip file. This is a real world type workload that fits into the cache of most drives.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="eSBVdN27sUtdSVSyiKAusC" name="ALLSSD-DiskBench50Copy" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/eSBVdN27sUtdSVSyiKAusC.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="XJeNFC8XrdMMiUBKan5wsC" name="ALLSSD-DiskBench50Write" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/XJeNFC8XrdMMiUBKan5wsC.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="DHcWda8snvUu4RjZjVakrC" name="ALLSSD-DiskBench65Read" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/DHcWda8snvUu4RjZjVakrC.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The Plus G4 is back to scoring where it should be on our DiskBench copy test: above the P510 and close to the MP700 Elite. It should match the MP700 Elite, and instead, it’s a little behind, but this is expected. The Plus G4 may be optimized differently for sustained writes, which will impact its write performance in this test. Additionally, it has a different firmware revision than the one we tested on the Corsair. Also expected is the P510 falling even more behind, but this perhaps warrants more discussion.</p><p>Careful readers will recall that in our <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/crucial-t710-2tb-ssd-review" target="_blank"><u>Crucial T710</u></a> review, we mentioned that the T710, with its six-plane Micron TLC flash, can actually be faster at the lowest 1TB capacity. Likewise, the P510 is rated higher for sequential writes at 1TB – with the same flash as the T710 – in comparison to the four-plane BiCS8 on the MP700 Elite and Plus G4. This means that bandwidth-hungry buyers should lean towards the P510 at 1TB, while BiCS8 is potentially better at 2TB. If you care less about throughput – and if so, why are you looking at a mid-range PCIe 5.0 drive? – then we have typically found BiCS8 to have better latency.</p><h2 id="synthetic-testing-atto-crystaldiskmark-2">Synthetic Testing — ATTO / CrystalDiskMark</h2><p>ATTO and CrystalDiskMark (CDM) are free and easy-to-use storage benchmarking tools that SSD vendors commonly use to assign performance specifications to their products. Both of these tools give us insight into how each device handles different file sizes and at different queue depths for both sequential and random workloads.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="BpcfGsLG4u7PbEbjEUUq9L" name="ALLSSD-ATTOLinWrite" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/BpcfGsLG4u7PbEbjEUUq9L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="dpz8uLfbgqMvjLRg2aoo9L" name="ALLSSD-ATTOLogRead" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/dpz8uLfbgqMvjLRg2aoo9L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="uxd2HggaqUBJ2TGAXssn9L" name="ALLSSD-ATTOLinRead" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/uxd2HggaqUBJ2TGAXssn9L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="bnp8MnjDrjPZ2DRr3WA99L" name="ALLSSD-ATTOLogWrite" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/bnp8MnjDrjPZ2DRr3WA99L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 5 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="rKXUbeP5QAuMg5qVqixR7L" name="ALLSSD-CDMSeqWriteQD8" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/rKXUbeP5QAuMg5qVqixR7L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 6 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="iX2NHQDpKgGibpWtjva77L" name="ALLSSD-CDMSeqReadQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/iX2NHQDpKgGibpWtjva77L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 7 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="Jnbx6nr7zwutq2oafXDy6L" name="ALLSSD-CDMSeqReadQD8" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/Jnbx6nr7zwutq2oafXDy6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 8 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="W7pedFJtwTAnL5i7RMfz6L" name="ALLSSD-CDMRandWriteIOPSQD256" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/W7pedFJtwTAnL5i7RMfz6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 9 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="2ChpF9ZnGAKLUdkEmj7x6L" name="ALLSSD-CDMRandWriteIOPSQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/2ChpF9ZnGAKLUdkEmj7x6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 10 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="fUbiaor7SCHNTKibFBzd6L" name="ALLSSD-CDMRandReadIOPSQD256" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/fUbiaor7SCHNTKibFBzd6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 11 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="b95RWoqMNcFCEQEK6ePv6L" name="ALLSSD-CDMRandWriteLatencyQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/b95RWoqMNcFCEQEK6ePv6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 12 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="AYMXfAoB4HWypWVLrchn6L" name="ALLSSD-CDMRandReadIOPSQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/AYMXfAoB4HWypWVLrchn6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 13 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="g4DzbBzXLLXtNA2bPG9R6L" name="ALLSSD-CDMSeqWriteQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/g4DzbBzXLLXtNA2bPG9R6L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 14 of 14</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="JHFU7piYTSTP7g3LdPmz4L" name="ALLSSD-CDMRandReadLatencyQD1" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/JHFU7piYTSTP7g3LdPmz4L.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The Plus G4 tracks closely with the MP700 Elite in ATTO, with a slight deviation at the largest block size for reads. These two drives are close to the P510 in writes, but they fall behind on reads starting at 256KiB. As all three are using the same controller, this is probably due to the difference in flash. If we had 1TB versions of these drives to compare, we could make a better guess as to why the drives perform this way. Most likely, it’s due to the plane count difference, as this can influence interleaving with superpages, probably explaining why the P510 dips at 128KiB as well.</p><p>This is reinforced to some extent by the sequential CDM results. QD1 sequential reads favor the P510, suggesting the higher plane count can be useful here. That lines up with our thoughts in the T710 review. As QD1 is a more realistic workload, going with Micron’s newest TLC flash has potential real-world advantages. This advantage disappears with queue depth. Also obvious here is that PCIe 4.0 drives have no chance of keeping up in bandwidth, and the fastest PCIe 5.0 drives are in a class of their own. The mid-range drives like the Plus G4 are still worth a look as they are less expensive and will perform well in a PCIe 4.0 M.2 slot if so required.</p><p>We then look at 4K random I/O performance with a specific emphasis on low queue depth latency. Yes, the ability of the Plus G4 to push over a million IOPS is incredible, but this class of drive is less likely to encounter such workloads. Luckily, the BiCS8 TLC flash does good work on this drive with top-notch 4K QD1 read and write latencies. It can’t match the Black SN8100, but it beats the rest. We’ve come to expect good things out of BiCS8 flash, and the Plus G4 doesn’t disappoint.</p><p>If you did need to use this drive for more powerful things, for AI or otherwise, it is certainly up to the task, but we’re not convinced it’s the best option for that.</p><h2 id="sustained-write-performance-and-cache-recovery-2">Sustained Write Performance and Cache Recovery</h2><p>Official write specifications are only part of the performance picture. Most SSDs implement a write cache, which is a fast area of pseudo-SLC (single-bit) programmed flash that absorbs incoming data. Sustained write speeds can suffer tremendously once the workload spills outside of the cache and into the "native" TLC (three-bit) or QLC (four-bit) flash. Performance can suffer even more if the drive is forced to fold, which is the process of migrating data out of the cache in order to free up space for further incoming data.</p><p>We use Iometer to hammer the SSD with sequential writes for 15 minutes to measure both the size of the write cache and performance after the cache is saturated. We also monitor cache recovery via multiple idle rounds. This process shows the performance of the drive in various states as well as the steady state write performance.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="T63ffo62xr5TKR4ZmRjM2X" name="ALLSSD-WriteSaturation-900s" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/T63ffo62xr5TKR4ZmRjM2X.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="qZ5tVCmQAUycd72ojiuQzW" name="ALLSSD-WriteSaturation-150s" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/qZ5tVCmQAUycd72ojiuQzW.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="3oCV8UN5T9uan28yFd7qtW" name="ALLSSD-WriteSaturation-AvgMBps" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/3oCV8UN5T9uan28yFd7qtW.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The 2TB Plus G4 first writes in its fastest, single-bit mode at over 8.9 GB/s. This is a temporary mode designed to trade capacity for speed. The cache size will vary with how full the drive is, but when empty, as in our testing, the cache extends to over 435GB. When converting 3-bit TLC flash to this pSLC mode, you can have up to almost 700GB, so this cache is of a more moderate size.</p><p>Larger, full-drive caches are typical for low-end DRAM-less drives, while small caches are rarer. The P510 is an example of the latter, and such a scheme allows it to write more consistently, which is particularly good for external use in an enclosure or for certain workloads such as NAS caching. The Plus G4 takes the more common course of somewhere in between.</p><p>Once the Plus G4 fills the cache, it falls to a direct-to-TLC mode at 1.5 GB/s, which is a pretty good speed and matches the MP700 Elite’s steady state write speed. Both it and the MP700 Elite write in TLC for quite a while before finally hitting a folding state. This occurs when the drive is forced to wait for data to be moved over from the cache to the native flash before it can accept incoming writes. The drive can and will move some data over while in TLC mode, but depending on the cache size and drive speed, this may be unsustainable. Folding is an undesirable state as it’s slower with higher latency, which can also impact reads for mixed workloads.</p><p>Generally, any given drive is limited to the base speed of its native flash. This is why QLC flash inevitably gets very slow. The pSLC write state is so much faster than the QLC flash – and QLC can be just as fast as TLC in that mode with the same plane count – that the drive hits a wall more quickly and more drastically, especially as QLC is going from 4-bit to 1-bit instead of 3-bit to 1-bit. The relevance here is that the Plus G4 looks worse than the MP700 Elite in the long run in this test despite having the same flash, but that’s likely because the Plus G4 is optimized differently. Client and OEM drives aren’t designed for sustained writes and often have a tighter power-thermal envelope. The performance here in pSLC and TLC is perfectly consistent, though.</p><p>The one standout here is the P510, which, as we mentioned above, has a smaller cache. The TLC state is then <em>fast</em> in comparison to the Plus G4, but is actually <em>slow</em> in terms of what the drive can do – the P510 has no trouble recovering to 4 GB/s with enough writes. Take into consideration that it’s not realistic to write the entire drive and that interpolation can get messy when we do this level of writes, but the results still suggest that Crucial is being conservative with the P510. We previously pointed out that this might be intended to improve the “quality of service” that was an issue on the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/crucial-2tb-t500-ssd-review" target="_blank"><u>T500,</u></a> or it could be hinting at future external drive products.</p><p>This type of write behavior would be perfect for an enclosure where bottlenecks don’t benefit much from pSLC anyway, and a consistent write speed is desirable. However, for desktop use and moving back to the drive under review, the Plus G4 is adequate for even fairly heavy use.</p><h2 id="power-consumption-and-temperature-2">Power Consumption and Temperature</h2><p>We use the Quarch HD Programmable Power Module to gain a deeper understanding of power characteristics. Idle power consumption is an important aspect to consider, especially if you're looking for a laptop upgrade as even the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ultrabooks-premium-laptops"><u>best ultrabooks</u></a> can have mediocre stock storage. Desktops may be more performance-oriented with less support for power-saving features, so we show the worst-case.</p><p>Some SSDs can consume watts of power at idle while better-suited ones sip just milliwatts. Average workload power consumption and max consumption are two other aspects of power consumption but performance-per-watt, or efficiency, is more important. A drive might consume more power during any given workload, but accomplishing a task faster allows the drive to drop into an idle state more quickly, ultimately saving energy.</p><p>For temperature recording we currently poll the drive’s primary composite sensor during testing with a ~22°C ambient. Our testing is rigorous enough to heat the drive to a realistic ceiling temperature.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="NT6DfieZo5tbUpYY9bWc7h" name="ALLSSD-QuarchEfficiency" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/NT6DfieZo5tbUpYY9bWc7h.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="jgRRxLY9hGo4V7z67rE47h" name="ALLSSD-QuarchMaxPower" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/jgRRxLY9hGo4V7z67rE47h.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="DVUpyQYhpiHb2rHsvDy68h" name="ALLSSD-QuarchIdlePower" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/DVUpyQYhpiHb2rHsvDy68h.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="ZogyLWW3sojjFhYrUFh78h" name="ALLSSD-QuarchAvgPower" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/ZogyLWW3sojjFhYrUFh78h.png" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The Plus G4 is quite efficient, more efficient than the P510 but less than the MP700 Elite. We already know from experience that BiCS8 flash is more efficient than Micron’s, with appropriate trade-offs, but the Plus G4 still falls behind the fastest BiCS8-based drives. This might again be due to optimization.</p><p>We would like to point out that among the E31T-based drives we’ve tested, some – specifically the P510 and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/pny-cs2150-2tb-ssd-review" target="_blank"><u>PNY CS2150</u></a> – have custom firmware strings, while others, including the MP700 Elite and Plus G4, have utilized standard Phison versioning. That does not mean there is or isn’t a lack of custom implementation, not least because Sandisk and Kioxia BiCS8 actually do not perform the same in all cases. That sounds unusual, as the flash should be identical; however, there are performance differences on some Phison controllers, such as the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/phison-e28-2tb-ssd-review" target="_blank"><u>E28,</u></a> based on early reports.</p><p>Regardless of the specific reasons for any differences, while the Plus G4 is less efficient on paper and has some minor performance quirks, the result is a more reliable experience. Our temperature testing backs this up as we hit a maximum temperature of 51°C, which is more than 30°C below the throttling point. This is an excellent result, making this a fantastic drive for laptops and other hot or confined environments. This is sensible since client and OEM drives often need to survive in low-airflow cases and warmer ambients.</p><h2 id="test-bench-and-testing-notes-2">Test Bench and Testing Notes</h2><div ><table><caption>Test Bench and Testing Notes</caption><tbody><tr><td class="firstcol " ><p><strong>CPU</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B09FXDLX95">Intel Core i9-12900K</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Motherboard</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B0BG6M53DG/">Asus ROG Maximus Z790 Hero</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Memory</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B0BJ1892HJ">2x16GB G.Skill DDR5-5600 CL28</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Graphics</strong></p></td><td
><p>Intel Iris Xe UHD Graphics 770</p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>CPU Cooling</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B07PB24DN2">Enermax Aquafusion 240</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Case</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B08412JPCH">Cooler Master TD500 Mesh V2</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Power Supply</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B0BXFQ6XPB">Cooler Master V850 i Gold</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>OS Storage</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B0BJ116VV2">Sabrent Rocket 4 Plus-G 2TB</a></p></td><td
></td></tr><tr><td class="firstcol " ><p><strong>Operating System</strong></p></td><td
><p><a href="https://www.amazon.com/dp/B09V71FYGS">Windows 11 Pro</a></p></td><td
></td></tr></tbody></table></div><p>We use an Alder Lake platform with most background applications such as indexing, Windows updates, and anti-virus disabled in the OS to reduce run-to-run variability. Each SSD is prefilled to 50% capacity and tested as a secondary device. Unless noted, we use active cooling for all SSDs.</p><h2 id="lexar-nm1090-pro-bottom-line-2">Lexar NM1090 Pro Bottom Line</h2><p>The Kioxia Exceria Plus G4 isn’t a drive we expected to be excited about, but Kioxia’s Exceria line has gained popularity, especially in non-U.S. regions, and the drives have at least a neutral reputation, and usually a positive one. Contrast this with the infamous problems that we see with some drives that have changing hardware, which includes not only going from TLC to QLC flash but also swaps to hotter or less reliable controllers, and the usual rumor mill of “broken” drives, like with the Phison E18 performance issue. Kioxia has effectively dodged all of this and has also managed to maintain respectable levels of performance and power efficiency.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="8M5RSBpTSth2mfJM8pPzeS" name="05" alt="Kioxia Exceria Plus G4 2TB SSD" src="https://cdn.mos.cms.futurecdn.net/8M5RSBpTSth2mfJM8pPzeS.jpg" mos="" align="middle" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>That’s both a positive and a negative. The Plus G4 doesn’t stand out in any way, but its baseline characteristics are sufficient. We would like better availability and maybe a wider capacity range. However, for many markets, the Plus G4 could be a diamond in the rough, and we feel it’s worth covering this drive for that reason alone.</p><p>Pricing right now in the U.S. isn’t competitive, but at least we get to see where this drive falls against the competition, most notably the Crucial P510. You can get higher bandwidth, decent power efficiency, good all-around and sustained performance, and reasonable pricing at the most popular capacities. You can’t go wrong buying any of these drives, including the Plus G4, and that's a good thing.</p><p>If you want something less expensive, there are plenty of PCIe 4.0 drives, and if you want something faster, there are high-end PCIe 5.0 drives available. If you need more capacity, well, there are many affordable 4TB drives, and the 8TB WD Black SN850X remains a good choice. Nothing much changes here, but the Plus G4 has its place.</p><p>We have the feeling that the Plus G4 would be a reliable drive that runs cool and has at least halfway decent software support. This isn’t a no-name brand slapping its name on random hardware. It’s a viable alternative and is a safe pick for a last-minute build or project. At the end of the day, the Plus G4 is not terribly exciting, but it’s a good SSD – not everything has to be covered in liquid cooling and RGB – and we can readily recommend it.</p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-ssds,3891.html"><strong>Best SSDs</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-external-hard-drive-ssd,5987.html"><strong>Best External SSDs</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ssd-for-steam-deck"><strong>Best SSD for the Steam Deck</strong></a></p>
https://www.tomshardware.com/pc-components/ssds/kioxia-exceria-plus-g4-2tb-ssd-review
Kioxia’s Exceria Plus G4 is a mid-range PCIe 5.0 drive that is a safe choice.
s9HjrRCbdngU2443r5r5Ko
Thu, 18 Sep 2025 12:00:00 +0000 SSDs
PC Components
Storage
Shane Downing
Tom&#039;s Hardware
Kioxia Exceria Plus G4 2TB SSD
Kioxia Exceria Plus G4 2TB SSD
<![CDATA[ Microsoft's new handheld gaming mode, exclusive to ROG Xbox Ally, has just leaked for every handheld running Windows 11 — all you need is the 25H2 update and a few registry tweaks ]]>
<p>Earlier this year, Microsoft and Asus announced the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/xbox/asus-partners-with-microsoft-launch-first-xbox-gaming-handhelds-the-rog-xbox-ally-and-ally-x">ROG Xbox Ally</a>, an update to the first-gen ROG Ally, now adorned with Xbox branding. Along with the new name came new specs, but more importantly, the Xbox partnership wasn’t just a token collaboration — rather, a deeply integrated experience finally meant to "fix" Windows on handheld devices. Hence, the ROG Xbox Ally would ship with a new full-screen Xbox app that it would boot into by default, superseding Windows 11 entirely. This was exclusive to the ROG Xbox Ally and was supposed to come to other handhelds later; however, it seems the opposite has happened.</p><p><a data-analytics-id="inline-link" href="https://x.com/tomwarren/status/1968436869982503121" target="_blank">The Verge's Tom Warren</a> reports that the full-screen Xbox experience meant for ROG Xbox Ally devices has leaked early, and can now be installed on any handheld running Windows. The actual ROG Xbox Ally is set to launch next month, which means that everyone else will get to enjoy Microsoft's new handheld gaming mode before the very device that was set to debut it. There's a <a data-analytics-id="inline-link" href="https://www.reddit.com/r/ROGAlly/comments/1niwsfi/guide_for_enabling_the_full_screen_experience_on/" target="_blank">full guide on Reddit</a> that breaks down how to install it, and it's relatively easy to follow as long as you know your way around Windows. If you have a regular Ally, Lenovo's Legion Go, or the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/handheld-gaming/best-pc-gaming-handhelds">myriad of PC handhelds out there</a>, you should be eligible.</p><blockquote class="reddit-card"
><a href="https://www.reddit.com/r/ROGAlly/comments/1niwsfi/guide_for_enabling_the_full_screen_experience_on">Guide for enabling the Full screen experience on the ROG Ally (and other handhelds)</a> from <a href="https://www.reddit.com/r/ROGAlly">r/ROGAlly</a></blockquote><script async src="//embed.redditmedia.com/widgets/platform.js" charset="UTF-8"></script><p>The only requirement for this to work is the 25H2 Windows 11 update, for which you need to be part of the Windows Insider program. The build resides in the Release Preview channel, so once you've got that going, there are a few registry edits you need to make <em>if </em>you don't see the "Enter full-screen experience on start up" toggle right away. After all is said and done, a restart should boot you directly into the new Xbox experience, where all your favorite games should be consolidated into one place — including stores like Steam, Epic Games, and Battle.net.</p><p>The full-screen handheld mode is still based on Windows 11, just running without any of the extra stuff that hogs up resources in the background. It should use less memory, and the UI should be a lot more handheld-friendly. <a data-analytics-id="inline-link" href="https://www.windowscentral.com/hardware/handheld-gaming-pc/we-hacked-the-new-windows-11-xbox-mode-onto-the-old-rog-ally-how-does-it-optimize-performance" target="_blank">Windows Central tested the update</a> on an original ROG Ally and saw marked improvements across the board in terms of performance—going from 29 FPS in Shadow of the Tomb Raider to 38 FPS—and even an extra hour gained in battery life. These upgrades mostly come courtesy of disabling unnecessary background processing and startup apps, not some significant underlying change to the Windows kernel itself.</p><div class="youtube-video" data-nosnippet ><div class="video-aspect-box"><iframe data-lazy-priority="low" data-lazy-src="https://www.youtube.com/embed/8cD4krJLndY" allowfullscreen></iframe></div></div><p>It's important to note that you're not locked into the full-screen experience either; the ability to Alt-Tab out of apps and go into the regular desktop environment is still there. That being said, Microsoft itself recommends using mouse and keyboard for that since it's not designed for handhelds.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/video-games/handheld-gaming/microsofts-new-handheld-gaming-mode-exclusive-to-rog-xbox-ally-has-just-leaked-for-every-handheld-running-windows-11-all-you-need-is-the-25h2-update-and-a-few-registry-tweaks
If you've been waiting for the new Xbox handheld experience Microsoft has cooked up with Asus for the Xbox ROG Ally, your wishes have been answered. A leaked version of the full-screen Xbox experience has surfaced online, allowing any Windows handheld to gain the USP of the ROG Xbox Ally.
eWP6oMJPsppDxxt42ab2p9
Thu, 18 Sep 2025 11:55:25 +0000 Handheld Gaming
Video Games
Console Gaming
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
Asus
Asus ROG Xbox Ally
Asus ROG Xbox Ally
<![CDATA[ China targets brain computer interface race with new standard — groundwork could lead to breakthroughs as soon as 2027 ]]>
<p>China appears to be moving fast to establish itself as a brain computer interface (BCI) leader. The nation's latest advance is the release of a medical device industry standard, the ‘Medical Device Terminology Using Brain-Computer Interface Technology,’ reports <a data-analytics-id="inline-link" href="https://www.ithome.com/0/883/421.htm" target="_blank">IT Home</a> (machine translation).</p><p>While many BCI headlines revolve around the Elon Musk-backed <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/wearable-tech/brain-interface-used-to-edit-youtube-video-paralyzed-neuralink-patient-also-uses-ai-to-narrate-with-his-own-voice">Neuralink </a>system, increasingly, we are seeing breakthrough advances from China. Earlier this year, we reported on a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/wearable-tech/china-launches-first-ever-invasive-brain-computer-interface-clinical-trial-tetraplegic-patient-could-skillfully-operate-racing-games-after-just-three-weeks">tetraplegic patient</a> skillfully playing racing games, and another subject <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/controllers-gamepads/chinese-brain-computer-interface-user-reportedly-plays-black-myth-wukong-other-games">enjoying complex PC games</a> such as <em>Black Myth: Wukong </em>and<em> Honor of Kings</em>.</p><p>For a while now, China has let it be known that it will try to coordinate its broad range of BCI research and development talents and commercial enterprises for the benefit of its industry on a wider scale. Earlier this month, we reported on a significant milestone towards this goal, with <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/china-bci-blueprint">a state-backed blitz</a> coordinating ministries, planners, and regulatory bodies, and laying out 17 steps from R&D to commercialization.</p><p>Returning to our headline news, and another piece of the jigsaw puzzle has been slotted into place. According to the source, China’s equivalent of the FDA will implement the new BCI standard from January 1, 2026.</p><p>An excerpt from the official release shows that the new standard, number YY/T 1987 – 2025, concerns “Medical devices using brain computer interface technology.” As with computing, setting a standard in any field can be a vital advantage to early adopters in the market. Standards setters can earn a range of advantages, and widespread use and adoption of useful early standards can establish an entity (company, country) as a clear leader.</p><h2 id="the-long-walk-from-what-if-to-what-is-2">The long walk from 'what if' to 'what is'</h2><p>There’s still a way to go, from planning to execution of this broad strategy. And, of course, the acceptance of China-established standards outside the country. However, should everything fall into place, China predicts most technical hurdles to be cleared by 2027. Moreover, it expects to incubate two to three leading BCI enterprises with global reach by 2030.</p><p>In the real world, we know there can be a technological or performance gulf between China's claims and China's results. So it seems healthy to throw a year or two onto the stated projections to account for that, and the typical extra distance between inspiration and implementation. Having healthy BCI competition from China could make such devices more accessible to patients in the rest of the world.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/peripherals/wearable-tech/china-targets-brain-computer-interface-race-with-new-standard-new-bci-standard-could-lead-to-breakthroughs-as-soon-as-2027
China appears to be moving fast to establish itself as a brain computer interface (BCI) leader, and officials have just published a new medical industry standard.
N7pRx4Kxgcj9dyGzAtULd8
Thu, 18 Sep 2025 11:39:00 +0000 Wearable Tech
Peripherals
Mark Tyson
Getty / Yuichiro Chino
Brain computer illustration
Brain computer illustration
<![CDATA[ Asus is 'actively investigating' ROG gaming laptop stuttering woes — Models released / sold between 2021 - 2024 affected by 'performance interruptions' ]]>
<p>Asus has acknowledged reports about an ongoing stuttering issue with some of its ROG <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/gaming-laptops/best-gaming-laptops">gaming laptops</a> that users claim has been negatively affecting performance and the user experience, according to <a data-analytics-id="inline-link" href="https://www.techpowerup.com/341121/asus-promises-fix-of-the-stuttering-issues-actively-investigates-the-problem" target="_blank">TechPowerUp</a>. The company stopped short of agreeing that an issue exists, but said that it was "actively investigating these cases," all the same.</p><p>"We've seen recent reports about performance interruptions on some ROG laptops, and we want you to know our team is actively investigating these cases," the Asus Statement reads. "We understand that smooth and reliable performance is crucial to high-performance machines like these, and we're dedicated to delivering that. Your feedback and detailed reports are invaluable, and we'll continue to provide updates and support through our official channels." It then thanked users for their patience.</p><p>The first reports of this problem started popping up earlier this week when GitHub user, Mohamed "Zephkek" Maatallah <a data-analytics-id="inline-link" href="https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive">created a new repository</a> collecting evidence about the problem. They claim it seems to be affecting a range of users on various Asus ROG gaming laptops, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/gaming-laptops/asus-rog-strix-g16-2025-review">including Strix</a>, Scar, and Zephyrus lines.</p><p>The issues include stuttering while watching YouTube videos, audio crackling and pops on Discord, and random mouse cursor freezes. After attempting a range of more generic fixes, Zephkek claims they narrowed the issue down to a problem with Asus' BIOS firmware.</p><p>Using the latency monitoring tool LatencyMon, Zephkek discovered that a single CPU core was being bottlenecked by interrupt requests, in some cases for as long as 90 seconds, hamstringing performance and making time-sensitive tasks stutter while core priority is juggled.</p><p>Deeper into their investigation, Zephkek also discovered strange power cycling of the dedicated GPU, turning it off and on again repeatedly every 15-30 seconds, even when it's supposed to be consistently active performing specific tasks.</p><p>Zephkek concluded that there are actually three problems affecting ROG laptops:</p><ul><li>Misunderstanding of interrupt context introducing unnecessary delays.</li><li>Mishandling of interrupt requests that aren't properly cleared, leading to looping interrupts.</li><li>GPU power cycling that doesn't check which GPU is currently in-use.</li></ul><p>They even did the legwork to track down the first public reports of these issues and <a data-analytics-id="inline-link" href="https://rog-forum.asus.com/t5/rog-strix-series/g15-advantage-edition-g513qy-severe-dpc-latency-audio-dropouts/m-p/809512" target="_blank">found one in August 2021</a> for a G15 Advantage Edition Asus laptop. They then discovered further reports of problems persisting with laptops through 2021 and into 2024, suggesting it affects multiple generations of Asus gaming laptops.</p><p>Asus has said it's looking into this issue, so hopefully it won't be long until it releases a firmware patch that fixes this for the majority of users.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/laptops/asus-is-actively-investigating-rog-gaming-laptop-stuttering-issue-2021-2024-models-affected-by-performance-interruptions
Asus has acknowledged reports of stuttering issues with its ROG gaming laptops from 2021 through 2024 and has confirmed it is investigating the issue, though stopped short of admitting any kind of fault.
T49knL6fZwC9VzL5ywWd7o
Thu, 18 Sep 2025 11:30:00 +0000 Laptops
Jon Martindale
Tom&#039;s Hardware
ASUS ROG Zephyr
ASUS ROG Zephyr
<![CDATA[ Nvidia and Intel announce jointly developed 'Intel x86 RTX SOCs' for PCs with Nvidia graphics, also custom Nvidia data center x86 processors — Nvidia buys $5 billion in Intel stock in seismic deal ]]>
<p>In a surprise announcement that finds two long-time rivals working together, Nvidia and Intel announced today that the companies will jointly develop multiple new generations of x86 products together — a seismic shift with profound implications for the entire world of technology. Before the news broke, Tom's Hardware spoke with Nvidia representatives to learn more details about the company’s plans.</p><p>The products include x86 Intel CPUs tightly fused with an Nvidia RTX graphics chiplet for the consumer gaming PC market, named the ‘Intel x86 RTX SOCs.’ Nvidia will also have Intel build custom x86 data center CPUs for its AI products for hyperscale and enterprise customers. Additionally, Nvidia will buy $5 billion in Intel common stock at $23.28 per share, representing a roughly 5% ownership stake in Intel. (Intel stock is now up 33% in premarket trading.)</p><p>The partnership between the two companies is in the very early stages, Nvidia told us, so the timeline for product releases, along with any product specifications, will be disclosed at a later, unspecified date. (Given the traditionally long lead-times for new processors, it is rational to expect these products will take at least a year, and likely longer, to come to market.)</p><p>Nvidia emphasized that the companies are committed to multi-generation roadmaps for the co-developed products, which represents a strong investment in the x86 ecosystem. But Nvidia representatives tell us it also remains fully committed to other announced product roadmaps and architectures, including the company's Arm-based <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidias-project-digits-desktop-ai-supercomputer-fits-in-the-palm-of-your-hand-usd3-000-to-bring-1-pflops-of-performance-home">GB10 Grace Blackwell processors for workstations</a> and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-unveils-144-core-grace-cpu-superchip-claims-arm-chip-15x-faster-than-amds-epyc-rome">Nvidia Grace</a> <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-details-grace-hopper-cpu-superchip-design-144-cores-on-4n-tsmc-process">CPUs for data centers</a>, as well as the next-gen <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidias-rubin-gpu-and-vera-cpu-data-center-ai-platforms-begin-tape-out-both-chips-in-fab-and-on-track-for-2026">Vera CPUs</a>. Nvidia says it also remains committed to products on its internal roadmaps that haven’t been publicly disclosed yet, indicating that the new roadmap with Intel will merely be additive to existing initiatives.</p><p>Nvidia hasn’t disclosed whether it will use Intel Foundry to produce any of these products yet. However, while Intel has used TSMC to manufacture some of its own recent products, its goal is to bring production of most high-performance products back into its own foundries.</p><p>Some products never left. For instance, Intel’s existing <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-launches-granite-rapids-xeon-6900p-series-with-120-cores-matches-amd-epycs-core-counts-for-the-first-time-since-2017">Granite Rapids</a> data center processors use the ‘Intel 3’ node, and the upcoming <a data-analytics-id="inline-link" href="https://www.tomshardware.com/desktops/servers/intel-reveals-288-core-xeon">Clearwater Forest Xeons</a> will use Intel’s own 18A process node for compute. This suggests that at least some of the Nvidia-custom x86 silicon, particularly for the data center, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-ceo-intel-test-chip-results-for-next-gen-process-look-good">could be fabbed on Intel nodes</a>. Intel also uses TSMC to fabricate many of its client x86 processors, however, so we won’t know for sure until official announcements are made — particularly for the RTX GPU chiplet.</p><p>In either case, Nvidia has been <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-in-talks-with-intel-foundry-intel-and-amd-know-all-our-secrets">mulling using Intel Foundry since 2022</a>, has <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-ceo-intel-test-chip-results-for-next-gen-process-look-good">fabbed test chips</a> there, and participates in the U.S. Defense Dept.'s <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-foundry-services-wins-us-defense-contract-for-chips-with-18a-node">RAMP-C project</a> with Intel. The DoD project involves Nvidia already <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-foundry-head-stu-pann-explains-companys-plan-to-build-arm-chips-move-more-manufacturing-to-the-us">making chips on Intel's 18A process node</a>, so it wouldn't be a total surprise.</p><p>While the two companies have engaged in heated competition in some market segments, Intel and Nvidia have partnered for decades, ensuring interoperability between their hardware and software for products spanning both the client and data center markets. The PCIe interface has long been used to connect Intel CPUs and Nvidia GPUs. The new partnership will find tighter integration using the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-announces-nvlink-fusion-to-allow-custom-cpus-and-ai-accelerators-to-work-with-its-products">NVLink interface for CPU-to-GPU communication</a>, which affords up to 14 times more bandwidth along with lower latency than PCIe, thus granting the new x86 products access to the highest performance possible when paired with GPUs. That's a strategic advantage. Let’s dive into the details we’ve learned so far.</p><h2 id="intel-x86-rtx-socs-for-the-pc-gaming-market-2">Intel x86 RTX SOCs for the PC gaming market</h2><p>For the PC market, the Intel x86 RTX SoC chips will come with an x86 CPU chiplet tightly connected with an Nvidia RTX GPU chiplet via the NVLink interface. This type of processor will have both CPU and GPU units merged into one compact chip package that externally looks much like a standard CPU, rivaling AMD’s competing APU products.</p><p>Intel's new x86 RTX CPUs will compete directly with AMD's APUs. For AMD, that means it faces intensifying competition from a company with the leading market share in notebook CPUs (<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago" target="_blank">Intel ships ~79% of laptop chips worldwide</a>) that's now armed with GPU tech from Nvidia, which <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amds-discrete-desktop-gpu-market-share-hits-all-time-low-as-nvidia-extends-its-lead" target="_blank">ships 92% of the world's gaming GPUs</a>.</p><p>This type of tight integration packs all the gaming prowess into one package without an external discrete GPU, providing power and footprint advantages. As such, we're told these chips will be heavily focused on thin-and-light gaming laptops and small form-factor PCs, much like today’s APUs from AMD. However, it’s possible the new Nvidia/Intel chips could come in multiple flavors and permeate further into the Intel stack over time.</p><p>Intel has worked on a similar type of chip before with AMD; there is at least one significant technical difference between these initiatives, however. Intel launched its <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/intel-hades-canyon-nuc-vr,5536.html">Kaby Lake-G chip in 2017</a> with an Intel processor fused into the same package as an AMD Radeon GPU chiplet, much the same as the description of the new Nvidia/Intel chips. You can see an image of the Intel/AMD chip below.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1280px;"><p class="vanilla-image-block" style="padding-top:65.70%;"><img id="v86mjFRvYe7QGC7NP6gPLm" name="8th-Gen-Intel-Core-processor.jpg" alt="sdf" src="https://cdn.mos.cms.futurecdn.net/v86mjFRvYe7QGC7NP6gPLm.jpg" mos="" link="" align="" fullscreen="" width="1280" height="841" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="caption-text">An RTX GPU chiplet connected to an Intel CPU chiplet via the fast and efficient NVLink interface.
</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1510px;"><p class="vanilla-image-block" style="padding-top:60.20%;"><img id="5Hd4zpDFEkftzMEPoMmF99" name="05.JPG" alt="asdf" src="https://cdn.mos.cms.futurecdn.net/5Hd4zpDFEkftzMEPoMmF99.jpg" mos="" link="" align="" fullscreen="" width="1510" height="909" attribution="" endorsement="" class=""></p></div></div></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1510px;"><p class="vanilla-image-block" style="padding-top:60.33%;"><img id="8RHXSnZBY8Zjvh8vzTPeuY" name="09.JPG" alt="afd" src="https://cdn.mos.cms.futurecdn.net/8RHXSnZBY8Zjvh8vzTPeuY.jpg" mos="" link="" align="" fullscreen="" width="1510" height="911" attribution="" endorsement="" class=""></p></div></div></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1510px;"><p class="vanilla-image-block" style="padding-top:59.34%;"><img id="y3y8rMjRgDiJ8hD326woUR" name="02.JPG" alt="asdf" src="https://cdn.mos.cms.futurecdn.net/y3y8rMjRgDiJ8hD326woUR.jpg" mos="" link="" align="" fullscreen="" width="1510" height="896" attribution="" endorsement="" class=""></p></div></div></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 5 of 5</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1510px;"><p class="vanilla-image-block" style="padding-top:59.93%;"><img id="Cqhf8QvkAC9x6sHChgeeCm" name="07.JPG" alt="asdf" src="https://cdn.mos.cms.futurecdn.net/Cqhf8QvkAC9x6sHChgeeCm.jpg" mos="" link="" align="" fullscreen="" width="1510" height="905" attribution="" endorsement="" class=""></p></div></div></figure></div></div></div><p>This SoC had a CPU at one end connected via a PCIe connection to the separate AMD GPU chiplet, which is flanked by a small, dedicated memory package. This separate memory package was only usable by the GPU. The Nvidia/Intel products will have an RTX GPU chiplet connected to the CPU chiplet via the faster and more efficient NVLink interface, and we’re told it will have uniform memory access (UMA), meaning both the CPU and GPU will be able to access the same pool of memory. Given the particulars of Nvidia's NVLink Fusion architecture, we can expect the chips to communicate via a refined interface, but it is unlikely that it will leverage Nvidia's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-details-grace-hopper-cpu-superchip-design-144-cores-on-4n-tsmc-process">C2C (Chip-to-Chip) technology</a>, an inter-die/inter-chip interconnect that's based on Arm protocols that aren't likely optimized for x86.</p><p>Intel notoriously <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-discontinue-kaby-lake-g-amd-graphics,40577.html">axed the Kaby Lake-G products in 2019</a>, and the existing systems were <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-graphics-driver-update-hades-canyon-amd-12-month-delay">left without proper driver support</a> for <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/windows-11-kaby-lake-g-drivers">quite some time</a>, in part because Intel was responsible for validating the drivers, and then finger-pointing ensued. We’re told that both Intel and Nvidia will be responsible for their respective drivers for the new models, with Nvidia naturally providing its own GPU drivers. However, Intel will build and sell the consumer processors.</p><p>We haven’t spoken with Intel yet, but the limited scope of this project means that Intel’s proprietary Xe graphics architecture will most assuredly live on as the primary integrated GPU (iGPU) for its mass-market products.</p><h2 id="nvidia-s-first-x86-data-center-cpus-2">Nvidia's first x86 data center CPUs</h2><p>Intel will fabricate custom x86 data center CPUs for Nvidia, which Nvidia will then sell as its own products to enterprise and data center customers. However, the entirety and extent of the modifications are currently unknown. We know that Nvidia will employ its NVLink interface, which suggests that the chips could leverage Nvidia’s new <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/nvidia-announces-nvlink-fusion-to-allow-custom-cpus-and-ai-accelerators-to-work-with-its-products" target="_blank">NVLink Fusion</a> technology for custom CPUs and accelerators, enabling faster and more efficient communication with Nvidia’s GPUs than is possible with the PCIe interface.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3485px;"><p class="vanilla-image-block" style="padding-top:52.88%;"><img id="MftMZVxs3dkte2VoNsxtMi" name="Screenshot 2025-05-19 115749.png" alt="NVLink Fusion" src="https://cdn.mos.cms.futurecdn.net/MftMZVxs3dkte2VoNsxtMi.png" mos="" align="middle" fullscreen="" width="3485" height="1843" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Nvidia)</span></figcaption></figure><p>Intel has long offered custom Xeons to its customers, primarily hyperscalers, often with relatively minor tweaks to clock rates, cache capacities, and other specifications. In fact, these mostly slightly-modified custom Xeon models once comprised more than 50% of Intel’s Xeon shipments. Intel has endured several years of market share erosion due to AMD’s advances, most acutely in the hyperscale market. Therefore, it is unclear if the 50% number still holds true, as hyperscalers were the primary customers for custom models.</p><p>Intel has<a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-announces-idm-20-foundry"> said that it will design completely custom x86 chips for customers</a> as part of its IDM 2.0 strategy. However, aside from a recent announcement of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-outlines-a-plan-to-get-back-in-the-game-pause-fab-projects-in-europe-make-the-foundry-unit-an-independent-subsidiary-and-streamline-the-x86-portfolio">custom AWS chips</a> that sound like the slightly modified Xeons mentioned above, we haven’t heard of any large-scale uptake for significantly modified custom x86 processors. Intel <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/intel-ousts-ceo-of-products-as-part-of-the-latest-executive-shake-up-ending-30-year-career-company-also-establishes-new-custom-chip-design-unit">announced a new custom chip design unit just two weeks ago</a>, so it will be interesting to learn the extent of the customization for Nvidia’s x86 data center CPUs.</p><p>Nvidia already uses Intel’s Xeons in several of its systems, like the Nvidia DGX B300, but these systems still use the PCIe interface to communicate with the CPU. Intel’s new collaboration with Nvidia will obviously open up new opportunities, given the tighter integration with NVLink and all the advantages it brings with it.</p><p>The likelihood of AMD adopting NVLink Fusion is somewhere around zero, as the company is heavily invested in its own <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/amd-infinity-fabric-cpu-to-gpu">Infinity Fabric (XGMI)</a> and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/ualink-has-nvidias-nvlink-in-the-crosshairs-final-specs-support-up-to-1-024-gpus-with-200-gt-s-bandwidth">Ultra Accelerator Link (UALink)</a> initiatives, which aim to provide an open-standard interconnect to rival NVLink and democratize rack-scale interconnect technologies. Intel is also a member of UALink, which uses AMD’s Infinity Fabric protocol as the foundation.</p><h2 id="dollar-and-cents-geopolitics-2">Dollar and Cents, Geopolitics</h2><p>Nvidia’s $5 billion purchase of Intel common stock will come at $23.28 a share, roughly 6% below the current market value, but several aspects of this investment remain unclear. Nvidia hasn’t stated whether it will have a seat on the board (which is unlikely) or how it will vote on matters requiring shareholder approval. It is also unclear if Intel will issue new stock (primary issuance) for Nvidia to purchase, as it did when the U.S. government recently became an Intel shareholder (that is likely). Naturally, the investment is subject to approval from regulators.</p><p>Nvidia’s buy-in comes on the heels of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/big-tech/trump-says-u-s-govt-will-take-a-10-percent-ownership-stake-in-intel-lip-bu-tan-reportedly-agreed-to-unprecedented-arrangement-for-a-domestic-chipmaker">U.S government buying $10 billion of newly-created Intel stock</a>, granting the country a 9.9% ownership stake at $20.47 per share. The U.S. government won’t have a seat on the board and agreed to vote with Intel’s board on matters requiring shareholder approval “with limited exceptions.” <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/softbank-to-buy-usd2-billion-in-intel-shares-at-usd23-each-firm-still-owns-majority-share-of-arm">Softbank has also recently purchased $2 billion worth of primary issuance of Intel stock</a> at $23 per share.</p><div ><table><caption>Purchases of Intel Stock</caption><tbody><tr><td class="firstcol empty" ></td><td
><p>Total</p></td><td
><p>Share Price</p></td><td
><p>Stake in Intel</p></td></tr><tr><td class="firstcol " ><p>Nvidia</p></td><td
><p>$5 Billion</p></td><td
><p>$23.28</p></td><td
><p>~5%</p></td></tr><tr><td class="firstcol " ><p>U.S. Government</p></td><td
><p>$9 Billion</p></td><td
><p>$20.47</p></td><td
><p>~9.9%</p></td></tr><tr><td class="firstcol " ><p>Softbank</p></td><td
><p>$2 Billion</p></td><td
><p>$23</p></td><td
></td></tr></tbody></table></div><p>The U.S. government says it invested in Intel with the goal of bolstering US technology, manufacturing, and national security, and the investments from the private sector also help solidify the struggling Intel. Altogether, these investments represent a significant cash influx for Intel as it attempts to maintain the heavy cap-ex investments required to compete with TSMC, all while struggling with a negative amount of free cash flow.</p><p>“AI is powering a new industrial revolution and reinventing every layer of the computing stack — from silicon to systems to software. At the heart of this reinvention is Nvidia’s CUDA architecture,” said Nvidia CEO Jensen Huang. “This historic collaboration tightly couples NVIDIA’s AI and accelerated computing stack with Intel’s CPUs and the vast x86 ecosystem—a fusion of two world-class platforms. Together, we will expand our ecosystems and lay the foundation for the next era of computing.”</p><p>“Intel’s x86 architecture has been foundational to modern computing for decades – and we are innovating across our portfolio to enable the workloads of the future,” said Intel CEO Lip-Bu Tan. “Intel’s leading data center and client computing platforms, combined with our process technology, manufacturing and advanced packaging capabilities, will complement Nvidia's AI and accelerated computing leadership to enable new breakthroughs for the industry. We appreciate the confidence Jensen and the Nvidia team have placed in us with their investment and look forward to the work ahead as we innovate for customers and grow our business.”</p><p>We’ll learn more details of the new partnership later today when Nvidia CEO Jensen Huang and Intel CEO Lip-Bu Tan hold a <a data-analytics-id="inline-link" href="https://events.q4inc.com/attendee/108505485">webcast press conference at 10 am PT</a>.
{<strong>EDIT</strong>: you can <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/teams-at-nvidia-and-intel-have-been-working-in-secret-on-jointly-developed-processors-for-a-year-the-trump-administration-has-no-involvement-in-this-partnership-at-all">read our futher coverage of that press event here</a>.}</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal
Nvidia and Intel announced today that the companies would jointly develop multiple new generations of products together. The products include x86 Intel CPUs tightly fused with an Nvidia RTX graphics chiplet for the consumer gaming PC market, and custom-built Intel x86 CPUs for Nvidia’s AI products for hyperscale and enterprise customers.
6S7ZPUsULrjZhoioYnhg6Z
Thu, 18 Sep 2025 11:00:11 +0000 CPUs
PC Components
palcorn@outlook.com (Paul Alcorn)
Paul Alcorn
Nvidia
asdf
asdf
<![CDATA[ Shai-Hulud malware campaign dubbed 'the largest and most dangerous npm supply-chain compromise in history' — 'hundreds' of JavaScript packages affected ]]>
<p>It's a bad time to be a JavaScript developer, after Koi Security <a data-analytics-id="inline-link" href="https://www.koi.security/blog/shai-hulud-npm-supply-chain-attack-crowdstrike-tinycolor" target="_blank">revealed</a> yesterday that it is tracking "the largest and most dangerous npm supply-chain compromise in history."<br><br>The security firm said the Shai-Hulud malware campaign "has now impacted <strong>hundreds of packages</strong> across multiple maintainers," including "popular libraries such as <strong>@ctrl/tinycolor</strong> as well as packages maintained by <strong>CrowdStrike</strong>." (Emphasis theirs.) And the problem is probably going to get worse before it gets better, because the malware in question is a worm that autonomously spreads from package to package.<br><br>"Attackers published malicious versions of @ctrl/tinycolor and other npm packages, injecting a large obfuscated script (bundle.js) that executes automatically during installation," Koi Security said in the blog post revealing this campaign. "This payload repackages and republishes maintainer projects, enabling the malware to <strong>spread laterally across related packages</strong> without direct developer involvement."<br><br>To be clear: This campaign is distinct from the incident that we covered on Sept. 9, which saw <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/cyber-security/javascript-packages-with-billions-of-downloads-were-injected-with-malicious-code-in-worlds-largest-supply-chain-hack-geared-to-steal-crypto-a-phishing-email-is-all-it-took-to-undermine-npm-packages" target="_blank">multiple npm packages</a> with billions of weekly downloads compromised in a bid to steal cryptocurrency. The ecosystem is the same — attackers have clearly realized the GitHub-owned npm package registry for the Node.js ecosystem is a valuable target—but whoever's behind the Shai-Hulud campaign is after more than just some Bitcoin.<br><br>"The injected script performs <strong>credential harvesting and persistence operations</strong>," Koi Security said. "It runs TruffleHog to scan local filesystems and repositories for secrets, including npm tokens, GitHub credentials, and cloud access keys for [Amazon Web Services], [Google Cloud Platform], and Azure. It also writes a hidden GitHub Actions workflow file (.github/workflows/shai-hulud-workflow.yml) that exfiltrates secrets during CI/CD runs, ensuring long-term access even after the initial infection. This dual focus on <strong>endpoint secret theft and backdoors</strong> makes Shai-Hulud one of the most dangerous campaigns ever compared to previous compromises."<br><br>That might be confusing to anyone who doesn't have to worry about developing and distributing Node.js software. But the long and short of it is that Shai-Hulud is using a well-known offensive security tool (TruffleHog) alongside developer tooling (GitHub Actions) in an environment that is designed specifically to help distribute software without much developer involvement (npm).<br><br>We suggested in our previous report that whoever compromised the npm packages to steal cryptocurrency did us a favor, because they could have used their access to those packages to accomplish far worse attacks. Now it seems that someone is looking to do just that — and it's hard to feign surprise when the Node.js ecosystem and the tooling built around it were practically built to enable widespread attacks like this.<br><br>Koi Security is updating its blog post with a list of npm packages known to have been compromised via the Shai-Hulud campaign. StepSecurity has also <a data-analytics-id="inline-link" href="https://www.stepsecurity.io/blog/ctrl-tinycolor-and-40-npm-packages-compromised#immediate-actions-required" target="_blank">published</a> indicators of compromise alongside a technical breakdown of how the malware spreads, what it does, and how organizations should respond if they discover that a compromised package has been used somewhere in their infrastructure.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><u><em>Tom's Hardware on Google News</em></u></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><u><em>add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/cyber-security/shai-hulud-malware-campaign-dubbed-the-largest-and-most-dangerous-npm-supply-chain-compromise-in-history-hundreds-of-javascript-packages-affected
Security researchers are tracking a malware campaign that has compromised hundreds of packages distributed via the npm ecosystem.
daBToSYGWTwwSJGUupnwkm
Thu, 18 Sep 2025 09:42:31 +0000 Cybersecurity
Tech Industry
Nathaniel Mott
Curly_photo / Getty
Cyberattack concept
Cyberattack concept
<![CDATA[ Logitech's next gaming mouse will have haptic-based clicks, adjustable actuation, and rapid trigger — new G Pro X2 Superstrike will land at $180 ]]>
<p>Logitech's latest addition to its ultra-light wireless gaming mouse lineup has something no other mouse has: an "innovative blend of inductive analog sensing and real-time click haptics." What this means is that Logitech's new G Pro X2 Superstrike mouse will feature an analog system that allows you to adjust the point at which your mouse switches actuate <em>and </em>still get the real-time feedback of a physical "click," thanks to haptics. <br><br>Logitech's new mouse will feature its new "Superstrike" technology, which involves a "bespoke Haptic Inductive Trigger System (HITS)" that "combines adjustable actuation point and rapid trigger capabilities with an innovative haptics system." <br><br>While we've seen analog switches and adjustable actuation a lot in the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-keyboards/best-gaming-keyboards">best gaming keyboards</a>, this is the first time we'll see it in a gaming mouse. This is because gamers <em>want </em>their mice to click exactly when the button actuates, and you can't have this without something like HITS, which changes the "click" to match the new actuation point. <br><br>According to Logitech, the mouse will feature 10 selectable actuation steps and five rapid trigger reset levels over 0.6mm of click travel, which seems like a fairly detailed amount of fine-tuning. But the brand does point out that the mouse is designed for professional athletes, so this makes sense. <br><br>Haptic Inductive Trigger System aside, the Pro X2 Superstrike will feature Logitech's latest Hero 2 optical sensor, which has a maximum resolution of 44,000 DPI and a maximum speed of 888 IPS and can handle up to 88 G's of acceleration. (This is the same sensor found in <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-mice/logitech-g-pro-x-superlight-2-dex-review">Logitech's G Pro X
Superlight 2 Dex</a>.) <br><br>It will also feature up to an 8,000 Hz polling rate and offer up to 90 hours of battery life, albeit not at the same time. Specs-wise, the mouse will weigh 65g (2.29oz) and measure 4.92 x 2.5 x 1.57 inches (125 x 63.5 x 40mm), which makes it identical to the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/logitech-g-pro-x-superlight-2">Logitech G Pro X Superlight 2</a> in size and shape and about 5g heavier in weight. <br><br>Logitech says the Pro X2 Superstrike will hit shelves in Q1 of 2026 for $179.99, so we'll probably see it showcased at CES 2026.</p>
https://www.tomshardware.com/peripherals/gaming-mice/logitechs-next-gaming-mouse-will-have-haptic-based-clicks-adjustable-actuation-and-rapid-trigger-new-g-pro-x2-superstrike-will-land-at-usd180
Logitech's latest addition to its ultra-light wireless gaming mouse lineup has something no other mouse has: an "innovative blend of inductive analog sensing and real-time click haptics."
URHBBM5rMRtWw9JhRL7kDi
Thu, 18 Sep 2025 09:00:00 +0000 Gaming Mice
Peripherals
Mice
Sarah Jacobsson Purewal
Logitech
logitech superstrike mouse
logitech superstrike mouse
<![CDATA[ PlayStation 5 Digital Edition with 1TB SSD downgraded to 825GB listed at the same price — CFI-2116 revision emerges overseas on Amazon ]]>
<p>The rumors (via <a data-analytics-id="inline-link" href="https://x.com/billbil_kun/status/1967899998382952566?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1967899998382952566%7Ctwgr%5E85f97f0baf6bbfa74ab49ae3fe800cdd6accf54c%7Ctwcon%5Es1_c10&ref_url=https%3A%2F%2Fkotaku.com%2Fsony-playstation-5-digital-slim-storage-price-2000622679">billbil-kun</a>) regarding Sony's revision of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/playstation-5-sony-ps5">PlayStation 5 </a>Digital Edition have been confirmed through various overseas Amazon listings. Although the new CFI-2116 revision maintains the original pricing, Sony has decreased the internal storage from 1TB to 825GB, representing a 17.5% reduction in capacity. This change further emphasizes the importance for owners to upgrade their consoles with one of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ps5-ssds">best PS5 SSDs.<br><br></a>The PlayStation 5 has been on the market for nearly five years, during which time the console has undergone multiple hardware revisions. These modifications have varied from minor optimizations, such as improvements in heatsink design, to substantial hardware alterations, including a die shrink, culminating in the transition to the PlayStation 5 Slim models. Consequently, the most recent CFI-2116 revision feels like a regression.<br><br>The original PlayStation 5, colloquially referred to now as the PlayStation 5 Fat, was equipped with 825GB of internal storage, of which approximately 650GB was accessible to the user, depending on system updates and other variables. The shift to the PlayStation 5 Slim introduced numerous enhancements, including an increase to 1TB of storage, providing the user with approximately 850GB of available space (subject to similar factors).<br><br>The CFI-2116 revision, also known as "Chassis E," marks the return of the 825GB SSD, which Sony advertises on the new packaging. Consumers are losing close to 200GB, or 24%, of usable, high-speed storage with the latest revision. You could argue that 200GB isn't a lot, and that's true in a way since some AAA titles — specifically, <em>Call of Duty: Black Ops Cold War —</em> are pushing over 300GB of installed size. But under normal circumstances, 200GB should be enough for one or two games.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1500px;"><p class="vanilla-image-block" style="padding-top:48.20%;"><img id="Mvu4VYuhS7ziGvZMKZKXb5" name="61h7VjYt-fL._AC_SL1500_ (1)" alt="PlayStation 5 Digital Edition" src="https://cdn.mos.cms.futurecdn.net/Mvu4VYuhS7ziGvZMKZKXb5.jpg" mos="" link="" align="" fullscreen="" width="1500" height="723" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Amazon Italy)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:459px;"><p class="vanilla-image-block" style="padding-top:154.68%;"><img id="8ZhCgh6rkTRDJ4ucLrcMGF" name="41gEsV574zL" alt="PlayStation 5 Digital Edition" src="https://cdn.mos.cms.futurecdn.net/8ZhCgh6rkTRDJ4ucLrcMGF.jpg" mos="" link="" align="" fullscreen="" width="459" height="710" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Amazon Italy)</span></figcaption></figure></div></div></div><p>The reason Sony has downgraded the internal storage in the CFI-2116 revision is unknown. If it weren't for past leaks or the Amazon listings, we wouldn't even know about the revision, since Sony hasn't officially announced it. However, given the current market situation, it's plausible that the downgrade could be a way for Sony to optimize production costs without resorting to another price hike. Sony already <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/console-gaming/sony-hikes-ps5-prices-by-usd50-starting-tomorrow-sony-adds-up-to-10-percent-to-the-price-of-every-model-from-august-21">increased the pricing</a> for the different PlayStation 5 models by $50 last month, citing the "challenging economic environment." <br><br>This generation of gaming consoles is the first to experience <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/console-gaming/gaming-consoles-are-becoming-more-expensive-as-they-age-for-the-first-time-in-history-gamers-blame-tariffs-for-playstation-xbox-and-switch-price-increases">price increases</a> rather than decreases. Despite this, Sony has sold over 77 million units as of May 2025, so the company is eager to keep up the momentum. Instead of raising prices again, Sony may have decided that reducing internal storage was a better way to keep prices stable.<br><br>Sony silently released the revised PlayStation 5 Digital Edition in Europe on Sept. 13. The console has already surfaced on <a data-analytics-id="inline-link" href="https://www.amazon.es/dp/B0FN7ZG39D">Amazon Spain</a>, <a data-analytics-id="inline-link" href="https://www.amazon.it/dp/B0FN7ZG39D">Amazon Italy</a>, <a data-analytics-id="inline-link" href="https://www.amazon.fr/dp/B0FN7ZG39D">Amazon France</a>, and <a data-analytics-id="inline-link" href="https://www.amazon.de/dp/B0FN7ZG39D">Amazon Germany</a>. It's available for purchase at Amazon Italy and Amazon Germany for €499, which aligns with the European MSRP for the existing PlayStation 5 Digital Edition (Chassis D). Amazon Germany has set a delivery date by October 23.<br><br>Initial rumors suggest that the CFI-2116 revision may be exclusive to the European market. However, it's plausible that the revised console could make its way to the U.S. market. Fortunately, the revision only affects the PlayStation 5 Digital Edition, so the disc version is safe. Nonetheless, if you're set on buying the digital edition, it might be a good time to pull the trigger, as there's no telling if or when the 1TB SKUs will disappear from the shelves to be replaced by the 825GB variant.<br><br><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/video-games/playstation/playstation-5-digital-edition-with-1tb-ssd-downgraded-to-825gb-listed-at-the-same-price-cfi-2116-revision-emerges-overseas-on-amazon
The new revision (CFI-2116) of the PlayStation 5 Digital Edition has been listed on Amazon Italy, Amazon France, and Amazon Germany at the same MSRP.
oV2NsfSYWNJxnc9ojG8jJH
Wed, 17 Sep 2025 18:31:41 +0000 PlayStation
Video Games
Console Gaming
Zhiye Liu
Amazon Italy
PlayStation 5 Digital Edition
PlayStation 5 Digital Edition
<![CDATA[ Alibaba’s AI chip goes head-to-head with Nvidia H20 in state-backed benchmark demo ]]>
<p>Alibaba’s semiconductor unit, T-Head, has reportedly <a data-analytics-id="inline-link" href="https://www.reuters.com/business/media-telecom/china-spotlights-major-data-centre-project-using-domestic-chips-2025-09-17/" target="_blank"><u>developed a new AI processor</u></a> that it claims matches the performance of Nvidia’s H20 — the GPU built specifically for the Chinese market that’s currently <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-h20-gpus-reportedly-caught-up-in-u-s-commerce-departments-worst-export-license-backlog-in-30-years-billions-of-dollars-worth-of-gpus-and-other-products-in-limbo-due-to-staffing-cuts-communication-issues"><u>stuck in geopolitical purgatory</u></a>. <br><br>The demonstration aired Tuesday, September 16, on China Central Television (CCTV), during a broadcast covering Premier Li Qiang’s visit to China Umicom’s Sanjiangyuan Energy Intelligent Computing Centre in Qinghai. In the segment, T-Head’s new “PPU” accelerator was directly compared with Nvidia’s H20 and A800, as well as Huawei’s Ascend 910B, with a chart implying performance parity between the Alibaba and Nvidia parts. <br><br>The chip, an ASIC designed for AI workloads, features 96 GB of HBM2e, 700 GB/s chip-to-chip interconnect, PCIe support, and 400 W board power, according to the on-screen specs as reported by <a data-analytics-id="inline-link" href="https://www.scmp.com/tech/tech-war/article/3325894/tech-war-alibaba-developed-ai-processor-par-nvidias-h20-chip-cctv-report-shows" target="_blank"><u><em>South China Morning Post</em></u></a>. While the broadcast didn’t disclose the specifics of the testing methodology used or publish raw figures, it’s the first public benchmark placing Alibaba’s hardware in the same class as Nvidia’s datacenter GPUs. <br><br>According to <em>Reuters</em>, China Unicom has already deployed 16,384 of Alibaba’s PPU cards across its infrastructure, accounting for more than half of the almost 23,000 domestic accelerators currently installed at the Qinghai facility. Together, the cards deliver 3,579 petaflops of compute, with the site expected to scale to more than 20,000 petaflops once all phases are complete.<br><br>There’s just as much geopolitical context behind the CCTV demonstration as there is technical. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/the-tale-of-nvidias-hgx-h20-how-an-ai-gpu-became-a-political-lightning-rod"><u>Nvidia’s H20</u></a> was introduced to comply with U.S. export controls limiting the sale of high-performance silicon to China. Built on Hopper architecture but cut down to meet restrictions, the H20 ships with 96 GB of HBM3 and roughly 4.0 TB/s of memory bandwidth. That lends some perspective to Alibaba’s matching 96 GB HBM2e capacity, though not necessarily its real-world performance. <br><br>The biggest unknown right now is on the software side. While Alibaba is understandably <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/china-bans-its-biggest-tech-companies-from-acquiring-nvidia-chips-says-report-beijing-claims-its-homegrown-ai-processors-now-match-h20-and-rtx-pro-6000d"><u>eager to show</u></a> it can meet AI hardware needs in-house, the company has not disclosed details about frameworks, toolchains, or compatibility with existing model stacks. Until independent benchmarks and developer support materialize, the PPU’s parity with Nvidia’s hardware is just a claim backed by Chinese state TV and endorsed by the Chinese government.<br><br><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><u><em> Tom's Hardware on Google News</em></u></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><u><em> add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/gpus/alibaba-ai-chip-goes-head-to-head-with-nvidia-h20
Alibaba’s semiconductor unit, T-Head, has reportedly developed a new AI processor that it claims matches the performance of Nvidia's H20 — the GPU built specifically for the Chinese market.
eTEwJwZgicad7UUJeZVdQU
Wed, 17 Sep 2025 18:02:57 +0000 GPUs
PC Components
lukejamesalden@gmail.com (Luke James)
Luke James
Getty/Bloomberg
Nvidia CEO Jensen Huang speaking to journalists in China.
Nvidia CEO Jensen Huang speaking to journalists in China.
<![CDATA[ MSI enters the US Electric Vehicle charger market with EV Life Series ]]>
<p>When I think of MSI, I think of<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/motherboards/msi-meg-x870e-godlike-motherboard-drops-at-an-eyewatering-usd1-099-unleashes-ddr5-9000-ram-five-m-2-slots-10-gbe-and-wi-fi-7-alongside-two-usb-4-0-40-gbps-ports"> <u>motherboards</u></a>,<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/msi-skips-rdna-4-and-will-not-manufacture-amd-radeon-9000-series-gpus"> <u>video cards</u></a>,<a data-analytics-id="inline-link" href="https://www.tomshardware.com/monitors/gaming-monitors/msis-new-500-hz-qd-oled-monitor-leverages-ai-tech-to-save-it-from-burn-in"> <u>gaming monitors</u></a>, and, more recently,<a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/handheld-gaming/msis-brings-amd-based-gaming-handheld-updated-mid-range-gaming-laptops-to-computex"> <u>PC gaming handhelds</u></a>. So, the thought of MSI entering the electric vehicle (EV) was a foreign concept to me. Unbeknownst to me, even as an enthusiast with two EVs, MSI has marketed EV chargers in other parts of the world for quite some time. However, the company is now ready to expand to North America with MSI's EV Life and EV Life Plus EV chargers.</p><p>The EV Life Series is available in four different models: you can opt for a SAE J1772 or NACS (Tesla) connector in NEMA 14-50 (think U.S. dryer outlet) or hardwired configurations. No matter which SKU you choose, you'll receive an incredibly long 24.6-foot, IP55-rated charging cable and 14.4kW/60A that will add between 43 and 59 miles of range per hour to the average EV (think Tesla Model 3 or Hyundai Ioniq 7). If you're driving something like a Chevrolet Silverado EV with a massive 200 kWh battery, you'll probably see those numbers halved.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.20%;"><img id="gt3mKdvxqy8ucyErnhWi8h" name="image1" alt="MSI EV Life Series" src="https://cdn.mos.cms.futurecdn.net/gt3mKdvxqy8ucyErnhWi8h.jpg" mos="" align="middle" fullscreen="" width="1920" height="1079" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: MSI)</span></figcaption></figure><p>When it comes to EVs, many owners like to geek out on charging stats and electricity running costs. With that in mind, the EV Life Series has built-in Bluetooth, which, when paired with the MSI aConnect app, provides a powerful tool for monitoring your EV and setting up scheduling routines. With aConnect, you can monitor current and historical charging times, how much you're saving by using electricity over a comparable gasoline- or diesel-powered vehicle, the total cost of the electricity you've pumped into your EV, and how much carbon emissions you've saved.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.20%;"><img id="kGYfASWF2L3JjNwwV42cxg" name="image2" alt="MSI EV Life Series" src="https://cdn.mos.cms.futurecdn.net/kGYfASWF2L3JjNwwV42cxg.jpg" mos="" align="middle" fullscreen="" width="1920" height="1079" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: MSI)</span></figcaption></figure><p>The EV Life Plus Series is in many ways similar to its lesser sibling. You'll find the same four connection options (NACS with NEMA 14-50 or hardwired, or SAE J1772 with NEMA 14-50 or hardwired). You also get the same 14.4KW/60A charging capabilities as on the EV Life. However, the EV Life Plus amps things up with RFID authentication support along with Wi-Fi and Ethernet connectivity. The latter two features allow you to monitor the charging progress of your vehicle from anywhere, instead of the short-range limitations of Bluetooth-only support.</p><p>The EV Life Plus Series also supports the OCPP 1.6J standard, which provides a secure, industry-standard communications protocol for charging. This helps avoid vendor lock-in through proprietary standards, which is why MSI's EV chargers can work not only with Tesla vehicles, which helped popularize the NACS connector, but also with vehicles that use the SAE J1772 connector.</p><p>The MSI EV Life with NACS or SAE J1772 connector is available for $449. If you want to connect to your home's grid with a NEMA 14-50 connection, the price increases to $499. The EV Life Plus starts at $549.99 for a hardwired connection with a NACS or SAE J1772 connector. You'll also pay a $50 premium for a NEMA 14-50 electrical hookup. The chargers are available directly from<a data-analytics-id="inline-link" href="https://us-store.msi.com/EV-Solution/EV-chargers"> <u>MSI</u></a> or from<a data-analytics-id="inline-link" href="https://www.amazon.com/stores/page/741AAEE3-21A0-46D6-9F0C-073BFB4E34DE"> <u>Amazon</u></a>. For comparison,<a data-analytics-id="inline-link" href="https://shop.tesla.com/product/wall-connector"> <u>Tesla's 11.5kW/48A Wall Connector is $420</u></a>.</p>
https://www.tomshardware.com/tech-industry/msi-enters-the-us-electric-vehicle-charger-market-with-ev-life-series
MSI's EV Life and EV Life Plus EV chargers support NACS and SAE J1772 vehicles.
Q8NGpFC6D8ZK7T4XeuKsyP
Wed, 17 Sep 2025 17:30:56 +0000 Tech Industry
brandon.hill@futurenet.com (Brandon Hill)
Brandon Hill
MSI
MSI EV Life Series
MSI EV Life Series
<![CDATA[ Exploring the RTX Pro 6000D, Nvidia's China-only GPU, which is now banned from sale — neutered specs cannot compete with grey-market chips ]]>
<p>Nvidia’s RTX 6000D was never going to be a hero product. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-preparing-rtx-6000d-for-chinese-market-to-comply-with-u-s-export-controls-fabricated-on-tsmc-n4-featuring-gddr7-memory-capable-of-delivering-1-100-gb-s-of-bidirectional-bandwidth">Built specifically for the Chinese market</a> to navigate U.S. export restrictions, it has a constrained design: a GDDR-based Blackwell GPU with no NVLink, targeting AI inference instead of full-scale training.</p><p>Two procurement sources speaking to the <a data-analytics-id="inline-link" href="https://www.scmp.com/tech/big-tech/article/3325740/nvidia-sees-tepid-demand-new-rtx6000d-ai-chip-chinese-tech-firms-sources"><em>South China Morning Post</em></a> say that demand is tepid and the chip’s value proposition is “expensive for what it does.” And that’s before you factor in the uncomfortable comparison to Nvidia’s own RTX 5090, the flagship gaming GPU that’s officially banned from China but widely available through grey-market channels. By some measure, that consumer card not only costs half as much but outperforms the 6000D in the same inference tasks Nvidia designed its export-safe silicon to run.</p><p>Shortly after this report, it was noted that <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/china-bans-its-biggest-tech-companies-from-acquiring-nvidia-chips-says-report-beijing-claims-its-homegrown-ai-processors-now-match-h20-and-rtx-pro-6000d">China has now banned its biggest tech companies from acquiring Nvidia chips</a>, which may be the real reason why 6000D interest was so tepid in the region. But there are still interesting observations to be made about the curious 6000D itself.</p><h2 id="blackwell-sans-bandwidth-2">Blackwell sans bandwidth</h2><p>Nvidia hasn’t published formal specifications for the RTX 6000D, but multiple sources indicate it uses <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-rtx-pro-6000d-b40-blackwell-gpus-reportedly-set-to-supersede-banned-h20-accelerators-in-china">Blackwell architecture with conventional GDDR memory</a>, and delivers around 1,398 GB/s of bandwidth — just under the 1.4 TB/s export limit. There are strong indications that it avoids HBM and high‑bandwidth interconnect packaging, suggesting simpler die structures and likely dependence on PCIe or external NICs, rather than NVLink.</p><p>In other words, the 6000D is a PCIe workstation card that looks a lot like the RTX 6000 Blackwell or a tweaked 5090, just with a different name and a dramatically higher price tag. But rather than specs, the core issue is what happens when you try to scale it.</p><p>Without NVLink, the 6000D may rely on PCIe or external NICs like ConnectX to communicate between GPUs. That puts it at an immediate disadvantage in large-model inference workloads. A 70B-parameter model at FP16 can easily require 140 GB or more just for weights. Even INT8 quantization struggles to fit under 50 GB once you add the KV cache, meaning that you’ll often need two or more GPUs just to serve a single model replica. At that point, GPU-to-GPU comms becomes the bottleneck.</p><p>PCIe 4.0 x16 delivers around 64 GB/s of bandwidth. Meanwhile, NVLink 5.0 is closer to 900 GB/s. Nvidia’s own documentation recommends keeping tensor parallelism inside the NVLink domain for exactly this reason — collective operations like all-reduce and activation exchange are latency-sensitive. Try doing that over PCIe or even 800Gbps Ethernet, and step time balloons. Add more 6000Ds to the cluster, and you can imagine how the payoff in throughput starts to collapse. From a buyer's perspective, what’s the point of doing that when you can do better with alternative hardware?</p><h2 id="grey-market-swarm-2">Grey-market swarm</h2><p>The RTX 6000D retailed in China for around $7,000 (¥50,000). That’s not far off the amount you might expect to pay for a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidias-next-gen-ai-chip-could-double-the-price-of-h20-if-china-export-is-approved-chinese-firms-still-consider-nvidias-b30a-a-good-deal">lower-end HBM-based GPU like the H20</a>, not a GDDR card with no NVLink. And unlike the H20, it’s not even pretending to be a training-class GPU.</p><p>Now compare that to what’s available unofficially. Grey-market RTX 5090s, which were designed for gamers but blessed with Nvidia’s full Blackwell silicon, are trading for as little as $3,500 (¥25,000). Some are even being <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidias-rtx-5090-gpus-with-blower-style-coolers-appear-in-china-design-optimizes-nvidias-fastest-gaming-gpus-for-use-in-ai-workloads">resold in blower-style enclosures</a> with expanded VRAM of up to 80GB or even 128GB in modded units. Despite being technically banned under U.S. export rules, they’re everywhere. And they perform.</p><p>So, why choose the 6000D when you can get a more powerful grey-market part without any issues? From a throughput-per-yuan perspective, the grey-market 5090 swarm makes the 6000D look like a bad joke. And with better options potentially being just over the horizon, why would Chinese buyers want to spend money on the 6000D at all?</p><h2 id="a-domestic-market-right-around-the-corner-2">A domestic market right around the corner?</h2><p>A big part of the 6000D’s lackluster reception is circumstantial. Chinese buyers were still waiting on shipments of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/the-tale-of-nvidias-hgx-h20-how-an-ai-gpu-became-a-political-lightning-rod">Nvidia’s sanctioned H20</a>, an HBM-based Hopper GPU approved for export in July but still stuck in limbo. It’s the chip many hyperscalers wanted for high-density inference, but the latest news of China's Nvidia ban calls into question whether those orders will even be fulfilled.</p><p>At the same time, there was a hope that the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-could-be-readying-b30a-accelerator-for-chinese-market-new-blackwell-chip-reportedly-beats-h20-and-even-h100-while-complying-with-u-s-export-controls">B30A</a> — a more powerful Blackwell part designed for training — would win approval for sale, too. However, with the new ban on acquiring Nvidia chips, this seems more unlikely than ever. The B30A was reportedly equipped with 144 GB of HBM3E and NVLink support, delivering up to six times the performance of the H20 for only double the price.</p><p>As evidenced by the Cyberspace Administration of China (CAC)'s latest actions, the country is clearly moving beyond reliance on Nvidia chips. There’s a deeper shift underway. China is pushing hard for domestic AI hardware adoption, mandating that state-backed clouds procure at least <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/china-mandates-domestic-firms-source-50-percent-of-chips-from-chinese-producers-beijing-continues-to-squeeze-companies-over-reliance-on-foreign-semiconductors">50% of their AI accelerators from Chinese vendors</a>. Huawei’s Ascend, Biren’s CloudMatrix, and Cambricon’s NPU lines are all on the table. So is CANN: Huawei’s CUDA alternative that just went fully open source.</p><p>This (in theory) allows developers to move workloads away from Nvidia and toward Ascend, but the transition has been rocky. Chinese LLM lab DeepSeek notoriously scrapped plans to train its next model on Ascend NPUs, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseek-reportedly-urged-by-chinese-authorities-to-train-new-model-on-huawei-hardware-after-multiple-failures-r2-training-to-switch-back-to-nvidia-hardware-while-ascend-gpus-handle-inference">much to the disdain of the Chinese government</a>, citing unstable performance and poor chip-to-chip communication.</p><p>For now, that leaves China’s major clouds effectively locked into CUDA. But the political and strategic pressure to break free is building. From China's perspective, there are too few advantages to justify doubling down on Nvidia’s ecosystem, which would be terminally behind Western counterparts, if export control rules continued.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em> Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/tech-industry/semiconductors/why-nobody-is-buying-nvidia-6000d-in-china
Nvidia's RTX 6000D might be banned from sale in China, but what lies inside the GPU itself? We explore why it's not as good a deal as grey-market RTX 5090s in the region.
SpJFSsot77AggpA5dYvRpU
Wed, 17 Sep 2025 17:17:05 +0000 Semiconductors
Tech Industry
Manufacturing
lukejamesalden@gmail.com (Luke James)
Luke James
Nvidia
Nvidia RTX 6000
Nvidia RTX 6000
<![CDATA[ These must-have accessories helped me power through my overseas trip to IFA 2025 ]]>
<p>I recently took a trip overseas to Germany for IFA 2025, which meant I had to bring some essential gear to keep my devices charged while on the go. These devices ranged from a multi-outlet USB-C wall adapter to a portable 25,000 mAh battery to a thin MagSafe battery for my iPhone to a Euro plug converter for keeping my devices charged in my hotel room in Berlin.</p><h3 class="article-body__section" id="section-ugreen-65-watt-retractable-usb-c-power-block"><span>Ugreen 65-watt Retractable USB-C Power Block</span></h3><p><strong>🧳 Ugreen USB-C Power Block</strong></p><p>I actually picked up both 45-watt and 65-watt Ugreen retractable USB-C power blocks during the last Amazon Prime Day event in July. I took the 65-watt version with me on my trip due to its higher power output.</p><div class="inlinegallery
mosaic-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="FQwGymMmGbnNEb5mhvWu9R" name="image5" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/FQwGymMmGbnNEb5mhvWu9R.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="zgcb6782EcBgUob5cfuDBR" name="image1" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/zgcb6782EcBgUob5cfuDBR.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>When using it to charge a single device, the retractable USB-C cable can deliver up to 60 watts. The USB-C port tops out at 60 watts, while the USB-A port doles out 22.5 watts. If you're charging two devices at once, either the retractable USB-C cable or the USB-C port can deliver a maximum of 45 watts, with the other topping out at 25 watts.</p><p>While in my hotel room, I used the Ugreen adapter to supply power to my 3-in-1 travel MagSafe charger via the retractable USB-C cable and to charge my MacBook Air with the USB-C port. Even with my MacBook Air, iPhone, Apple Watch, and AirPods Pro charging all at once, the adapter was just barely warm to the touch. The thermal performance is likely due to the Gallium Nitride (GaN) power transistors, which help improve efficiency and thus reduce heat output.</p><p>The<a data-analytics-id="inline-link" href="https://www.amazon.com/gp/product/B0DNSQCRJB"> <u>65-watt Green USB-C Power Bank</u></a> is currently on sale for $37.99. If you can get by with the lower-output<a data-analytics-id="inline-link" href="https://www.amazon.com/gp/product/B0DRP9HKKC"> <u>45-watt version</u></a>, it sells for $28.99.</p><h3 class="article-body__section" id="section-anker-737-power-bank"><span>Anker 737 Power Bank</span></h3><p><strong>🧳 Anker 737</strong></p><p>My Anker 737 is my go-to power source when flying, and I've had it for nearly two years at this point. The power bank features a 24,000 mAh internal battery that has enough juice to charge an iPhone 16 Pro from empty to full four times.</p><p>The Anker 737 has two USB-C ports, each of which can deliver up to 140 watts if just one device is attached. There's also a USB-A port that tops out at 18 watts. When you've fully depleted the power bank, if you have a 140-watt charger on hand, you can get it back to a 100 percent charge in 52 minutes.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="Y4D8fGbJagDGwmXRncqwDR" name="image3" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/Y4D8fGbJagDGwmXRncqwDR.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:75.04%;"><img id="SzvhHKwwx74V2Xdn8iMaAR" name="image4" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/SzvhHKwwx74V2Xdn8iMaAR.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1500" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>One of my favorite features of the power bank is the built-in OLED display, which provides information on the current charge capacity, the estimated time to deplete the battery based on the current output, and the wattage delivered to each port.</p><p>While crossing the Atlantic on my NYC to Berlin leg of my trip, I used the Anker 737 to charge my iPhone 16 Pro (not in use) and iPad Pro (as I binge-watched The Pitt). Granted, I could have used the power outlet near the floor, mounted on the seat in front of me. However, since I was in the aisle seat, and the two passengers beside me kept getting up to use the bathroom, which would have required me to keep unplugging to let them pass. With the Anker 737, I just set the battery beside me in my seat. It's also a lot easier than fumbling, trying to find the seat-mounted power outlet in the dark.</p><p>The Anker 737 is<a data-analytics-id="inline-link" href="https://www.amazon.com/Anker-PowerCore-Portable-Charger-Compatible/dp/B09VPHVT2Z/"> <u>currently priced at $87.99</u></a>, or 20 percent off its MSRP of $109.99.</p><h3 class="article-body__section" id="section-baseus-magsafe-portable-charger-for-iphone-10-000mah-20w-magnetic-power-bank"><span>Baseus MagSafe Portable Charger for iPhone, 10,000mAh 20W Magnetic Power Bank</span></h3><p><strong>🧳 Baseus MagSafe Portable Chargerk</strong></p><p>When on the ground in Berlin, I was in and out of meetings, in and out of Ubers, and walking around the show floor without easy access to power. It's easy to run through my phone's battery when taking tons of pictures, uploading those images to the cloud, and recording interviews for execs.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="Q9cvoTamgXCYF5eTu7wX7R" name="image6" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/Q9cvoTamgXCYF5eTu7wX7R.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="t7pR42fJ82iE9rSYGqWX7R" name="image2" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/t7pR42fJ82iE9rSYGqWX7R.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.23%;"><img id="4oTgfnyVuhsp9hufB4PW7R" name="image8" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/4oTgfnyVuhsp9hufB4PW7R.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1124" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>As a result, I use a Baseus 10,000 mAh 20W MagSafe battery. It magnetically attaches to the back of my phone, doubling its thickness. Despite the added heft, I still have no trouble fitting it in my front pants pocket. It has enough capacity to provide a 0-100 percent charge (and a bit more) on my iPhone 16 Pro. The battery pack recharges via its USB-C port. You can also recharge your phone or another device via the USB-C port at up to 20 watts if you don't want to bother with the MagSafe function.</p><p>I bought my Baseus MagSafe battery charger on clearance from<a data-analytics-id="inline-link" href="https://sellout.woot.com/offers/baseus-magsafe-10000mah-20w-power-bank"> <u>Woot.com for $18.99</u></a>. However, a newer, 22.5-watt version of the device is<a data-analytics-id="inline-link" href="https://www.amazon.com/Baseus-Portable-10000mAh-Wireless-Compatible/dp/B0DZWVN6GX"> <u>currently available from Amazon for $26.99</u></a>.</p><h3 class="article-body__section" id="section-vintar-international-power-plug-adapter"><span>VINTAR International Power Plug Adapter</span></h3><p><strong>🧳 Vintar Power Plug Adapter</strong></p><p>All of my U.S. plugs are useless in Europe without a travel plug adapter. I previously bought a VINTAR 2-pack of Euro travel adapters for a family vacation to Greece last year, and took one along for my trip to Berlin.</p><div class="inlinegallery
mosaic-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="KGxASjBnxZoxR5QRmkW39R" name="image7" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/KGxASjBnxZoxR5QRmkW39R.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1999px;"><p class="vanilla-image-block" style="padding-top:56.28%;"><img id="ouoCKHwWTSSpNazsYrbnAR" name="image9" alt="Travel Tech" src="https://cdn.mos.cms.futurecdn.net/ouoCKHwWTSSpNazsYrbnAR.jpg" mos="" link="" align="" fullscreen="" width="1999" height="1125" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The plug is quite versatile, offering two U.S.-style outlets, three USB-A ports, and one USB-C port for your devices. The plug is sturdy and doesn't feature moving parts, making it less susceptible to breaking from continual use and being tossed in my carry-on bag.</p><p>The VINTAR European Travel Plug Adapter is available in a<a data-analytics-id="inline-link" href="https://docs.google.com/document/d/1dKE0okuJ9y2nEjas3J5U2rJF5EPG4MibXPYEiqDBoaM/edit?tab=t.0"> </a><a data-analytics-id="inline-link" href="https://www.amazon.com/European-VINTAR-International-Compatible-American/dp/B07WRWX15J"><u>two-pack for $19.99 at Amazon</u></a>.</p>
https://www.tomshardware.com/peripherals/cables-connectors/these-must-have-accessories-helped-me-power-through-my-overseas-trip-to-ifa-2025
From MagSafe batteries to a retractable cable USB-C charger, here’s what’s in my overseas travel bag.
kDaHzRL7PR2egyoQqa6mjG
Wed, 17 Sep 2025 17:10:11 +0000 Cables and Connectors
Peripherals
brandon.hill@futurenet.com (Brandon Hill)
Brandon Hill
Tom&#039;s Hardware
Travel Tech
Travel Tech
<![CDATA[ This upcoming Thunderbolt 5 eGPU dock lets you mount an entire mini-PC on the side — also features aftermarket ATX and SFX power supply support ]]>
<p>Mini-PC and eGPU maker Aoostar has added yet another eGPU dock to its arsenal of products. On <a data-analytics-id="inline-link" href="https://www.reddit.com/r/eGPU/comments/1nhzw0s/aoostar_ag02_vs_wait_for_aoostar_eg01/" target="_blank">Reddit</a>, the company announced the EG01, a Thunderbolt 5 graphics card dock that supports full-size desktop graphics cards, ATX/SFX power supplies, and features an optional mini-PC holder on top. Pricing and a release date have yet to be disclosed.</p><p>Not much information has been publicly revealed about the dock; however, <a data-analytics-id="inline-link" href="https://www.notebookcheck.net/Aoostar-EG01-Thunderbolt-5-and-OCuLink-eGPU-dock-revealed-globally-with-mini-PC-mount.1116086.0.html" target="_blank">Notebookcheck</a> was able to get some in-depth information about the EG01. But the outlet was able to find that the dock allegedly takes advantage of a Thunderbolt 5 interface and is compatible with OCuLink, making it one of the first docks to feature both connectivity standards. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/thunderbolt-5-debuts-120-gbps-speed-is-three-times-faster-than-previous-gen">Thunderbolt 5</a> and OCuLink provide a PCIe 4.0 x4 interface from the host system to the graphics card, offering the best connectivity you'll see on eGPU docks right now.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="wKBkGKqE6saJwuuoGBeDSZ" name="Aoostar EG01 eGPU dock" alt="Aoostar EG01 eGPU Dock" src="https://cdn.mos.cms.futurecdn.net/wKBkGKqE6saJwuuoGBeDSZ.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Aoostar)</span></figcaption></figure><p>The EG01 differs significantly from most of Aoostar's other eGPU docks, featuring full compatibility for desktop GPUs, but having no embedded PSU, instead having a mount for aftermarket units. For those who want to upgrade power supplies down the road, this is a great feature and allows the user to choose whatever ATX or SFX PSU they want to use, significantly increasing the dock's flexibility. Past iterations of Aoostar docks have featured embedded power supplies, ranging from 400W to 800W of power output. Besides its 800W offerings, Aoostar's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/aoostar-ag01-egpu-with-oculink-and-built-in-400w-psu-released-at-dollar150">older trims,</a> sporting 400W or <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/docking-stations-hubs/aoostar-ag02-egpu-dock-with-oculink-support-and-500w-psu-announced-for-usd219">500W</a> power supplies, would limit GPU power draw to a max of 250W or 350W, respectively, limiting GPU options to AMD or Nvidia's mid-range desktop graphics cards.</p><p>With the EG01, you have the option of building a GPU setup that only needs enough power for what you need in the current moment, with an upgrade path down the road. If you find you want to upgrade to a more power-hungry GPU in the future, you can easily swap out the PSU with a more potent unit if necessary.</p><p>The dock can be used with any Thunderbolt or OCuLink capable device; however, the dock has also been designed with mini-PCs in mind. The power supply bracket includes an optional mini-PC mounting solution on the top, turning the eGPU dock into a computer that mimics a mini-ITX PC. This will inevitably be a highly used feature for those who plan on daily driving the EG01 with a mini-PC or NUC-like device rather than a laptop or handheld gaming PC.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/desktops/mini-pcs/this-upcoming-thunderbolt-5-egpu-dock-lets-you-mount-an-entire-mini-pc-on-the-side-also-features-aftermarket-atx-and-sfx-power-supply-support
Aoostar has announced an upcoming eGPU dock that features Thunderbolt 5 connectivity and OCuLink connectivity, plus support for aftermarket ATX/SFX power supplies. The cherry on top is the included mini-PC mount for mini-PC daily drivers.
4ThCooLt9dGFS5h6rhyT8b
Wed, 17 Sep 2025 16:52:46 +0000 Mini PCs
Desktops
editors@tomshardware.com (Aaron Klotz)
Aaron Klotz
Aoostar
Aoostar EG01 eGPU Dock
Aoostar EG01 eGPU Dock
<![CDATA[ Modern memory is still vulnerable to Rowhammer vulnerabilities — Phoenix root privilege escalation attack proves that Rowhammer still smashes DDR5 security to bits ]]>
<p>Scientists from the Computer Security Group (COMSEC) at the ETH Zürich college, in conjunction with Google, have published a proof-of-concept attack on DDR5 RAM called <a data-analytics-id="inline-link" href="https://comsec.ethz.ch/research/dram/phoenix/" target="_blank">Phoenix</a>, with the CVE number 2025-6202. The new attack causes bit-flips in memory, leading to a set of vulnerabilities that includes high-level privilege escalation. Phoenix adeptly bypasses DDR5's preventive measures for<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/new-rowhammer-attack-silently-corrupts-ai-models-on-gddr6-nvidia-cards-gpuhammer-attack-drops-ai-accuracy-from-80-percent-to-0-1-percent-on-rtx-a6000"> Rowhammer-style attacks</a>, and neither ECC nor ODECC (on-die ECC) are of help.</p><p>It's worth noting that COMSEC only tested the attacks on an AMD Zen 4 platform, against 15 SK hynix DDR5 DIMMs from 2021-2024. The team states it chose the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/sk-hynix-dethrones-samsung-to-become-worlds-top-selling-memory-maker-for-the-first-time-success-mostly-attributed-to-its-hbm3-dominance-for-nvidias-ai-gpus">largest DRAM vendor</a>, as the analysis is time-intensive even with the help of dedicated FPGA test boards. Having said that, the research is part of a <a data-analytics-id="inline-link" href="https://security.googleblog.com/2025/09/supporting-rowhammer-research-to.html" target="_blank">Google-led effort</a> for better RAM security in cooperation with JEDEC, the consortium that defines memory standards. It's also not the first time that COMSEC has worked on RAM security, having previously cooperated with VUSec to create the <a data-analytics-id="inline-link" href="https://www.vusec.net/projects/trrespass/" target="_blank">TRRespass attack</a>.</p><p>Phoenix is a mix-up and evolution of existing Rowhammer-style attacks that repeatedly "hammer" a set of RAM locations with reads, in a specific pattern, in a bid to force at least one bit to flip via electromagnetic interference. This allows for extracting data or modifying code to an attacker's preference. The scenario is concerning enough in desktops and workstations, but particularly worrying in large-scale servers hosting thousands of clients. You can download the proof-of-concept software at <a data-analytics-id="inline-link" href="https://github.com/comsec-group/phoenix" target="_blank">COMSEC's Phoenix GitHub repository</a> if you want to test your systems.</p><p>Phoenix's creators tested specific scenarios and had a 100% success rate in replicating attacks that manipulate Page Table Entries (PTE), granting access to forbidden locations in memory; a 73% chance of extracting the SSH login keys from a virtual machine in the same server; and a 33% probability of straight up getting root access thanks to manipulating the in-memory binary for the <strong>sudo</strong> utility. The team replicated the privilege escalation scenario in the <a data-analytics-id="inline-link" href="https://github.com/comsec-group/rubicon" target="_blank">Rubicon suite</a> in only 5 minutes and 19 seconds flat. COMSEC's researchers revealed their findings past June 6 to SK hynix, CPU vendors, and the major cloud platforms, and will publish their findings at the IEEE Security & Privacy 2026 conference.</p><p>There's no bulletproof mitigation for this issue yet — at least for the tested SK hynix DIMMs — but the researchers state that increasing the row refresh rate (tREFI) in the machine's UEFI by 3 times down to around 1.3 μs makes the attacks unlikely to succeed. However, that comes at a steep cost, as a benchmark with the SPEC CPU2017 suite revealed a nasty 8.4% performance hit. COMSEC says there's an impending BIOS update for AMD client systems to address this problem, but couldn't verify its effectiveness as of the date of its publication.</p><p><a data-analytics-id="inline-link" href="https://security.googleblog.com/2025/09/supporting-rowhammer-research-to.html" target="_blank">Google points out in a related blog post</a> that DDR5's TRR (Target Row Refresh) and ECC/ODECC can't quite fix the problem as they're not deterministic. For example, TRR's mechanism of triggering a refresh of a memory row doesn't keep an exact count of the number of accesses to it, making it easy to exploit by increasing the attack surface. Meanwhile, even the mighty ODECC only corrects bit-flips when data is written, or after a certain time (usually hours), meaning that just keeping an attack going for a long while is enough.</p><p>Those circumstances led to the creation of the PRAC (Per-Row Activation Counting) JEDEC standard, <a data-analytics-id="inline-link" href="https://www.jedec.org/news/pressreleases/jedec-updates-jesd79-5c-ddr5-sdram-standard-elevating-performance-and-security" target="_blank">first announced</a> in April 2024 for a future DD5 revision. PRAC keeps an accurate count of sequential accesses to a memory row and alerts the host system if a limit is exceeded, so that mitigation measures (likely a refresh) are implemented. Predictably, the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/dram/jedec-publishes-first-lpddr6-standard-new-interface-promises-double-the-effective-bandwidth-of-current-gen">upcoming LPDDR6 standard</a> is integrating PRAC from the get-go. Here's to hoping the new feature will finally put a stake through Rowhammer's heart.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/tech-industry/cyber-security/modern-memory-is-still-vulnerable-to-rowhammer-vulnerabilities-phoenix-root-privilege-escalation-attack-proves-that-rowhammer-still-smashes-ddr5-security-to-bits
A new attack on DDR5 further demonstrates that current countermeasures against Rowhammer-style assaults aren't enough.
bQRwTZSjALkiVeM5QEDxVj
Wed, 17 Sep 2025 16:40:53 +0000 Cyber Security
Tech Industry
Bruno Ferreira
Getty Images
A hammer smashing a laptop computer.
A hammer smashing a laptop computer.
<![CDATA[ I got excited for the idea of sub-$1,000 gaming laptops with integrated graphics — but there are more than a few reasons why that's probably not happening ]]>
<p>There's been a trend the last two or three years in gaming laptops (and elsewhere) that no one likes: Prices are going up. It hasn't been surprising to see systems with the most powerful graphics, along with high-refresh displays, mechanical keyboards, or tons of RAM, to cost anywhere between $3,000 and $5,000, and more. See some of our most powerful picks like the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/gaming-laptops/msi-titan-18-hx-ai-review"><u>MSI Titan 18 HX AI</u></a> and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/gaming-laptops/razer-blade-18-review"><u>Razer Blade 18</u></a>.</p><p>But that same thing has been happening on the low end. Systems that used to be $999 or less are now often at least $1,100. Those laptops often use older processors and the lowest-end current GPUs.</p><p>Up until Lenovo announced that its Legion Go 2 handheld would start at $1,049, I was thinking that handhelds might replace the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-gaming-laptops-under-1000"><u>best gaming laptops under $1,000</u></a>.</p><p>We're in a place where it feels like we need something new to broaden what's available. Could gamers get a cheaper portable rig if they were willing to get handheld-style performance with integrated graphics?</p><p>That might not sound so appealing, but the best thing for PC gamers is to have options, including gaming laptops with discrete GPUs at $1,000 or less if that is the best they can afford. But with an increase in integrated GPU power that we've seen in everything from laptop chips to handheld <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/apu-accelerated-processing-unit-definition,37645.html"><u>APUs</u></a> to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amds-game-changing-strix-halo-apu-formerly-ryzen-ai-max-poses-for-new-die-shots"><u>Strix Halo</u></a>, along with the economies of scale that already build budget gaming PCs, could we finally see a new low-end gaming laptop with an iGPU?</p><p>It’s a nice idea, but the more I thought it through with my colleagues, the more quickly my dreams were dashed.</p><h2 id="there-s-precedent-but-it-makes-more-sense-now-2">There's precedent, but it makes more sense now</h2><p>Back in 2021, Adata released the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/xpg-xenia-xe"><u>XPG Xenia Xe</u></a>. It was a whitebox system from Intel, but more importantly, it used Intel's Core i7-1165G7 CPU with integrated Intel Iris Xe graphics. Still, Adata referred to it as a "gaming lifestyle notebook.” Its predecessor, the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/xpg-xenia-15"><u>Xenia 15</u></a>, had used a GTX 1660 Ti. We saw a similar idea in <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/alienware-concept-ufo-gaming-handheld-hands-on"><u>Alienware's Concept UFO</u></a>, which used a 10th Gen Intel CPU with integrated graphics to power the gaming handheld, but that never turned into a real product.</p><p>I scoffed at the idea. My colleague at the time, Michelle Ehrhardt, titled her review of the system "expensive and unbalanced." It was $1,600. She was right.</p><p>But what if the Xenia Xe hadn't been designed to be premium? I could see a version of that, using today's chips, making a new kind of low-end gaming laptop.</p><p>Imagine an ultraportable-sized system, perhaps with a 14-inch, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/what-is-fhd-full-hd,5741.html"><u>1080p</u></a> display up to 120 Hz, using something like the AMD Ryzen Z2 Extreme (or its most equivalent laptop part that exists). Other than some beefed up cooling, designs using largely plastic chassis that could work are probably already sitting on shelves.</p><h2 id="what-would-make-that-gaming-2">What would make that gaming?</h2><p>On the other hand, there are laptops out there now with chips using AMD's Radeon 890M, though they're generally in premium <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-ultrabooks-premium-laptops"><u>ultrabooks</u></a>. People do play games on those, the same way Apple's entire product line doesn't include a dedicated "gaming" laptop. There's no reason you couldn't use one of them, but they're probably priced higher than I'm thinking.</p><p>So, now we have a theoretical plastic laptop with an otherwise strong chip that could, generously, play games at 1080p on medium settings. Hopefully, that could make for something affordable, even if it's not powerful.</p><p>With handhelds, gamers accept that lack of power because they get portability. Gamers expect portability from laptops already. So what would make
something like this a gaming laptop? What could give that <em>value?</em></p><p>For starters, I would want to see an OS focused on gaming. What if it was officially licensed to run SteamOS, and you could use Arch Linux for productivity? Or perhaps it could run Windows 11's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/handheld-gaming/microsoft-focusing-on-handheld-gaming-support-with-new-xbox-compact-mode"><u>upcoming handheld gaming mode</u></a> that will debut on the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/handheld-gaming/asus-rog-xbox-ally-and-xbox-ally-x-to-launch-october-16-co-branded-handhelds-sport-new-cpus-game-friendly-windows-tweaks-but-pricing-is-still-unknown"><u>Asus ROG Xbox Ally</u></a>. In exchange for power, get rid of bloatware and give people a gaming-focused experience. Using one of these would also allow for game validation, like what the Steam Deck and upcoming Xbox Ally will offer, so you can get an idea if games will run. But honestly, these should be options on high-end machines, too.</p><p>The laptop companies could also team up with Nvidia or Microsoft to get lengthy trial subscriptions to streaming services for games that may have trouble running well on integrated graphics.</p><p>Lastly, if you're not paying for a discrete GPU, maybe toss a mechanical keyboard in there. It's not unprecedented. The Dell G16 previously had cheap configurations with a Cherry keyboard, and that was a great value-add (the Dell Gaming lineup has since been discontinued).</p><p>There's also the question of which companies might be bold enough to put their gaming brand on a laptop without a discrete GPU.</p><h2 id="counterpoint-the-market-is-complex-2">Counterpoint: The market is complex</h2><p>But enough brainstorming. When you start to think about how things really work, this idea is harder than it seems.</p><p>Take my thought that companies could stick a chip with strong integrated graphics in some existing system's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/pc-chassis-definition,37651.html"><u>chassis</u></a>. They probably could, to some degree, but more cooling would still be helpful. And existing gaming laptop designs are built around the idea that a dedicated GPU is there, so tweaking that would require more tooling. Additionally, using an existing gaming chassis defeats the idea that you could get a slimmer laptop if you don't have a GPU to cool.</p><p>Next up is that in most systems, the companies that make gaming laptops can pair the CPU and GPU they want together. That's why we're seeing so many RTX 50-series laptops with 13th Gen Intel Core CPUs — those CPUs are fast <em>enough</em>, and they're likely cheaper. Companies can mix and match to hit whatever price point they want. But APUs using graphics like Radeon 890M are the highest-end parts, paired with high-end CPU cores, and they're sold at a premium.</p><p>If a laptop company wanted to make something like what I described, they might have to go to Intel or AMD and ask for something custom, and that would require a big order. (This is also what happened with the Steam Deck. Valve got a custom chip.)</p><p>Otherwise, we're hoping Intel or AMD come around and make an SoC with a "good enough" CPU but the best GPU cores on the market. That doesn't seem likely, especially with limited fab capacity and a demand for higher-margin parts.</p><p>In an ideal world, maybe one day, the companies that make these systems could bring costs back down. But realistically, I don't see that happening. That makes each gaming laptop below $1,000 a rare bird these days. And while an RTX xx50-class mobile GPU might not excite you, it's all some people can afford, especially as prices creep up.</p><p>Despite the hardships of doing it cheaply, we are already seeing some steps down this road; they're just not cheap. After all, we saw AMD's Radeon 8060S with the Ryzen AI Max+ 395 in the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/desktops/gaming-pcs/framework-desktop-review"><u>Framework Desktop</u></a>, as well as in the Asus ROG Flow Z13. The latter is technically a laptop (well, tablet) and is selling <a data-analytics-id="inline-link" href="https://www.bestbuy.com/product/asus-rog-flow-z13-13-4-2-5k-180hz-touch-screen-gaming-laptop-copilot-pc-amd-ryzen-ai-max-395-64gb-ram-1tb-ssd-off-black/JJGGLHG8X9"><u>for $2,400 on Best Buy</u></a>. Admittedly, it would be easy to drop the price by using something without Strix Halo and 64GB of RAM. But even then, it’s probably not getting close to the sub $1,000 mark, at least until it goes on clearance.</p><p>And companies that make gaming handhelds have been <em>raising</em> prices. If those devices sell, no company is going to have much incentive to put those in laptops at a lower price.</p><h2 id="almost-but-not-quite-there-2">Almost, but not quite, there</h2><p>I think we're a lot closer to the idea of laptop gaming on integrated graphics than we've ever been. (You can argue it's been happening for a long time! People who want to game will find the means to play on any system they have.)</p><p>On the other hand, people buy gaming laptops because they want to play games – usually modern AAA titles. Some of the initial handheld chips, like the Steam Deck's Aerith, are showing their age. This happens to all systems, eventually.</p><p>That might just be the way games are now, with less optimization and more graphical capabilities (Nvidia and AMD are trying to sell their high-end GPUs, after all). But to call a laptop a gaming laptop, it really needs to play all of the games, at least for a while after launch. So maybe a few more iterations will be required before this idea is ready for prime time.</p><p>But it also means we need to wait for great integrated graphics to get even cheaper for market forces to get in line. I think it would be great to see a true "gaming lifestyle notebook" that’s slim, powerful enough for most games, and ready with plenty of gaming features at the OS level. But with all of the factors making it tough on the low end, we'll have to settle for pricier Strix Halo experiments, at least for now.</p>
https://www.tomshardware.com/laptops/gaming-laptops/i-got-excited-for-the-idea-of-sub-usd1-000-gaming-laptops-with-integrated-graphics-but-there-are-more-than-a-few-reasons-why-thats-probably-not-happening
The prices of gaming laptops have been going up. Is it possible for laptops with integrated graphics to bridge the gap? It sounds like a good idea until you look into market realities.
DCjNGnQ2Hpd2Wb4onrsCPC
Wed, 17 Sep 2025 16:38:13 +0000 Gaming Laptops
Laptops
Andrew E. Freedman
Tom&#039;s Hardware, AMD
Sub-$1,000 gaming laptops with integrated graphics
Sub-$1,000 gaming laptops with integrated graphics
<![CDATA[ DOOM left running on ASUS MyPal PDA for 2.5 years finally crashes — bug that crashes the game when gametic value hits 2,147,483,647 ticks likely to blame ]]>
<p>Released in December of 1993 after an unbelievable amount of hype among PC gamers hooked up to the then-novel Internet, <em>DOOM</em> codified the standards of the nascent first-person shooter genre and was so popular that "<em>Doom</em> clone" was the way we described first-person shooters for years after its release. Gamers have put millions, if not billions of hours, into the title in the nearly 32 years since its launch, thanks to a virtual cornucopia of mods and user levels, but it's pretty unlikely that many of them have left the game running for upwards of two years straight. <a data-analytics-id="inline-link" href="https://lenowo.org/viewtopic.php?t=31" target="_blank">At least one person did</a>, though, and the result is... that it crashed.</p><p>Posting at LenOwO, site admin Minki remarks that they have reproduced the expected crash by loading<em> WinDOOM</em> on what appears to be an ASUS MyPal A620 pocket PC from 2003 running the then-novel Windows Mobile on an <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-builds-10nm-arm-chips,32501.html" target="_blank">Intel XScale ARMv5 SoC</a>. Minki says that the device was modified to use a "DIY 18650 [lithium cell] based UPS which was itself connected to the USB port of my router for a constant 5V supply." They left the system running and mostly forgot about it until yesterday, when they noticed a pop-up appearing on the device and complaining of an application crash:</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="sKBtPo78Th4NWPqyebDtuA" name="windoom-fatal-error-fhd-crop" alt="A cropped photo showing the WinDOOM crashing after over two years on Windows Mobile 2003." src="https://cdn.mos.cms.futurecdn.net/sKBtPo78Th4NWPqyebDtuA.png" mos="" align="middle" fullscreen="1" width="1920" height="1080" attribution="" endorsement="" class="expandable"></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">The crash, in the ASUS MyPal's Windows Mobile 2003 operating system. </span><span class="credit" itemprop="copyrightHolder">(Image credit: minki/LenOwO)</span></figcaption></figure><p>Like most source ports of the era, WinDOOM is based on the original source code release from 1997, and so it reproduces most features — and most bugs — of the original game. Like most large commercial software projects, <em>DOOM</em> has <a data-analytics-id="inline-link" href="https://doomwiki.org/wiki/Engine_bug" target="_blank">numerous known bugs</a> even in its final 1.9 release. Among them is a curious quirk where, when playing back "demo" files internally, usually for the game's "attract" loop, the "gametic" value does not reset upon starting a new demo playback. This value is used for tracking game timing for various purposes, and it increments at a rate of 35 Hz, or 35 times per second, independent of the game's render loop.</p><p>It doesn't take even high school-level math to figure out that the gametic value never resetting will eventually result in an enormous value over time. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/in-1991-after-a-28-hour-coding-spree-the-efforts-of-john-carmack-doomed-us-all" target="_blank">Principal <em>DOOM</em> engine coder John Carmack</a> was surely aware of this when he programmed it, but he likely reasoned that it simply didn't matter because the value is stored as a signed 32-bit integer. That means that it can reach a maximum value of 2,147,483,647 ticks before rolling over. Integer overflow behavior is undefined in C, but on x86 PCs it always results in a roll-over to the maximum negative value of -2,147,483,647. Unsurprisingly, the game doesn't handle this very gracefully, which is to say it crashes, at least on Windows Mobile 2003.</p><a href="https://github.com/chocolate-doom/chocolate-doom/issues/1287#issuecomment-636414423"><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:648px;"><p class="vanilla-image-block" style="padding-top:54.17%;"><img id="qCUbVy2Ac6u3H9fXxEdFCk" name="doom-dosbox-crash" alt="A screenshot of a more traditional DOOM crash in DOSBox." src="https://cdn.mos.cms.futurecdn.net/qCUbVy2Ac6u3H9fXxEdFCk.png" mos="" align="middle" fullscreen="1" width="648" height="351" attribution="" endorsement="" class="expandable"></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">Of course, there are many other ways to crash DOOM, such as loading an invalid level. </span><span class="credit" itemprop="copyrightHolder">(Image credit: GitHub/AXDOOMER)</span></figcaption></figure></a><p>At 35 ticks per second, it takes about 1.95 years to overflow the gametic value. That's a bit less than Minki's estimate, but who knows how long the ASUS PDA sat before they noticed the error message on screen; from the photo, it doesn't look like the 22-year-old pocket computer gets a lot of attention. It's also possible that Doom4CE, the Windows CE port of WinDoom that Minki was likely using, reduces the game tick rate to 30 Hz for better frame pacing and reduced hardware demands; this was common in the console ports of DOOM, such as the Jaguar and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/hack-snes-classic-add-games" target="_blank">Super NES versions</a>. If that's the case, it would take around 2.26 years to overflow the gametic value, closer to the stated 2.5.</p><p>Whatever the case, the takeaway is this: don't leave <em>DOOM</em> running for two years — or any game, probably, at least if it's a game client and not a dedicated server. Other thoughts provoked by Minki's experiment include both an appreciation for scientific rigor (experimental testing of even irrelevant conclusions) and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/3d-printing/this-3d-printer-was-repurposed-as-a-robotic-camera-and-it-doubles-as-a-photogrammetry-rig-for-3d-scanning" target="_blank">clever re-use of "junk" hardware</a>. That appears to be a theme of the Len0w0 boards, so kudos to that gang for doing what nobody else bothered to do.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/video-games/pc-gaming/doom-left-running-on-asus-mypal-pda-for-2-5-years-finally-crashes-bug-that-crashes-the-game-when-gametic-value-hits-2-147-483-647-ticks-likely-to-blame
Gamers have put millions of hours into DOOM in the 32 years since its launch, but it's unlikely that many of them have left the game running for upwards of two years straight. At least one person did, though, and the result is... that it crashed.
x6y4rhXBfFAtmock42JjkE
Wed, 17 Sep 2025 16:23:25 +0000 PC Gaming
Video Games
Zak Killian
Zenimax/Id Software
The title screen of the classic video game DOOM.
The title screen of the classic video game DOOM.
<![CDATA[ China's largest chipmaker testing first homegrown immersion DUV litho tool — SMIC takes significant step on road to wafer fab equipment self-sufficiency ]]>
<p>SMIC, the largest foundry in China, is test-driving one of China's first domestic immersion DUV lithography tools, reports <a data-analytics-id="inline-link" href="https://www.ft.com/content/8fd79522-e34f-4633-bc87-ef0aae2d9159">Financial Times</a>. The system was developed by Shanghai Yuliangsheng Technology Co., which is linked to Huawei's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/chinas-sicarrier-challenges-u-s-and-eu-with-full-spectrum-of-chipmaking-equipment-huawei-linked-firm-makes-an-impressive-debut">SiCarrier</a>, and is believed to be a significant part of China's effort to become self-sufficient in wafer fab equipment.</p><p>SMIC's test platform from Yuliangsheng involves a DUV machine that uses immersion lithography and is reportedly designed for 28nm-class fabrication technologies, though it could be used for 7nm or even 5nm production nodes by applying multipatterning. The Yuliangsheng tool is mostly made from components sourced within China, although some parts are still imported. The company is actively working to localize the entire supply chain. Once that is achieved (though it is unclear when), it would allow China to operate outside the influence of the U.S. or European export policies in this segment of chip production.</p><p>If the description of the tool by <em>Financial Times</em> is accurate, then the Yuliangsheng immersion DUV system currently being tested by SMIC resembles ASML's <a data-analytics-id="inline-link" href="https://www.asml.com/en/news/press-releases/2008/asml-launches-the-twinscan-nxt1950i-immersion-lithography-system">Twinscan NXT:1950i</a> from 2008, which was designed for 32nm-class process technology in one exposure. The unit featured optics with 1.35 numerical aperture, a 2.5nm overlay, a 38nm resolution, and could be used for making chips on a 22nm-class fabrication node. While theoretically the NXT:1950i could be used to make chips on 7nm and 5nm-class nodes, ASML has developed NXT:2000i for such fabrication technologies, which is generations ahead of the NXT:1950i.</p><p>It is unclear whether the Yuliangsheng tool is being tested within SMIC's production flow (i.e., they are producing actual chips or patterns) or if the company is just beginning to test the scanner and has merely reached first light on the wafer or first patterning milestones (a more likely scenario). If it is the latter, then the scanner is years away from mass-producing actual chips. Indeed, the goal is reportedly to integrate domestic immersion DUV lithography machines into production lines starting in 2027, after their qualification. Before that, SMIC will continue to rely on tools from ASML.</p><p>It should be noted that while SMIC (and probably Yuliangsheng) believes that it is possible to build chips on 7nm and 5nm-class process technologies on the same tools that are used for 28nm-class production nodes, it remains to be seen whether this is possible without a dramatic improvement of a 28nm-class tool when it comes to overlay performance, precision, control, and complexity. Essentially, after the existing tool matures and gets inserted into SMIC's 28nm flow in 2027, it <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/china-injects-tens-of-billions-of-dollars-in-chipmaking-tools-but-its-easily-more-than-a-decade-behind-the-market-leaders-heres-why">will take Yuliangsheng years to jump to 16nm and then to 7nm fabrication nodes</a> with a significantly redesigned scanner, so do not expect SMIC's sub-10nm fabrication processes on domestic lithography systems earlier than in 2030.</p><p>The tool is codenamed 'Mount Everest,' after the world's tallest mountain, perhaps highlighting the importance of the project. Interestingly, but SiCarrier also tends to call its WFE projects after mountains, which perhaps proves that SiCarrier and Shanghai Yuliangsheng Technology Co. are not only affiliated (SiCarrier is reportedly an investor of Yuliangsheng), but likely belong to the same group working on the same goal. It is noteworthy that Shanghai Yuliangsheng Technology Co. is already known to the U.S. Department of Commerce, which put it into its <a data-analytics-id="inline-link" href="https://www.federalregister.gov/documents/2024/12/05/2024-28267/additions-and-modifications-to-the-entity-list-removals-from-the-validated-end-user-veu-program">Entity List in late 2024</a>.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/semiconductors/chinas-largest-foundry-testing-first-domestic-immersion-duv-lithography-tool-smic-takes-significant-step-on-road-to-wafer-fab-equipment-self-sufficiency
SMIC is testing a domestically built immersion DUV lithography system developed by Yuliangsheng and capable of 28nm-class process technology. But while it is said that the tool could be used to make 7nm or even 5nm-class chips with multipatterning, it remains to be seen whether this is going to happen any time soon.
3HazxWKQJrJJADHwtkLJG
Wed, 17 Sep 2025 14:11:15 +0000 Semiconductors
Tech Industry
Manufacturing
ashilov@gmail.com (Anton Shilov)
Anton Shilov
SMIC
SMIC
SMIC
<![CDATA[ Cooler Master debuts new 3D Heatpipe tech in new coolers — Hyper 212 3DHP promises reduced thermals and improved efficiency
]]>
<p>Cooler Master showed off its 3D Heatpipe technology back at Computex 2025, and now it's finally coming to the public just a few months later. This quick turnaround time can be attributed to perhaps the ingenious simplicity of this solution. Instead of having the heatpipes only go up against the edges of the heatsink fin stack, another one cuts through the center, allowing for more even heat dissipation — and now Cooler Master is bringing it to its legendary Hyper 212 lineup, as reported by <a data-analytics-id="inline-link" href="https://www.techpowerup.com/340696/cooler-master-intros-hyper-212-3dhp-cpu-cooler-with-3d-heat-pipe-technology" target="_blank">TechPowerUp</a>.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:800px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="dXUGmaPtsgULVTUFF8bpQE" name="inside-3dhp-p2-01-ezgif.com-video-to-gif-converter" alt="Cooler Master's 3D Heatpipe tech" src="https://cdn.mos.cms.futurecdn.net/dXUGmaPtsgULVTUFF8bpQE.gif" mos="" align="middle" fullscreen="" width="800" height="450" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Cooler Master)</span></figcaption></figure><p>If you've been in the PC community for even a smidge, the name Hyper 212 must ring a bell. Originally launched almost two decades ago, the Hyper 212 has gone through countless revisions, modernizing it for every generation of PC gaming. The latest in this line of iterations is the aforementioned 3D Heatpipe technology, dubbing the new cooler "Hyper 212 3DHP." To understand why this is special, we should first look at how (most) standard air coolers work. Generally, these tower coolers feature a dense heatsink with multiple fins stacked atop each other, through which a U-shaped heatpipe runs.</p><p>This pipe takes heat from the CPU's IHS and carries it across the finstack, where the mounted fans blow fresh air onto it to cool it down. This is a pretty decent thermal exchange, but it can be made better. Instead of just two heatpipes at the periphery of the fin stack, Cooler Master introduced a third one running through the middle, essentially forming a trident-like shape. This results in much more efficient distribution and dissipation of heat, since each pipe will not only be responsible for less heat now, but they will cover a larger area on the fin stack.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:741px;"><p class="vanilla-image-block" style="padding-top:74.09%;"><img id="TyDeStcnYhq7nX7HNbAdCU" name="RW6YnbvAXNkk1mHI" alt="Cooler Master's 3D Heatpipe tech" src="https://cdn.mos.cms.futurecdn.net/TyDeStcnYhq7nX7HNbAdCU.jpg" mos="" link="" align="" fullscreen="" width="741" height="549" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Cooler Master)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:871px;"><p class="vanilla-image-block" style="padding-top:62.23%;"><img id="Lkjor7X9aGycVsFeZHuwCU" name="GME5C0gyanRqHrFe" alt="Cooler Master's 3D Heatpipe tech" src="https://cdn.mos.cms.futurecdn.net/Lkjor7X9aGycVsFeZHuwCU.jpg" mos="" link="" align="" fullscreen="" width="871" height="542" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Cooler Master)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:756px;"><p class="vanilla-image-block" style="padding-top:55.56%;"><img id="uCTTzBVpRYmBLTAf8HXUv7" name="hp-3dhp-side-by-side" alt="Cooler Master's 3D Heatpipe tech" src="https://cdn.mos.cms.futurecdn.net/uCTTzBVpRYmBLTAf8HXUv7.png" mos="" link="" align="" fullscreen="" width="756" height="420" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Cooler Master)</span></figcaption></figure></div></div></div><p>It sounds simple (and it is), but it works, and that's why it's first appearing in the Hyper 212 series. Cooler Master is prepping two variants of the Hyper 212 3DHP: the standard one featuring ARGB fan(s) and the 3DHP Black, which, shocker, comes in black and with a non-LED-lit fan. Owing to the lineup's affordable nature, the Hyper 212 3DHP Black will cost just $29.99 — same as the standard non-3DHP model — and include a generous 5-year warranty, where Hyper 212s usually only get two.</p><p>The specs are otherwise identical between the ARGB and Black models; both come with two 3D Heatpipes, one on either side so totaling six ends at the top. While the company did not disclose weight, the dimensions are 133mm x 86mm x 158mm. The supplied fans will spin up to 2,050 RPM, moving 63.1 CFM of airflow with a static pressure of 2.69 mm. Noise is also kept under control with a max claimed output of just 27 dBA. The Hyper 212 3DHP is not available right away on Cooler Master's website, but it should start to show up soon at retailers.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><u><em> Tom's Hardware on Google News</em></u></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><u><em> add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/air-cooling/cooler-master-debuts-new-3d-heatpipe-tech-in-new-coolers-hyper-212-3dhp-promises-reduced-thermals-and-improved-efficiency
Cooler Master is updating its iconic lineup of Hyper 212 coolers with its proprietary 3D Heatpipe tech. Hyper 212 3DHP, therefore, has two trident-like heatpipes running through its finstack that dissipate heat more evenly while costing the same. They will come in two flavors: Black and ARGB, and will be backed with 5-year warranties.
KLbDDtbtNwcFAd2odhxixS
Wed, 17 Sep 2025 13:57:08 +0000 Air Cooling
PC Components
Cooling
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
Cooler Master / Future
Cooler Master Hyper 212 3DHP
Cooler Master Hyper 212 3DHP
<![CDATA[ $320 camera lens buyers hit with $2,000 delivery fee in tariffs fight — some sellers implement exorbitant shipping costs to dissuade US customers ]]>
<p>President Donald Trump ended the de minimis exemption for goods coming from China and Hong Kong in May 2025, with imports from the rest of the world following suit at the end of July 2025. This means that products entering the U.S. are now subject to customs duties and import taxes, even if their value is less than $800. This change most heavily affected online shoppers, especially those who are used to purchasing cheap goods from abroad. This has caused some retailers to take extreme 'countermeasures,' often to dissuade Americans from purchasing their products.</p><p>Some sellers, particularly on eBay, have resorted to charging exorbitant shipping fees to deter Americans from purchasing their items. <a data-analytics-id="inline-link" href="https://www.404media.co/2-000-shipping-international-sellers-charge-absurd-prices-to-avoid-dealing-with-american-tariffs/"><em>404 Media</em></a> reports seeing several listings of lightweight items from across the globe with high delivery costs.</p><p>For example, there’s a camera lens from Japan priced at around $320, which only costs $29 to ship globally — except for the U.S., which has a $2,000 delivery fee. Other sellers charge more than $500 to ship their items to the U.S., far more expensive than the actual price of the product. According to the report, this is much easier than taking down hundreds of listings and excluding the U.S. from them. There’s also the issue that some American buyers might not understand how tariffs work, and leave negative feedback on the seller, affecting their profile on the platform.</p><p>According to the <a data-analytics-id="inline-link" href="https://www.wsj.com/business/logistics/the-new-pitfall-of-online-shopping-a-surprise-tariff-bill-bc4f333f"><em>Wall Street Journal</em></a>, one customer bought a $77 shirt from a Swedish brand and was charged an extra $42.35 on top of the $30 shipping fee. Another person bought $640 worth of oven replacement parts from Canada and was surprised by a $1,196.12 “government charges” bill and a brokerage fee of $128.17. These tariffs and import taxes heavily affect tech goods, especially as many of them are manufactured outside of the U.S. <em>Tom’s Hardware </em>staff are also encountering a similar problem, though not to these extremes, even when dealing with review samples.</p><p>This change has caused widespread confusion in international shipping, with many post offices across the globe suspending e-commerce shipments until they’ve updated their systems to account for the new charges. The big shipping companies in the U.S. — FedEx, DHL, and UPS — reported that customers are confused by the new rules. And although they process tariff payments for imports, either by the buyer or the seller, the service is not free, resulting in additional brokerage or processing fees.</p><p>Some sellers pay the tariffs upfront to make the transaction smoother for their customers, while others leave all these extra costs to the recipient. Nevertheless, the person who bought the item will ultimately be the one responsible for import taxes. But even though couriers are trying to make the process as smooth as possible, the payment process can sometimes be confusing.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><u><em>Tom's Hardware on Google News</em></u></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><u><em>add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/usd320-camera-lens-buyers-hit-with-usd2-000-delivery-fee-in-tariffs-fight-some-sellers-implement-exorbitant-shipping-costs-to-dissuade-us-customers
Some sellers are resorting to ridiculous shipping fees to deter American customers from buying their items.
ZEZC2J5QMbzGyykdgajEWM
Wed, 17 Sep 2025 13:38:15 +0000 Tech Industry
editors@tomshardware.com (Jowi Morales)
Jowi Morales
Shutterstock
Shipping containers
Shipping containers
<![CDATA[ NAND and DRAM prices surge by up to 20% — contract price increases driven by AI demands and tight supply ]]>
<p>Contract prices for both NAND and DRAM have jumped by an estimated 15-20% in the fourth quarter of 2025, according to numbers published by <a data-analytics-id="inline-link" href="https://www.digitimes.com/news/a20250917PD211.html" target="_blank"><em>DigiTimes</em></a> on September 17, an off-season surge the publication ties directly to AI infrastructure build-outs and supply tightness.</p><p>“Supply shortages led to aggressive procurement from cloud service providers, with high-stack 3D NAND products nearly sold out,” writes <em>DigiTimes</em>, adding that, “3D NAND… has attracted strong priority purchasing interest from CSP customers,” citing demand for faster read speed and larger die capacities. This is a stark difference from the normal Q4 pattern, when component prices typically drift lower.</p><p>There are signs that the supply side is tightening. TrendForce says SanDisk pushed for a roughly 10% NAND hike in September, while Micron temporarily suspended DRAM and NAND price quotations to reassess allocations after customer forecasts pointed to shortages. The firm also flags a structural shortage in nearline HDDs that is forcing hyperscalers to accelerate plans for QLC SSD deployments in 2026.</p><p>DigiTimes goes further, saying <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/storage/bigger-and-cheaper-ssds-are-coming-thanks-to-samsung-chipmaker-starts-mass-producing-280-layer-qlc-9th-generation-v-nand">Samsung’s next-gen V9 NAND</a> for 2026 is “nearly sold out”, with cloud customers locking in capacity early due to its improved density and cost advantages. A separate TrendForce brief from this week, however, says Samsung has delayed V9 QLC to the first half of 2026, which suggests customers may be reserving capacity ahead of firm volume timing. Either way, it’s clear to see that cloud buyers are aggressively securing supply well into the future.</p><p>This could easily have a knock-on effect on consumer prices. If hyperscalers are absorbing more wafers for enterprise SSDs while DRAM makers prioritize server parts and HBM, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ram/ddr4-costs-soar-as-manufacturers-pull-the-plug">retail pricing</a> is going to lose some of its slack. TrendForce has already warned that legacy DRAM types remain under the most pressure as capacity is reallocated, and if cloud orders continue to rise, the familiar winter bargains on NVMe drives could be thinner than expected.</p><p>One tell that money is switching hands is controller specialist Phison’s record August figures. It reported a revenue of NT$5.934 billion, up 23% year over year. That’s a dramatic jump from last year’s weak base. The company has attributed the strength to non-consumer demand and closer tie-ups with NAND makers, which fits the broader theme of data-center-led flash tightness.</p><p>The bottom line is that multiple data points now converge on a common theme: AI is rewriting cloud storage hierarchy while HDD supply is constrained and flash makers have more pricing power than they usually do in the year’s final quarter. If you’re planning an upgrade, watch retail memory prices closely and move quickly when a good price pops up, because it probably won’t stick around for long.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em> Tom's Hardware on Google News</em></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em> add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/nand-and-dram-prices-spike-in-q42025
Contract prices for both NAND and DRAM have jumped by an estimated 15-20% in the fourth quarter of 2025, according to numbers published by DigiTimes on September 17.
dufqRVzPk924sCrymrz9XE
Wed, 17 Sep 2025 12:31:45 +0000 Tech Industry
lukejamesalden@gmail.com (Luke James)
Luke James
Samsung
Samsung 9th Gen QLC V-NAND
Samsung 9th Gen QLC V-NAND
<![CDATA[ AMD launches Ryzen 9000 PRO series, flagship model tops out at 12 cores — new enterprise lineup includes 3 CPUs for OEMs featuring added business and security features ]]>
<p>AMD released a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-launches-four-new-ryzen-cpus-including-cutdown-zen-4-and-zen-3-models-most-only-available-in-global-markets" target="_blank">bunch of new Zen 3, Zen 4, and Zen 5 processors yesterday</a>, refreshing both existing and older families of CPUs. During this time, the company also silently launched its new enterprise lineup: <a data-analytics-id="inline-link" href="https://www.amd.com/en/products/processors/ryzen-for-professionals.html#tabs-9610e26165-item-17365abc16-tab" target="_blank">Ryzen PRO 9000</a>. These include three new SKUs built on the same Zen 5 "Granite Ridge" architecture as the mainline Ryzen 9000 series, but they feature cut-down silicon in favor of enterprise management and security features not present on standard CPUs.</p><p>First up, we have the Ryzen 5 Pro 9645. It comes with 6 cores and 12 threads, clocked at 3.9 GHz with boost speeds up to 5.4 GHz. Then there's the Ryzen 7 Pro 9745 with 8 cores and 16 threads, featuring the same 5.4 GHz boost clock but a slightly reduced 3.8 GHz base clock. The final model is the Ryzen 9 Pro 9945, which only has 12 cores and 24 threads, clocked at 3.4 GHz and boosting up to 5.4 GHz. Cache levels also remain unaltered in comparison to analogous Ryzen 9000 models.</p><div ><table><caption>Ryzen PRO 9000</caption><thead><tr><th class="firstcol " ><p>SKU</p></th><th
><p>Core Count</p></th><th
><p>Base Clock / Boost Clock</p></th><th
><p>Cache</p></th><th
><p>TDP</p></th></tr></thead><tbody><tr><td class="firstcol " ><p>Ryzen 9 PRO 9945</p></td><td
><p>12C / 24T</p></td><td
><p>3.4 GHz / Up to 5.4 GHz</p></td><td
><p>76 MB</p></td><td
><p>65W</p></td></tr><tr><td class="firstcol " ><p>Ryzen 7 PRO 9745</p></td><td
><p>8C / 16T</p></td><td
><p>3.8 GHz / Up to 5.4 GHz</p></td><td
><p>40 MB</p></td><td
><p>65W</p></td></tr><tr><td class="firstcol " ><p>Ryzen 5 PRO 9645</p></td><td
><p>6C / 12T</p></td><td
><p>3.9 GHz / Up to 5.4 GHz</p></td><td
><p>38 MB</p></td><td
><p>65W</p></td></tr></tbody></table></div><p>All three SKUs share the same 65W TDP, despite the standard Ryzen 9 9900X — which the Pro 9945 would be based on — featuring a 120W TDP. But it makes sense given there are 4 fewer cores on the Pro 9945. There are consistent base and boost clock gains over the previous generation Ryzen PRO 7000 series, like a 300 MHz boost clock increase on the Ryzen 5 Pro SKUs. However, the Ryzen Pro 9945 loses 300 MHz in its base clock when compared to its predecessor (3.4 GHz vs 3.7 GHz).</p><p>Performance-wise, AMD shared some slides highlighting the improvements these Zen 5-based enterprise processors carry. Even though they're technically not focused on raw numbers, the Ryzen 9 Pro 9945 is reportedly up to 44% faster in Blender and up to 22% faster in other productivity benchmarks, when compared to Intel's Core i7-14700 processor. Moreover, surprisingly, these CPUs will also come with a bundled Wraith Stealth stock cooler</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2672px;"><p class="vanilla-image-block" style="padding-top:56.96%;"><img id="wpYN3Gs6Yo2qkJZ6p7v3in" name="Screenshot 2025-09-17 at 4.56.26 PM" alt="AMD Ryzen PRO 9000 performance" src="https://cdn.mos.cms.futurecdn.net/wpYN3Gs6Yo2qkJZ6p7v3in.png" mos="" align="middle" fullscreen="" width="2672" height="1522" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p>Pricing and availability are not clear because these CPUs are distributed to OEMs that supply them in bulk to enterprises around the world. Ryzen PRO is generally less mature than Intel's competing vPro technology, but both offer similar features with an overlapping goal in mind.</p><p><em>Follow</em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><u><em> Tom's Hardware on Google News</em></u></a><em>, or</em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><u><em> add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/cpus/amd-launches-ryzen-9000-pro-series-flagship-model-tops-out-at-12-cores-new-enterprise-lineup-includes-3-cpus-for-oems-featuring-added-business-and-security-features
AMD has just launched its latest series of Zen 5 processors aimed at enterprise. Featuring largely the same silicon as their mainstream counterparts, these Ryzen 9000 Pro models are set to become available with OEMs soon. They feature extensive security and management features that are crucial for the sector.
j3aAV7axbvVdzwccGATGn9
Wed, 17 Sep 2025 12:23:24 +0000 CPUs
PC Components
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
AMD
AMD Ryzen AI Pro
AMD Ryzen AI Pro
<![CDATA[ UK cosies up to big tech with $42 billion data center and AI investment deal ]]>
<p>A bevy of tech companies have announced over $40 billion in collective investment in the UK to coincide with the second state visit of President Trump. With announcements of cooperation from Nvidia, OpenAI, Google, Microsoft, and others, the UK is set to build new data centers, as well as AI and energy infrastructure, reports<a data-analytics-id="inline-link" href="https://www.reuters.com/world/uk/uk-us-agree-42-billion-tech-pact-mark-trumps-visit-2025-09-16/" target="_blank"> Reuters</a>. The British government hopes that having a lighter approach to AI legislation and planning will allow the country to attract further investment from technology companies that may be deterred by tighter regulations in the EU.</p><p>Since coming to power in spring 2024, the UK's Labour government has struggled to balance its progressive policies with a need to drive economic growth in the wake of years of stagnation within the country. Despite improving wages for public sector workers and driving investment into public services, the economy has failed to rally like the government hoped. Encouraging such large-scale investment from so many international companies is a potential way to dig the UK out of the economic hole it's found itself in.</p><p>British Prime Minister Keir Starmer hailed the news as both prosperous for the UK and the relationship between the United States and the UK.</p><p>"This Tech Prosperity Deal marks a generational step change in our relationship with the US, shaping the futures of millions of people on both sides of the Atlantic, and delivering growth, security and opportunity up and down the country," he said.</p><p>"By teaming-up with world-class companies from both the UK and US, we’re laying the foundations for a future where together we are world leaders in the technology of tomorrow, creating highly skilled jobs, putting more money in people’s pockets and ensuring this partnership benefits every corner of the United Kingdom."</p><p>Several deals have been announced from various companies, ranging from hundreds of millions of dollars of investment to several billions. <a data-analytics-id="inline-link" href="https://blog.google/around-the-globe/google-europe/united-kingdom/waltham-cross-data-centre/">Google opened a new data center in Waltham</a>, Lincolnshire, as part of a $6.8 billion investment package that will see it expand its London-based Google DeepMind research center for AI research in science and healthcare. These projects will support over 8,250 jobs and help Google enhance its Maps, Cloud, and search tools in the UK.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="oGqaWHG28MBTNkp9YXSDvF" name="googlewaltham" alt="Google Waltham Data Center shown on Google Maps in 3D." src="https://cdn.mos.cms.futurecdn.net/oGqaWHG28MBTNkp9YXSDvF.jpg" mos="" align="middle" fullscreen="" width="1600" height="900" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">Google's new Watlham-based data center will use a new deal between Google and Shell to expand Google's battery-based energy storage in the UK, too. </span><span class="credit" itemprop="copyrightHolder">(Image credit: Google)</span></figcaption></figure><p>OpenAI <a data-analytics-id="inline-link" href="https://openai.com/index/introducing-stargate-uk/" target="_blank">announced Stargate UK</a> in partnership with Nvidia and UK-based Nscale. Together they're developing several new AI data centers beginning at the site of a former coal power station in Northumberland, north-east England. This is one of the UK's newly designated <a data-analytics-id="inline-link" href="https://www.gov.uk/government/news/north-east-england-set-for-billions-in-investment-and-thousands-of-jobs-as-uk-and-us-ink-tech-partnership" target="_blank">"AI Growth Zones,"</a> which are designed to ease the building of data centers and revitalize under-invested areas of the UK.</p><p>Backed by an additional $13.5 billion in investment from financial firm Blackstone, the area already has an established wind power presence, with further plans to develop solar energy and battery storage facilities to reduce the environmental impact of these new developments.</p><p>To power the new data centers, OpenAI and Nscale will be importing tens of thousands of graphics cards from Nvidia. OpenAI said that it would "offtake up to 8,000 GPUs in Q1 2026 with the potential to scale to 31,000 GPUs over time."</p><p>That's just the start, though. In <a data-analytics-id="inline-link" href="https://nvidianews.nvidia.com/news/nvidia-and-united-kingdom-build-nations-ai-infrastructure-and-ecosystem-to-fuel-innovation-economic-growth-and-jobs" target="_blank">its announcement, Nvidia</a> claimed it was in talks to provide up to 60,000 Grace Blackwell GPUs to the UK, as well as an additional 120,000 Blackwell GPUs for AI data centers in the UK. Over 20,000 of those will be invested in the new supercomputer Nscale is developing in Essex, with Microsoft now joining the project to provide additional investment.</p><p><a data-analytics-id="inline-link" href="https://www.bbc.co.uk/news/articles/c7016ljre03o" target="_blank">Microsoft's UK investment could well be the largest</a>. With over $30 billion promised over the next four years, Microsoft pledged at least half of that towards AI infrastructure, and the rest to expanding Microsoft's existing activities in the country, including gaming and Windows services.</p><p>There is also <a data-analytics-id="inline-link" href="https://www.gov.uk/government/news/us-uk-pact-will-boost-advances-in-drug-discovery-create-tens-of-thousands-of-jobs-and-transform-lives" target="_blank">a range of investments in the 10s to multiple hundreds of millions of dollars</a> from ARM, Coreweave, ScaleAI, DataVita, TechUK, Pathfinder, Salesforce, and Blackstone, among others. They're driving further investment into quantum computing, medicine, space exploration, digital security, and more.</p><p>As if in anticipation of concerns over power and water usage at the range of new data centers, the UK government also announced new UK and U.S. partnerships over the development of nuclear energy. Under the scheme, companies will be able to acquire licenses to build nuclear reactors faster in the UK. It will also facilitate greater research into the development of commercial fusion power.</p><h2 id="the-balancing-act-2">The balancing act</h2><p>From a purely economic perspective, it's hard to find fault in all these announcements. Any economy can benefit enormously from tens of billions of dollars of outside investment, and especially when that comes with closer ties to some of the world's most valuable companies, which are all spending big to ensure their own gravy trains keep on rolling.</p><p>But it's all a delicate balancing act for the UK government. While the news of AI investment is indeed positive for the country from an economic perspective, whether or not these investments turn into tangible, usable resources for the country and its AI efforts remains to be seen. Regardless of the outcome, this scale of planning is exactly what Nvidia CEO Jensen Huang wants: to build AI infrastructure across the globe.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/uk-cosies-up-to-big-tech-with-usd42-billion-data-center-and-ai-investment-deal
The UK government has secured tens of billions of dollars of investment from a range of major tech companies in a deal that comes at a time of particular political pressure in the UK and may well help drive the growth the UK so desperately needs.
P4jdwnTQgpiXEFxxQapvWa
Wed, 17 Sep 2025 12:23:08 +0000 Tech Industry
Jon Martindale
Getty Images/WPA Pool
UK Prime Minister Keir Starmer in front of the Union Jack.
UK Prime Minister Keir Starmer in front of the Union Jack.
<![CDATA[ Asus ProArt PA32QCV 6K professional monitor review: Plenty of pixels, color, and brightness ]]>
<p>When <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/4k-definition,37642.html">4K</a> monitors first appeared more than 10 years ago, my first thought was, “How long before we have 6K and 8K?” Surprisingly, 4K is still the highest resolution available in the mainstream, but there are a few monitors out there featuring panels with higher resolution and pixel density. I looked at a 5K Asus ProArt screen last year, the PA27JCV. It has 218ppi density (5120x2880) and an impressive feature set worthy of creatives’ attention for around $800.</p><p>The Asus ProArt line now includes a 6K monitor, the PA32QCV. It has the same 218ppi from a 32-inch panel with 6016x3384 resolution, 600 nits peak brightness for SDR and HDR, an IPS panel with over 1,600:1 native contrast, wide gamut color, and plenty of color modes for any industry standard you might need for video, gaming, or photo production. Let’s take a look.</p><h2 id="asus-proart-pa32qcv-specs-2">Asus ProArt PA32QCV Specs</h2><div ><table><tbody><tr><td class="firstcol " ><p>Panel Type / Backlight</p></td><td
><p>IPS / W-LED, edge array</p></td></tr><tr><td class="firstcol " ><p>Screen Size / Aspect Ratio</p></td><td
><p>32 inches / 16:9</p></td></tr><tr><td class="firstcol " ><p>Max Resolution and Refresh Rate</p></td><td
><p>6016x3384 @ 60 Hz</p></td></tr><tr><td class="firstcol empty" ></td><td
><p>FreeSync and G-Sync compatible</p></td></tr><tr><td class="firstcol " ><p>Native Color Depth and Gamut</p></td><td
><p>10-bit / DCI-P3</p></td></tr><tr><td class="firstcol " ><p>Response Time (GTG)</p></td><td
><p>5ms</p></td></tr><tr><td class="firstcol " ><p>Brightness (mfr)</p></td><td
><p>600 nits</p></td></tr><tr><td class="firstcol " ><p>Contrast (mfr)</p></td><td
><p>1,500:1</p></td></tr><tr><td class="firstcol " ><p>Speakers</p></td><td
><p>2x 2w</p></td></tr><tr><td class="firstcol " ><p>Video Inputs</p></td><td
><p>1x DisplayPort 1.4 w/DSC</p></td></tr><tr><td class="firstcol empty" ></td><td
><p>1x HDMI 2.1, 2x Thunderbolt 4.0</p></td></tr><tr><td class="firstcol " ><p>Audio</p></td><td
><p>3.5mm headphone output</p></td></tr><tr><td class="firstcol " ><p>USB 3.2</p></td><td
><p>1x up, 3x down</p></td></tr><tr><td class="firstcol " ><p>Power Consumption</p></td><td
><p>41.4w, brightness @ 200 nits</p></td></tr><tr><td class="firstcol " ><p>Panel Dimensions</p><p> WxHxD w/base</p></td><td
><p>28.1 x 19-24.2 x 9.5 inches</p><p> (714 x 483-615 x 241mm)</p></td></tr><tr><td class="firstcol " ><p>Panel Thickness</p></td><td
><p>1.8 inches (46mm)</p></td></tr><tr><td class="firstcol " ><p>Bezel Width</p></td><td
><p>Top/sides: 0.28 inch (7mm)</p><p> </p></td></tr><tr><td class="firstcol empty" ></td><td
><p>Bottom: 0.67 inch (17mm)</p></td></tr><tr><td class="firstcol " ><p>Weight</p></td><td
><p>20.5 pounds (9.3kg)</p></td></tr><tr><td class="firstcol " ><p>Warranty</p></td><td
><p>3 years</p></td></tr></tbody></table></div><p>Despite its cutting-edge specs, the PA32QCV keeps things simple, meaning the price isn’t too high; $1,299 at this writing. The main missing feature is a whole array Mini LED with local dimming. Edge lighting is the tech used here, but there’s no shortage of brightness. The HDR rating is VESA DisplayHDR 600, and you get the same peaks for SDR. I measured almost 650 nits in my tests, and that was from both full field and window patterns. There is dynamic dimming with a few options to tailor the speed and ratio.</p><p>Though the PA32QCV can be used for gaming, it is limited to 60 Hz (however, there is Adaptive-Sync and overdrive). You also get HDR10 but not Dolby Vision. HDR modes include multiple luminance curves and OSD calibration controls.</p><p>The big draw is a group of precise color modes. Rather than calling them things like Racing or FPS, they are termed by their color standard. You get everything currently in standard use from sRGB to BT.2020. The native color gamut tops out at around 100% of DCI-P3. Each mode is factory calibrated, and you can tweak them further using the OSD or Asus’ DisplayWidget Center. You can also do an auto calibration using Portrait Displays’ Calman software and a meter of your choice.</p><p>In addition to the tremendous pixel density, the PA32QCV employs other features to improve image quality. One element that LCDs struggle with is the anti-glare layer. This often reduces clarity and perceived color saturation, so Asus has included a technology called LuxPixel, which gives you the benefits of an optically clean screen that also rejects ambient light. It works as advertised and has the further advantage of completely hiding any visible pixel structure.</p><p>There are numerous convenience features, including KVM, USB ports, two Thunderbolt inputs, and an ambient light sensor that can be engaged to control brightness during changes in the viewing environment. The stand is super solid and fully adjustable for tilt, height, swivel, and rotation. Asus has demonstrated many times that pro monitors don’t have to cost $5,000. The PA32QCV is a perfect example of this philosophy, with a reasonable price tag and very high performance.</p><h2 id="assembly-and-accessories-2">Assembly and Accessories</h2><p>The PA32QCV’s stand, base, and panel ship in fully recyclable packaging that uses molded cardboard pulp to protect the contents. The parts assemble without tools into a quality piece that feels premium in every way. The cable bundle includes IEC power, HDMI, and Thunderbolt/USB-C. You also get a small microfiber cleaning cloth.</p><h2 id="product-360-2">Product 360</h2><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:94.20%;"><img id="93KHvApYZoxdTa6aPsP6Qd" name="a-front" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/93KHvApYZoxdTa6aPsP6Qd.jpg" mos="" link="" align="" fullscreen="" width="1000" height="942" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Asus)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:93.00%;"><img id="ehjYxLgQq5gPQyAqWhh7Qd" name="a-angle" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/ehjYxLgQq5gPQyAqWhh7Qd.jpg" mos="" link="" align="" fullscreen="" width="1000" height="930" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Asus)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:100.00%;"><img id="vJmkwivfiVnAo52aJcbjMd" name="a-back" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/vJmkwivfiVnAo52aJcbjMd.jpg" mos="" link="" align="" fullscreen="" width="1000" height="1000" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Asus)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:52.60%;"><img id="bqzsNeRbDP8qVLSfsi2TNd" name="a-inputs" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/bqzsNeRbDP8qVLSfsi2TNd.jpg" mos="" link="" align="" fullscreen="" width="1000" height="526" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Asus)</span></figcaption></figure></div></div></div><p>The PA32QCV maintains Asus’ ProArt styling with equal measures of form and function. The front is all screen, with a super-thin, flush bezel around the top and sides, measuring just 7mm, and a 17mm strip at the bottom. It’s just large enough to accommodate a row of control keys plus a tiny joystick and a small power LED. The screen’s LuxPixel tech is evidenced by a complete lack of reflections from ambient light and a crisply saturated image with no visible pixel structure.</p><p>The stand is a pole-shaped upright with a cable hole in the middle. It attaches to a large base plate for a wobble-free package. Ergonomics include 5.2 inches of height, 5/23 degrees tilt, 30 degrees swivel, and a 90-degree portrait mode. The panel is a bit thinner than other 32-inch monitors I’ve encountered at just 1.8 inches deep. It features a 100mm VESA pattern on the back for aftermarket mounts, with fasteners included. Asus also makes a desk clamp available that interfaces with the upright.</p><p>Inputs are plentiful and include one each of DisplayPort 1.4 and HDMI 2.1. Two Thunderbolt 4.0 ports provide 96 watts of power and allow two PA32QCVs to be daisy-chained. USB is supported by two Type-C and two Type-A ports along with a KVM setup in the OSD. A pair of integrated speakers plays high frequencies politely with two watts of power.</p><h2 id="osd-features-2">OSD Features</h2><p>The PA32QCV’s OSD is extensive but logically laid out. It will be familiar to Asus ProArt users and is based on industry-standard color modes. It also includes calibration controls, HDR options, KVM, Asus QuickFit, and a host of other convenience features.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:61.60%;"><img id="EDGuTajC8KaiChEpnZvcXA" name="osd1" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/EDGuTajC8KaiChEpnZvcXA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="616" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:61.60%;"><img id="xiKCqSJrUeAWzX8TA7yDXA" name="osd2" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/xiKCqSJrUeAWzX8TA7yDXA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="616" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:61.80%;"><img id="GmJHNMgjTsK4Q3JNmErfWA" name="osd3" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/GmJHNMgjTsK4Q3JNmErfWA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="618" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:61.70%;"><img id="LDFE4gDst3HAAawRCzhAXA" name="osd4" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/LDFE4gDst3HAAawRCzhAXA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="617" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 5 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:61.30%;"><img id="hR5k5NGdJVqdBRLkL4gLXA" name="osd5" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/hR5k5NGdJVqdBRLkL4gLXA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="613" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 6 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:61.20%;"><img id="kHDhfxTTLDnZSTJuZkdSXA" name="osd6" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/kHDhfxTTLDnZSTJuZkdSXA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="612" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 7 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:61.10%;"><img id="3KT6eZYJ5yrTqYCKdvKwWA" name="osd7" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/3KT6eZYJ5yrTqYCKdvKwWA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="611" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 8 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:61.20%;"><img id="kBmZe3ecRzPeRNyTVvUQXA" name="osd8" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/kBmZe3ecRzPeRNyTVvUQXA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="612" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 9 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:61.50%;"><img id="YYBFCRbLNuwXkvqQ4THSXA" name="osd9" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/YYBFCRbLNuwXkvqQ4THSXA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="615" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 10 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:60.60%;"><img id="ozYRSghtB8wovqdwYebWXA" name="osd10" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/ozYRSghtB8wovqdwYebWXA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="606" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 11 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:61.20%;"><img id="NtA5wTVmxG5rQHtMHQc3XA" name="osd11" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/NtA5wTVmxG5rQHtMHQc3XA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="612" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 12 of 12</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:59.40%;"><img id="aQ2wFsqkbPCW8VRd33XUXA" name="osd13" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/aQ2wFsqkbPCW8VRd33XUXA.jpg" mos="" link="" align="" fullscreen="" width="1000" height="594" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The OSD begins with the color modes, and there are seven for SDR and three for HDR, plus two user memories. Native is the default, and it is equivalent to Display P3 with around 100% coverage of that gamut, D65 white point and 2.2. gamma. P3 includes Display P3, Cinema P3, and M-Model, which correspond to macOS standards. You also get DICOM for medical imaging equipment. HDR10 signals get a choice of three different luminance (EOTF) curves. PQ Optimized is the default and best option there.</p><p>To calibrate, you can choose from fixed color temps by Kelvin value. Gamma comes with five presets from 1.8 to 2.6 in 0.2 increments. RGB tuning is a precise two-point grayscale control that can alter any of the existing picture modes except sRGB. Black level affects how low the backlight goes when dynamic dimming is engaged. Deep means it will turn off when there is no signal.</p><p>In the Image menu are sharpness and trace-free options. The latter is Asus’ term for overdrive, and it works best on its default setting of 60. In practice, there’s only so much that can be done at 60fps. You can reduce motion blur, but it won’t be eliminated. Looking for Adaptive-Sync? I’m getting to it, keep reading.</p><p>The PA32QCV includes plenty of PIP and PBP options for viewing two or three video inputs at once. Each window can have separate color settings and be sized. QuickFit is a staple on all ProArt monitors, and it is super handy for document creation and video production. You can put markers and framing limits on the screen to help with composition during live shoots or when cropping in postproduction.</p><p>Dynamic Dimming is a field dimming feature that doubles contrast for SDR content and takes HDR’s range from 1,600:1 to almost 10,000:1. Remember that the PA32QCV has Adaptive-Sync? It’s in the Settings menu under the heading MediaSync. Confusing, yes. Not only is it apart from other video processing options, but it’s also called a term only used by Asus. But now that you’ve read this, you know, and can tell your friends.</p><p>KVM gets its own sub-menu and it’s very easy to set up bindings between USB ports and video inputs. That way, you can control multiple systems with a single set of input devices. The two shortcut menus refer to two of the control keys on the front bezel. They give quick access to a variety of different functions.</p><h2 id="asus-proart-pa32qcv-calibration-settings-2">Asus ProArt PA32QCV Calibration Settings</h2><p>Typically, ProArt monitors don’t need calibration out of the box. Just pick your desired color mode and go. My sample was a bit off the mark for grayscale though, with slight green errors. I suspect this is because I received an early production sample. Color and gamma were spot-on in every mode. To verify the monitor’s operation, I calibrated the Native mode with excellent results. My settings are below, though I suspect you won’t need them. The PA32QCV can be easily calibrated in any of its modes with Calman’s autocal feature using a meter of your choice or with the one Asus makes available that works with all ProArt monitors.</p><p>In HDR mode, there are three luminance curves, called PQ (Perceptual Quantization). Optimized is the default and best option there.</p><div ><table><tbody><tr><td class="firstcol " ><p>Picture Mode</p></td><td
><p>Native</p></td></tr><tr><td class="firstcol " ><p>Brightness 200 nits</p></td><td
><p>86</p></td></tr><tr><td class="firstcol " ><p>Brightness 120 nits</p></td><td
><p>50</p></td></tr><tr><td class="firstcol " ><p>Brightness 100 nits</p></td><td
><p>41</p></td></tr><tr><td class="firstcol " ><p>Brightness 80 nits</p></td><td
><p>32</p></td></tr><tr><td class="firstcol " ><p>Brightness 50 nits</p></td><td
><p>19 (min. 10 nits)</p></td></tr><tr><td class="firstcol " ><p>Contrast</p></td><td
><p>80</p></td></tr><tr><td class="firstcol " ><p>Gamma</p></td><td
><p>2.2</p></td></tr><tr><td class="firstcol " ><p>Color Temp User</p></td><td
><p>Gain – Red 219, Green 194, Blue 205</p></td></tr><tr><td class="firstcol empty" ></td><td
><p>Bias – Red 200, Green 200, Blue 200</p></td></tr></tbody></table></div><h2 id="setup-and-hands-on-2">Setup and Hands-on</h2><p>Calibrating the PA32QCV initially is unnecessary unless you want to create a custom setup that falls outside industry standards for grayscale, gamma and color gamut. You only need to select your desired mode, and it will deliver that spec with extreme accuracy. That said, I found my sample had slightly less precise grayscale tracking than other ProArt displays I’ve reviewed. Errors averaged around 3dE, which tells me I likely received a pre-production sample. Other ProArt displays I’ve reviewed were around 1dE for grayscale. Gamma and color gamut results were nearly perfect in all color modes except for BT.2020, which the monitor isn’t actually capable of. Its max color volume is around 100% of P3. I summarize all the results in the test notes on the next three pages.</p><p>The sRGB mode is brightness limited to 80 nits so as an alternative, you can pick Rec.709, which lets you select from four color temps and five gamma values. P3 offers three options. The cinema version with a D63 color temp and 2.6 gamma, Display P3 with D65 and 2.2, and M-Model, which matches the standard used by macOS. This is a new addition to the ProArt toolkit. Alternatively, you can use Native, which corresponds to Display P3 with a D65 color point and 2.2 gamma. Adobe RGB is there for photographers, along with BT.2020 mode, which has selectable gamma and color temp.</p><p>The HDR10 mode has three possible PQ curves. I found PQ Optimized to be the most accurate in testing, but you may wish to change it for the sake of the content you’re creating. You can choose DCI-P3 or Rec.2020 color and adjust the color temp with a single set of RGB sliders.</p><p>In use, the PA32QCV is incredibly competent and flexible. For those who enjoy a colorful picture, the default Native mode is fine for everyday tasks like writing or web browsing. When photo editing tops the to-do list, just pick the necessary standard from the presets. If you need a different gamma or color temp, say Rec.709 with a sepia tone for that vintage Hollywood look, just pick one of the fixed Kelvin values. You can do the same thing with gamma in all modes except sRGB.</p><p>For a totally DIY standard, there are two settings memories labeled User that can be used for any combination you like. The only limitation here is in the BT.2020 mode where coverage tops out at around 74%. The PA32QCV is not a Quantum Dot display, so it stops at DCI-P3, which it covers just under 100% of.</p><p>The only thing I recommend avoiding here is full motion gaming. The PA32QCV maxes at 60 Hz and though there is overdrive (TraceFree) available, I am far too spoiled by fast refresh screens to even consider gaming at 60fps. Motion blur is significant. And if you use Adaptive-Sync, TraceFree is off the table. Static games like Myst look great thanks to that insanely high pixel density. So, if lush graphics are your thing, there is no Ultra HD monitor that will deliver the smooth rendering of the PA32QCV.</p><p>I was also impressed by the panel’s high contrast. You won’t mistake it for an OLED, but it has greater dynamic range than other IPS panels, 1,600:1 in fact. If you engage the dimming option, it jumps to over 3,000:1 for SDR and almost 10,000:1 for HDR. Black levels are very good, and highlights are even better with peaks near 650 nits.</p><p><strong>Takeaway:</strong> As long as you keep moving content to a fixed frame rate, as you would when editing video, the PA32QCV is a great-looking monitor. It’s bright and colorful for sure and delivers better blacks than typical IPS panels. For content creation, it represents supreme flexibility with near-perfect accuracy in every color mode. No tweaking is required in any of the presets. Even if you just want a nice monitor for everyday use, it isn’t super expensive and it just works.</p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-gaming-monitors,4533.html"><strong>Best Gaming Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking"><strong>How We Test PC Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/monitor-buying-guide,5699.html"><strong>How to Buy a PC Monitor</strong></a></p><p>Comparing the PA32QCV to speedy gaming monitors isn’t fair, so for this review, I’ll be sharing test results and commenting on them relative to the expectations of a reference-level professional display.</p><h2 id="pixel-response-and-input-lag-2">Pixel Response and Input Lag</h2><p><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking/4"><strong>Click here</strong></a><strong> to read up on our pixel response and input lag testing procedures.</strong></p><ul><li><strong>Response Time </strong>Full black to white transition – 16ms</li><li><strong>Absolute Input Lag </strong>Full black to white transition – 56ms</li></ul><p>The PA32QCV is not meant to be a gaming monitor, but it does include Adaptive-Sync, which improves motion processing. It’s unfortunate that you can’t use overdrive and AS at the same time though. With only 60 Hz available, you won’t want to engage in anything too frenetic. Exploring game environments is an amazing experience, but shooters will look quite blurry if you’re used to speedy screens like I am.</p><p><strong>Test Takeaway: </strong>The PA32QCV’s video processing isn’t game-focused, but since it’s marketed as a content creation tool, it should have a faster refresh rate and a good overdrive that can be used with Adaptive-Sync. That would allow creators to evaluate their game titles on a single display.</p><h2 id="viewing-angles-2">Viewing Angles</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:66.20%;"><img id="AT5QdK8ArChuU7tJKhAuVA" name="PA32QCV viewing" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/AT5QdK8ArChuU7tJKhAuVA.jpg" mos="" align="middle" fullscreen="" width="1000" height="662" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>I did a double-take when I took the above photos. You’d think you’re looking at an OLED here, not an LCD. Asus’s LuxPixel technology obviously works, and I won’t be surprised to see it appear on other brands’ displays in the future. The side view has no change in color, brightness or gamma. Like, none. This is phenomenal performance. The top view is more typical of IPS screens with reduced brightness and red tint. But a serious step forward has been made by the PA32QCV and Asus.</p><h2 id="screen-uniformity-2">Screen Uniformity</h2><p><strong>To learn how we measure screen uniformity,</strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking/4"><strong> </strong></a><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking/4"><strong>click here.</strong></a></p><ul><li><strong>Screen Uniformity Deviation From Center </strong>0% Black Field – 9.44%</li></ul><p>The PA32QCV doesn’t have uniformity compensation like some professional screens, but given the above result, I have no complaints. This is typical IPS monitor performance.</p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-gaming-monitors,4533.html"><strong>Best Gaming Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking"><strong>How We Test PC Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/monitor-buying-guide,5699.html"><strong>How to Buy a PC Monitor</strong></a></p><p><strong>To read about our monitor tests in-depth, please check out</strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking"><strong> </strong></a><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking"><strong>Display Testing Explained: How We Test PC Monitors.</strong></a> <strong>We cover brightness and contrast testing on</strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking/2"><strong> </strong></a><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking/2"><strong>page two.</strong></a></p><h2 id="maximum-backlight-level-2">Maximum Backlight Level</h2><ul><li><strong>Maximum White Luminance </strong>Native Mode – 648.9998 nits</li><li><strong>Maximum Black Luminance </strong>Native Mode – 0.4111 nit</li><li><strong>Maximum Contrast Ratio </strong>Native Mode – 1,578.7:1</li></ul><p>The PA32QCV is very bright whether dynamic dimming is engaged or not. I measured the same peaks from full field and 25% window patterns. Dimming only affects the black level, and if you use it, the contrast ratio rises to around 3,000:1. The brightness slider is very precise with 400 clicks of resolution. You can turn it down all the way to 10 nits if you like. I noted that the peak changed depending on picture mode. The above result was from Native.</p><h2 id="after-calibration-to-200-nits-2">After Calibration to 200 nits</h2><p>Since my PA32QCV sample showed slight green errors in the grayscale tests, I calibrated the Native mode using the RGB sliders with the peak value set to 200 nits.</p><ul><li><strong>Calibrated Contrast Ratio (200 nits) </strong>– 1,602.6:1</li><li><strong>16-point ANSI Contrast Ratio</strong> – 1,553.7:1</li></ul><p>The PA32QCV demonstrated consistent performance in all modes. Though the peak values changed, the contrast ratio was always around 1,600:1. This is as it should be for a professional display. ANSI contrast also remained solid. The PA32QCV has greater native dynamic range than typical IPS monitors with excellent black levels thanks to the dynamic dimming option.</p><p><strong>Test Takeaway: </strong>The PA32QCV delivers more depth and contrast than typical IPS LCD monitors. You can get better performance from an OLED or Mini LED, but at a higher price. 1,600:1 is enough range to render solid blacks and saturated color. And there is more than enough light output to use it outdoors in a production setting.</p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-gaming-monitors,4533.html"><strong>Best Gaming Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking"><strong>How We Test PC Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/monitor-buying-guide,5699.html"><strong>How to Buy a PC Monitor</strong></a></p><p>To verify the PA27JCV’s factory calibration, I measured the modes that will be most commonly used, P3 (cinema and display), sRGB, Adobe RGB, BT.709, and BT.2020. At the end of this page, there’s a summary of all the test results for each mode.</p><h2 id="grayscale-and-gamma-tracking-2">Grayscale and Gamma Tracking</h2><p><strong>Our grayscale and gamma tests use Calman calibration software from</strong><a data-analytics-id="inline-link" href="https://www.portrait.com/"><strong> </strong></a><a data-analytics-id="inline-link" href="https://www.portrait.com/"><strong>Portrait Displays</strong></a><strong>. We describe our grayscale and gamma tests in detail</strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking/3"><strong> here.</strong></a></p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:605px;"><p class="vanilla-image-block" style="padding-top:107.77%;"><img id="vwX4rTtCfaBhXvkNokL5MP" name="PA32QCV gray sRGB" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/vwX4rTtCfaBhXvkNokL5MP.jpg" mos="" link="" align="" fullscreen="" width="605" height="652" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:605px;"><p class="vanilla-image-block" style="padding-top:107.77%;"><img id="URGqkCrjTdJPRZ4arq25MP" name="PA32QCV gray Adobe" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/URGqkCrjTdJPRZ4arq25MP.jpg" mos="" link="" align="" fullscreen="" width="605" height="652" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:605px;"><p class="vanilla-image-block" style="padding-top:107.77%;"><img id="UN75RYhZQoc9bEBJssvyLP" name="PA32QCV gray 2020" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/UN75RYhZQoc9bEBJssvyLP.jpg" mos="" link="" align="" fullscreen="" width="605" height="652" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:605px;"><p class="vanilla-image-block" style="padding-top:107.77%;"><img id="AyVtV5M2XrDpRmkKxQT6MP" name="PA32QCV gray P3" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/AyVtV5M2XrDpRmkKxQT6MP.jpg" mos="" link="" align="" fullscreen="" width="605" height="652" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 5 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:605px;"><p class="vanilla-image-block" style="padding-top:107.77%;"><img id="ZX6HysxA9JwdCyuWQNzNMP" name="PA32QCV gray Cinema P3" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/ZX6HysxA9JwdCyuWQNzNMP.jpg" mos="" link="" align="" fullscreen="" width="605" height="652" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 6 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:605px;"><p class="vanilla-image-block" style="padding-top:107.77%;"><img id="RXmEaKrPYF7Kyjq9bzvyLP" name="PA32QCV gray 709" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/RXmEaKrPYF7Kyjq9bzvyLP.jpg" mos="" link="" align="" fullscreen="" width="605" height="652" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 7 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:605px;"><p class="vanilla-image-block" style="padding-top:107.77%;"><img id="NYoah65pauWUnnKrsFN5MP" name="PA32QCV gray native post" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/NYoah65pauWUnnKrsFN5MP.jpg" mos="" link="" align="" fullscreen="" width="605" height="652" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div></div></div><p>All the average error values are around 3dE, which tells me this PA32QCV is an early or pre-production sample. Other ProArt monitors I’ve reviewed are closer to 1dE. The green errors seen in the charts above can be adjusted away using the OSD. To confirm this, I calibrated the Native mode, represented by the last chart and its invisible error of 1.30dE. The only exception to this is sRGB mode, which is fixed at 80 nits with all color controls grayed out.</p><p>Gamma tracks perfectly in every case and uses the correct value for each mode. BT.709 and BT.2020 use the power function at 2.4. sRGB, Adobe RGB, and Display P3 use the power function at 2.2. Cinema P3 uses the power function at 2.6. I calibrated Native also using the power function at 2.2</p><h2 id="color-gamut-accuracy-2">Color Gamut Accuracy</h2><p><strong>Our color gamut and volume testing use</strong><a data-analytics-id="inline-link" href="https://www.portrait.com/"><strong> </strong></a><a data-analytics-id="inline-link" href="https://www.portrait.com/"><strong>Portrait Displays’</strong></a><strong> Calman software. For details on our color gamut testing and volume calculations,</strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking/3"><strong> </strong></a><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking/3"><strong>click here.</strong></a></p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:490px;"><p class="vanilla-image-block" style="padding-top:104.08%;"><img id="zrmHB7ctAoqfopH4cfNKva" name="PA32QCV color sRGB" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/zrmHB7ctAoqfopH4cfNKva.jpg" mos="" link="" align="" fullscreen="" width="490" height="510" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:490px;"><p class="vanilla-image-block" style="padding-top:104.08%;"><img id="FBR8Pq96VLvxmW352xDMva" name="PA32QCV color Adobe" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/FBR8Pq96VLvxmW352xDMva.jpg" mos="" link="" align="" fullscreen="" width="490" height="510" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:490px;"><p class="vanilla-image-block" style="padding-top:104.08%;"><img id="bUX7DjSXMX9M6JwrQQKWva" name="PA32QCV color 2020" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/bUX7DjSXMX9M6JwrQQKWva.jpg" mos="" link="" align="" fullscreen="" width="490" height="510" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:490px;"><p class="vanilla-image-block" style="padding-top:104.08%;"><img id="83HxsvXGVTjZ5fhxc7YHva" name="PA32QCV color P3" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/83HxsvXGVTjZ5fhxc7YHva.jpg" mos="" link="" align="" fullscreen="" width="490" height="510" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 5 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:490px;"><p class="vanilla-image-block" style="padding-top:104.08%;"><img id="8QGtctqM9LsXeLMgZnfDva" name="PA32QCV color Cinema P3" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/8QGtctqM9LsXeLMgZnfDva.jpg" mos="" link="" align="" fullscreen="" width="490" height="510" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 6 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:490px;"><p class="vanilla-image-block" style="padding-top:104.08%;"><img id="7qU56PbAEhzyHQJ5CP4Dva" name="PA32QCV color 709" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/7qU56PbAEhzyHQJ5CP4Dva.jpg" mos="" link="" align="" fullscreen="" width="490" height="510" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 7 of 7</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:490px;"><p class="vanilla-image-block" style="padding-top:104.08%;"><img id="byx84F6a66giTaCgqpiHva" name="PA32QCV color native post" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/byx84F6a66giTaCgqpiHva.jpg" mos="" link="" align="" fullscreen="" width="490" height="510" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div></div></div><p>The color charts all show precision with average errors under 2dE in all cases except BT.2020, which rendered 2.66. This is solely due to the lack of gamut volume, as the PA32QCV is a P3 monitor with no Quantum Dot tech. However, inner targets are on point. The only issue that can be seen in any chart is the slightly green white point that I spoke of earlier.</p><h2 id="grayscale-gamma-and-color-gamut-test-summary-2">Grayscale, Gamma and Color Gamut Test Summary</h2><p>Here are the grayscale, gamma and gamut results in a summary table. Error values are referenced to the Delta E 2000 spec. Gamma range refers to the difference between the highest and lowest value. Gamma Average is the percentage deviation from the standard for each color mode.</p><div ><table><tbody><tr><td class="firstcol empty" ></td><td
><p><strong>Grayscale Error</strong></p></td><td
><p><strong>Gamma Range</strong></p></td><td
><p><strong>Gamma Average</strong></p></td><td
><p><strong>Gamma Actual</strong></p></td><td
><p><strong>Gamut Error</strong></p></td><td
><p><strong>Gamut Volume</strong></p></td></tr><tr><td class="firstcol " ><p><strong>sRGB</strong></p></td><td
><p>2.86dE</p></td><td
><p>0.05</p></td><td
><p>0.91%</p></td><td
><p>2.22</p></td><td
><p>1.41</p></td><td
><p>99.38%</p></td></tr><tr><td class="firstcol " ><p><strong>Adobe RGB</strong></p></td><td
><p>3.40dE</p></td><td
><p>0.06</p></td><td
><p>0.91%</p></td><td
><p>2.22</p></td><td
><p>1.88</p></td><td
><p>88.22%</p></td></tr><tr><td class="firstcol " ><p><strong>BT.2020</strong></p></td><td
><p>3.17dE</p></td><td
><p>0.06</p></td><td
><p>0.42%</p></td><td
><p>2.41</p></td><td
><p>2.66</p></td><td
><p>72.72%</p></td></tr><tr><td class="firstcol " ><p><strong>Display P3</strong></p></td><td
><p>3.66dE</p></td><td
><p>0.05</p></td><td
><p>0.45%</p></td><td
><p>2.21</p></td><td
><p>1.56</p></td><td
><p>99.58%</p></td></tr><tr><td class="firstcol " ><p><strong>Cinema P3</strong></p></td><td
><p>3.42dE</p></td><td
><p>0.06</p></td><td
><p>0.38%</p></td><td
><p>2.59</p></td><td
><p>1.52</p></td><td
><p>99.48%</p></td></tr><tr><td class="firstcol " ><p><strong>BT.709</strong></p></td><td
><p>3.47dE</p></td><td
><p>0.06</p></td><td
><p>0.00%</p></td><td
><p>2.40</p></td><td
><p>1.53</p></td><td
><p>99.53%</p></td></tr><tr><td class="firstcol " ><p><strong>Native calibrated</strong></p></td><td
><p>1.30dE</p></td><td
><p>0.06</p></td><td
><p>0.45%</p></td><td
><p>2.21</p></td><td
><p>0.77</p></td><td
><p>99.70% </p></td></tr></tbody></table></div><p><strong>Test Takeaway: </strong>The PA32QCV has superb gamut and gamma accuracy with a few slight grayscale tracking issues seen in my review sample. Given the performance I’ve seen from other ProArt monitors, I conclude that this is an anomaly. At any rate, one can correct any errors easily using the available calibration methods.</p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-gaming-monitors,4533.html"><strong>Best Gaming Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking"><strong>How We Test PC Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/monitor-buying-guide,5699.html"><strong>How to Buy a PC Monitor</strong></a></p><p><strong>Our HDR benchmarking uses</strong><a data-analytics-id="inline-link" href="https://www.portrait.com/"><strong> </strong></a><a data-analytics-id="inline-link" href="https://www.portrait.com/"><strong>Portrait Displays’</strong></a><strong> Calman software. To learn about our HDR testing, see our breakdown of</strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking/5"><strong> how we test PC monitors.</strong></a></p><p>The PA32QCV supports HDR10 signals by switching automatically. There are three PQ options available, each with a slightly different luminance curve.</p><h2 id="hdr-brightness-and-contrast-2">HDR Brightness and Contrast</h2><ul><li><strong>HDR White Luminance</strong> – 644.9141 nits</li><li><strong>HDR Black Level</strong> – 0.0660 nit</li><li><strong>HDR Sequential Contrast</strong> – 9,771.3:1</li></ul><p>To find the highest possible contrast ratio, I turned on the dynamic dimming option with its default parameters. At nearly 10,000:1, the PA32QCV delivers superb HDR image quality. Black levels are deep with clear detail, which highlights pop with texture. The best option is PQ Optimized, which, as you’ll see below, delivers the correct luminance tracking.</p><h2 id="grayscale-eotf-and-color-2">Grayscale, EOTF and Color</h2><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:750px;"><p class="vanilla-image-block" style="padding-top:63.47%;"><img id="vohsufRCKgN8TgYSQRCsLP" name="PA32QCV HDR Gray EOTF" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/vohsufRCKgN8TgYSQRCsLP.jpg" mos="" link="" align="" fullscreen="" width="750" height="476" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:500px;"><p class="vanilla-image-block" style="padding-top:86.00%;"><img id="5WWkU3NYkatYBcN8e67Pva" name="PA32QCV HDR P3" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/5WWkU3NYkatYBcN8e67Pva.jpg" mos="" link="" align="" fullscreen="" width="500" height="430" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:500px;"><p class="vanilla-image-block" style="padding-top:86.00%;"><img id="yHqvzpepPnpDnK7THNMHva" name="PA32QCV HDR 2020" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/yHqvzpepPnpDnK7THNMHva.jpg" mos="" link="" align="" fullscreen="" width="500" height="430" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Portrait Displays Calman)</span></figcaption></figure></div></div></div><p>The grayscale result has no issues to report aside from a touch too much blue around 60 and 65% brightness. The luminance curve starts a tad light but meets the reference line by 10% and correctly transitions to tone-mapping at the 70% step.</p><p>The DCI-P3 color result shows some undersaturation, which is also likely to be a sample-specific issue. The PA32QCV is fully capable of rendering all of this gamut as the SDR tests showed. In practice, HDR color looks a bit muted though there is no shortage of brightness. The BT.2020 test shows the same behavior with general undersaturation until color runs out at 82% red, 65% green and 95% blue.</p><p><strong>Test Takeaway: </strong>The PA32QCV has more HDR brightness and contrast than typical IPS monitors with edge backlighting. Dynamic dimming is very effective and takes the ratio up to nearly 10,000:1 without penalty. HDR grayscale and EOTF tracking is on point but color in my sample was slightly muted.</p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-gaming-monitors,4533.html"><strong>Best Gaming Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking"><strong>How We Test PC Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/monitor-buying-guide,5699.html"><strong>How to Buy a PC Monitor</strong></a></p><p>There is no escaping the fact that higher pixel density means a sharper and smoother image. When you paint a picture with dots, you can’t have too many. 4K has been, and still is the gold standard up to the 32-inch panel size. But inevitably, someone will find a way to pack even more dots into the same area.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1000px;"><p class="vanilla-image-block" style="padding-top:66.60%;"><img id="GYTpjWpG5xL3ASxJWoanMd" name="a-final" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/GYTpjWpG5xL3ASxJWoanMd.jpg" mos="" align="middle" fullscreen="" width="1000" height="666" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Asus)</span></figcaption></figure><p>The Asus ProArt PA32QCV sports the same 218ppi that I saw in last year’s PA27JCV, but has a 32-inch panel instead of 27. It’s hard to describe how cool it is to get right up to the screen and still be unable to see the pixels. And with the addition of LuxPixel technology, there is no change in quality when viewing from the sides, nor do ambient light reflections affect the picture.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1280px;"><p class="vanilla-image-block" style="padding-top:82.03%;"><img id="SzVqJUMRGyjrbmvY8UmH3g" name="a-main" alt="Asus ProArt PA32QCV" src="https://cdn.mos.cms.futurecdn.net/SzVqJUMRGyjrbmvY8UmH3g.jpg" mos="" align="middle" fullscreen="1" width="1280" height="1050" attribution="" endorsement="" class="expandable"></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>The PA32QCV sticks to the ProArt model of correctly termed picture modes so it’s super easy to set up and use. Just pick the standard you need and go. My sample had slight grayscale errors, which in any other monitor would be a non-issue. But it was a tad less precise than other ProArt screens. Fixing this was easy since I could adjust RGB values independently for each mode. And the flexibility of Calman autocal or Asus DisplayWidget Center means there are multiple ways to tune the monitor. I also noted that the PA32QCV was one of the brightest and most contrasty IPS screens I’ve seen to date. It hit 650 nits in SDR and HDR, full field and window pattern, and topped 1,600:1 natively.</p><p>At $1,299, the Asus ProArt PA32QCV is a relative bargain among professional monitors. It doesn’t have fast refresh or Mini LED, but it still delivers precise performance. If you need more pixel density than 4K offers, it’s definitely worth checking out.</p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/best-gaming-monitors,4533.html"><strong>Best Gaming Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reference/how-we-test-pc-monitors-benchmarking"><strong>How We Test PC Monitors</strong></a></p><p><strong>MORE: </strong><a data-analytics-id="inline-link" href="https://www.tomshardware.com/reviews/monitor-buying-guide,5699.html"><strong>How to Buy a PC Monitor</strong></a></p>
https://www.tomshardware.com/monitors/asus-proart-pa32qcv-32-inch-6k-professional-monitor-review
Asus ups the pixel density ante with its 6K ProArt PA32QCV. This 32-inch IPS display has 6016x3384 resolution, 218ppi, HDR10, DisplayHDR 600 and color modes for every industry standard. It serves creatives with precision at a relatively low price.
Raan7pjXHAiSv2DKeuY8r3
Wed, 17 Sep 2025 12:00:00 +0000 Monitors
Christian Eberle
Tom&#039;s Hardware
Asus PA32QCV
Asus PA32QCV
<![CDATA[ Hack to the Future — here's how you can write BASIC code on a modern-day PC ]]>
<p>The Beginner’s All-Purpose Symbolic Instruction Code, BASIC, is where I started my coding journey. Sat, aged four or five in front of a Commodore 16, I typed in lines of words and numbers which made up an application or sound effect. Sometimes they worked, often they did not. The words that I typed, along with “syntax error,” meant nothing to a five-year-old child who wanted sugar-coated cereal and Transformers cartoons, but I persevered.</p><p>Fast forward to the 21st century, and I still use BASIC from time to time. I write BASIC on a Commodore 64 and a ZX Spectrum that I recently renovated. I’ve even written <a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/raspberry-pi-pico-basic-controlled-neopixels">BASIC on a Raspberry Pi Pico</a> to control a series of NeoPixels.</p><p>Recently, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/bill-gates-48-year-old-microsoft-6502-basic-goes-open-source" target="_blank">Microsoft open-sourced its MOS,6502 BASIC</a> from 1978, and that got me thinking about writing BASIC on a PC. Sure, I remember QBasic and the gorillas.bas game. I loved tweaking that game on my 486 DX33 back in the 1990s,
but I wanted to write BASIC on my Windows 10 PC in 2025.</p><p>I can’t use MOS6502 BASIC without spending a lot of time getting it to work on my PC, so what can I use instead? It turns out that there is, and it is called <a data-analytics-id="inline-link" href="https://www.qb64phoenix.com/" target="_blank"><u>QB64 Phoenix Edition,</u></a> and it looks and feels just like QBasic, but with many more features.</p><ul><li>Realtime input error checking</li><li>Online and offline help system</li><li>Syntax highlighting</li><li>Automatic source formatting</li><li>Debugging</li><li>Compiles native binaries for Windows, macOS and Linux</li></ul><p>QB64 Phoenix Edition (QB64 PE) is a fresh offshoot of the original QB64 project, which seemed to die out in the early 2020s. The project’s goal is to keep the spirit of the old application alive and to provide more people with access to running BASIC on their computers. QB64 PE is cross-platform and it has compatibility with QBasic and QuickBasic 4.5 code. But QB64 PE also gives us an extended BASIC and OpenGL, meaning that we can create applications/games with graphics and sound.</p><p>So let's take a look at QB64 PE and create our own BASIC project.</p><h2 id="getting-started-with-qb64-pe-2">Getting Started with QB64 PE</h2><p>It really couldn’t be any simpler to get started with QB64 PE.</p><p><strong>1. Go to the </strong><a data-analytics-id="inline-link" href="https://www.qb64phoenix.com/"><u><strong>QB4 PE project page</strong></u></a><strong> and download the package for your operating system.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1022px;"><p class="vanilla-image-block" style="padding-top:33.46%;"><img id="stkPJXAqq3TYA7d25LzWpS" name="qb1" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/stkPJXAqq3TYA7d25LzWpS.jpg" mos="" align="middle" fullscreen="" width="1022" height="342" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>2. Extract the file to a directory.</strong></p><p><strong>3. Navigate to that directory and double left click on the qb64pe.exe application.</strong></p><div class="inlinegallery
mosaic-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:622px;"><p class="vanilla-image-block" style="padding-top:13.18%;"><img id="SFtZoFMytDTRNub5gVW8oS" name="qb2" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/SFtZoFMytDTRNub5gVW8oS.jpg" mos="" link="" align="" fullscreen="" width="622" height="82" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 2</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1274px;"><p class="vanilla-image-block" style="padding-top:109.26%;"><img id="PYVZv3eqgfso4CS7XEBssS" name="qb-ui.JPG" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/PYVZv3eqgfso4CS7XEBssS.jpg" mos="" link="" align="" fullscreen="" width="1274" height="1392" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure></div></div></div><p>The QB64 PE user interface is very much like QBasic. We have the menu at the top, the coding area in the middle, and a status/output/debugging area at the bottom. Our project will see us working with the File and Run menu.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1274px;"><p class="vanilla-image-block" style="padding-top:109.26%;"><img id="gNtNq8QMX27GSGtWKHJKrS" name="qb3.JPG" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/gNtNq8QMX27GSGtWKHJKrS.jpg" mos="" align="middle" fullscreen="" width="1274" height="1392" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>On first boot, QB64 PE will tell us that the application and any executables made by it can be falsely identified by your anti-virus package. You can optionally whitelist the directory, but I have not had any issues in my testing.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1262px;"><p class="vanilla-image-block" style="padding-top:103.65%;"><img id="88yjGmNbfvkDgS8j9ggA6T" name="wiki" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/88yjGmNbfvkDgS8j9ggA6T.jpg" mos="" align="middle" fullscreen="" width="1262" height="1308" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>QB64 PE is an awesome application, and I urge you to dig into the <a data-analytics-id="inline-link" href="https://qb64phoenix.com/qb64wiki/index.php/Main_Page"><u>gloriously in-depth documentation</u></a> (which is where I found how to show images) and learn more about QB64 PE.</p><h2 id="a-short-history-of-basic-2">A short history of BASIC</h2><p>The BASIC language has been with us since 1964, and it was developed as a high-level (easy-to-read) language by John G Kemeny, chairman of the Dartmouth College Math Department. The name BASIC came from Thomas Kurtz.</p><p>The structure of BASIC is meant to be over-simplified on purpose. Each line of BASIC is read by the compiler and turned into bytecode, which the machine can then run. But BASIC can be compiled or interpreted, depending on the version you are using. For example, on the Commodore 64, we see Commodore 64 BASIC V2, a version of Microsoft BASIC, which has a limited number of keywords.</p><h2 id="the-project-goal-to-make-a-simple-game-2">The Project Goal: To Make A Simple Game</h2><p>Learning BASIC, or any coding language, really, is best done by creating a project. I could sit here and teach you about core programming concepts, or we could have some fun and make a simple game. The game in question is inspired by Usborne’s 1980s BASIC coding books, specifically Computer Space Games, which I adored as a child.</p><p>The game is “Escape from planet LXF329,” and the goal of the game is to take off and escape a group of aliens who are trying to capture the spaceship. Essentially, this is a number-guessing game, but with a sci-fi theme.</p><ul><li>Learn the BASIC syntax</li><li>Learn how to display images</li><li>Use conditional statements</li><li>Create variables for integers and strings</li><li>Get user input</li></ul><p>You may have spotted “Learn how to display images,” and yes, I will show you how to display a static image on the screen using QB64 PE. In the interest of full disclosure, the images used in the game were generated using Adobe Firefly, an AI service. Why? I could’ve used stock images or spent some time in GIMP and Inkscape, but I didn’t have the time, nor the talent, so Adobe Firefly did the work for me.</p><p>We’ll start by creating the starting screen, an image that advertises the game and sets the scene.</p><p><strong>1. Create a new blank document and click on File >> Save and save the project as space.bas.</strong> Remember to save often.</p><p>2. Create a new screen object, setting the screen to 1024 x 1024 pixels and 32-bit color, then set the window title to ESCAPE FROM PLANET LXF329”.</p><pre class="line-numbers language-basic" language="basic" ><code>Screen _NewImage(1024, 1024, 32)_Title "ESCAPE FROM PLANET LXF329"</code></pre><p><strong>3. Create a variable, myImage& and store your image.</strong></p><pre class="line-numbers language-basic" language="basic" ><code>myImage& = _LoadImage("title.png")</code></pre><p><strong>4. Create an error handler to check that the image has been loaded correctly.</strong> This is basically a conditional check that checks for the loaded file in the variable. If there is nothing there, 0, then it will print an error message.</p><pre class="line-numbers language-basic" language="basic" ><code>If myImage& = 0 Then
Print "Error loading image!"
EndEnd If</code></pre><p><strong>5. Put the image onto the screen at position 0,0 (top left) then pause the code, waiting for the user to press a key.</strong></p><pre class="line-numbers language-basic" language="basic" ><code>_PutImage (0, 0), myImage&Sleep</code></pre><p><strong>6. Save the code, and click on Run >> run Only (no EXE) to start the code. Click OK in the dialog to start.</strong> You should see the image appear on the screen. <strong>Press any key to close the window.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1279px;"><p class="vanilla-image-block" style="padding-top:109.30%;"><img id="pRxWSaRuacNynXdW5qY73T" name="qb5" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/pRxWSaRuacNynXdW5qY73T.jpg" mos="" align="middle" fullscreen="" width="1279" height="1398" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>7. Create a label, START: </strong>Labels enable us to write code that can jump around to different sections by using a Go To command. You may be familiar with using numbers and jumping to a numbered line. Here ,we are not using numbers, so labels will do the same job.</p><pre class="line-numbers language-basic" language="basic" ><code>START:</code></pre><p><strong>8. To begin the game code, clear the screen (CLS) and then use a series of print statements to print a narrative</strong> to get the reader up to speed on the game mechanics and story.</p><pre class="line-numbers language-basic" language="basic" ><code>ClsPrint "ESCAPE FROM PLANET LXF329"Print "Your vessel, the USS TOMSHARDWARE has landed on LXF329"Print "on a routine survey mission"PrintPrint "But the evil aliens, led by HOAL are after your blood!"PrintPrint "Calculate the thrust necessary to take-off and escape"Print "the planet's gravity!"PrintPrint "You only have ten seconds until the aliens burst into"Print "the starship and lay waste to your crew!"</code></pre><p><strong>9. Ensure that the game uses pseudo-random numbers.</strong> If we don’t do this then the game will pick the same numbers each time.</p><pre class="line-numbers language-basic" language="basic" ><code>Randomize Timer</code></pre><p><strong>10. Create two variables</strong>, g (gravity) and w (weight), which are random integers, multiplied by 20 and 40, respectively. BASIC uses LET to create variables.</p><pre class="line-numbers language-basic" language="basic" ><code>Let g = Int(Rnd * 20)Let w = Int(Rnd * 40)</code></pre><p><strong>11. Create a third variable to store the correct thrust to leave the planet.</strong></p><pre class="line-numbers language-basic" language="basic" ><code>Let r = g * w</code></pre><p><strong>12. For debug purposes, print the answer to the screen. </strong>Later, comment this out using the REM (remark) keyword.</p><pre class="line-numbers language-basic" language="basic" ><code>Print r</code></pre><p>Change to this for the final game</p><pre class="line-numbers language-basic" language="basic" ><code>REM Print r</code></pre><p><strong>13. Print the planet’s gravity and then ask the user to make their thrust calculation.</strong></p><pre class="line-numbers language-basic" language="basic" ><code>Print "Planet Gravity = "; gPrint "Type in thrust to escape: "</code></pre><p><strong>14. Using a for loop, we give the player ten guesses before the aliens enter the ship.</strong></p><pre class="line-numbers language-basic" language="basic" ><code>For c = 1 To 10</code></pre><p><strong>15. Stored the player’s guess in a variable, f.</strong> That value is compared to the answer, r. If the thrust is too high or low, the for loop repeats until the player runs out of guesses. If the guess is correct, then the game goes to the ENDGAME label.</p><pre class="line-numbers language-basic" language="basic" ><code>For c = 1 To 10
Input f
If f > r Then Print "Thrust too high";
If f < r Then Print "Thrust too low";
If f = r Then GoTo ENDGAMENext c</code></pre><p><strong>16. Print “the bad ending” to the screen. </strong>This section runs if the player fails to make the correct calculations, and then the for loop ends, throwing the user into this nightmare scenario.</p><pre class="line-numbers language-basic" language="basic" ><code>PrintPrint "The aliens have entered the starship and you and your"Print "crew are now their prisoners!"Print</code></pre><p><strong>17. Add a graphic to show your ship losing against the alien onslaught.</strong> You will need to create a PNG file, stored in the same directory as the game file. This is the same code as used for the start screen. Sleeping for 5 is important; otherwise, there is a bug where the screen automatically closes and ends the game.</p><pre class="line-numbers language-basic" language="basic" ><code>Screen _NewImage(1024, 1024, 32)_Title "YOU DIED!"myImage& = _LoadImage("die.png")If myImage& = 0 Then
Print "Error loading image!"
EndEnd If_PutImage (0, 0), myImage&Sleep 5</code></pre><p><strong>18. Using a GoTo, send the player to the REPLAY section of code.</strong></p><pre class="line-numbers language-basic" language="basic" ><code>GoTo REPLAY</code></pre><p><strong>19. Print a message to the user to say that they have won. Then show an image on the screen.</strong> The ENDGAME label is where the successful player is sent.<strong> </strong>They are rewarded with a message and an escape image showing them blasting off into space!</p><pre class="line-numbers language-basic" language="basic" ><code>Print "You have successfully taken off!"Print "The aliens burn in the wake of your engines"Screen _NewImage(1024, 1024, 32)_Title "YOU WON!"myImage& = _LoadImage("escape.png")If myImage& = 0 Then
Print "Error loading image!"
EndEnd If_PutImage (0, 0), myImage&Sleep 5</code></pre><p><strong>20. Create a means to ask the player if they would like to try again.</strong> The player’s input is stored as a variable, a. But note that the variable contains a string ($) which can be y or n. If it is y, the game goes back to the START label. Otherwise, the game stops. We use the REPLAY label to identify what the code is for and to direct the player through the game.</p><pre class="line-numbers language-basic" language="basic" ><code>REPLAY:Print "Would you like to try again? Press y or n"Input a$If a$ = "y" Then
GoTo STARTElse
StopEnd If</code></pre><p><strong>21. Save the code, and click on Run >> run Only (no EXE) to start the code. Click OK in the dialog to start.</strong></p><p><strong>22. Run the game a few times, win and lose.</strong> Remember that the answer is printed to the screen, so make sure to comment that out when letting your friends have a go.</p><h2 id="complete-code-listing-2">Complete Code Listing</h2><pre class="line-numbers language-basic" language="basic" ><code>Screen _NewImage(1024, 1024, 32)_Title "ESCAPE FROM PLANET LXF329"myImage& = _LoadImage("title.png")If myImage& = 0 Then
Print "Error loading image!"
EndEnd If_PutImage (0, 0), myImage&SleepSTART:ClsPrint "ESCAPE FROM PLANET LXF329"Print "Your vessel, the USS TOMSHARDWARE has landed on LXF329"Print "on a routine survey mission"PrintPrint "But the evil aliens, led by HOAL are after your blood!"PrintPrint "Calculate the thrust necessary to take-off and escape"Print "the planet's gravity!"PrintPrint "You only have ten seconds until the aliens burst into"Print "the starship and lay waste to your crew!"Randomize TimerLet g = Int(Rnd * 20)Let w = Int(Rnd * 40)Let r = g * wPrint rPrint "Planet Gravity = "; gPrint "Type in thrust to escape: "For c = 1 To 10
Input f
If f > r Then Print "Thrust too high";
If f < r Then Print "Thrust too low";
If f = r Then GoTo ENDGAMENext cPrintPrint "The aliens have entered the starship and you and your"Print "crew are now their prisoners!"PrintScreen _NewImage(1024, 1024, 32)_Title "YOU WON!"myImage& = _LoadImage("die.png")If myImage& = 0 Then
Print "Error loading image!"
EndEnd If_PutImage (0, 0), myImage&Sleep 5GoTo REPLAYENDGAME:Print "You have successfully taken off!"Print "The aliens burn in the wake of your engines"Screen _NewImage(1024, 1024, 32)_Title "YOU WON!"myImage& = _LoadImage("escape.png")If myImage& = 0 Then
Print "Error loading image!"
EndEnd If_PutImage (0, 0), myImage&Sleep 5REPLAY:Print "Would you like to try again? Press y or n"Input a$If a$ = "y" Then
GoTo STARTElse
StopEnd If</code></pre><h2 id="sharing-your-work-2">Sharing your work!</h2><p>If you want to share the game with your friends, then you can! Just bear in mind that the resulting executable is bound to the OS it was created with. Additionally, as QB64 PE notes, your executable may be flagged as a false positive by your antivirus.</p><p><strong>1. Click on Run >> Make EXE Only. You can also click on Run >> Start to compile and run the code. This way you get both the executable, and the code will run in the editor.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:548px;"><p class="vanilla-image-block" style="padding-top:49.82%;"><img id="imjPFBbd2XWxRMRbZSSCpS" name="exe1" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/imjPFBbd2XWxRMRbZSSCpS.jpg" mos="" align="middle" fullscreen="" width="548" height="273" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>2. Click OK to compile the executable into the qb64pe folder</strong>, the same folder where QB64 PE is being run.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:585px;"><p class="vanilla-image-block" style="padding-top:32.48%;"><img id="JYqzN67e9BnXw6tBYv99nS" name="exe2" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/JYqzN67e9BnXw6tBYv99nS.jpg" mos="" align="middle" fullscreen="" width="585" height="190" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>3. Wait for the code to compile. Check the status bar at the bottom of the screen to monitor the progress.</strong> On modern systems this will take seconds.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1270px;"><p class="vanilla-image-block" style="padding-top:16.77%;"><img id="yWBBvsYVU3j8v8nNtomHpS" name="exe3" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/yWBBvsYVU3j8v8nNtomHpS.jpg" mos="" align="middle" fullscreen="" width="1270" height="213" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>4. Navigate to the folder with the compiled game code.</strong> I scanned my compiled code using Microsoft Defender and Malwarebytes, and nothing nasty was found.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1024px;"><p class="vanilla-image-block" style="padding-top:70.31%;"><img id="u7sLFfo7sJMiPr6vg2KXpS" name="exe4.JPG" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/u7sLFfo7sJMiPr6vg2KXpS.jpg" mos="" align="middle" fullscreen="" width="1024" height="720" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><strong>5. Double click on the executable to run the game.</strong></p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1272px;"><p class="vanilla-image-block" style="padding-top:54.01%;"><img id="VLUzeJjeHe6GTGmqGwMVFT" name="game-small" alt="BASIC 2025" src="https://cdn.mos.cms.futurecdn.net/VLUzeJjeHe6GTGmqGwMVFT.gif" mos="" align="middle" fullscreen="" width="1272" height="687" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">The images used in the game are AI generated using Adobe Firefly </span><span class="credit" itemprop="copyrightHolder">(Image credit: The images used in the game are AI generated using Adobe Firefly)</span></figcaption></figure>
https://www.tomshardware.com/software/programming/hack-to-the-future-heres-how-you-can-write-basic-code-on-a-modern-day-pc
BASIC was the Python of its day and it inspired many bedroom coders to spend hours in their room, hacking around to make games and tools. Now you can do the same in the 21st century.
iqZoxKqL6xBngcrDieFPsG
Wed, 17 Sep 2025 11:49:35 +0000 Programming
Software
Les Pounder
AI generated using Adobe Firefly
BASIC 2025
BASIC 2025
<![CDATA[ 'Beyond EUV' chipmaking tech pushes Soft X-Ray lithography closer to challenging Hyper-NA EUV — 'B-EUV' uses new resist chemistry to make smaller chips ]]>
<p>Researchers at Johns Hopkins University have unveiled a new approach to chipmaking that uses lasers with a 6.5nm ~ 6.7nm wavelength — also known as Soft X-rays — that could increase the resolution of lithography tools to 5nm and below, reports <a data-analytics-id="inline-link" href="https://cosmosmagazine.com/science/engineering/microchip-beyond-extreme-uv/" target="_blank">Cosmos,</a> citing a paper published in <a data-analytics-id="inline-link" href="https://www.nature.com/articles/s44286-025-00273-z" target="_blank">Nature</a>.</p><p>The scientists call their method 'beyond-EUV' — suggesting that their technology could replace industry-standard EUV lithography — but the researchers admit they are currently years away from building even an experimental B-EUV tool.</p><h2 id="soft-x-rays-can-challenge-hyper-na-on-paper-2">Soft X-Rays can challenge Hyper-NA. On paper</h2><p>The most advanced chips nowadays are made using EUV lithography, which operates at a wavelength of 13.5 nm and can produce features as small as 13nm (Low-NA EUV of 0.33 numerical aperture), 8nm (<a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/manufacturing/asml-fires-back-at-accusations-that-its-next-gen-high-na-euv-chipmaking-tools-are-too-expensive">High-NA EUV</a> of 0.55 NA), or even 4nm ~ 5nm (<a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/manufacturing/asml-explores-hyper-na-chipmaking-tools-as-the-next-step-in-shrinking-transistors-tools-would-debut-in-2030-but-significant-technology-and-cost-hurdles-remain">Hyper-NA EUV</a> on 0.7 – 0.75 NA) at the cost of extreme complexity of the lithography systems that have very advanced optics that cost hundreds of millions of dollars.</p><p>By using a shorter wavelength, researchers from Johns Hopkins University can get an intrinsic resolution boost even with lenses with moderate NA. However, they face many challenges with B-EUV.</p><p>Firstly, B‑EUV light sources are not yet ready. Various researchers have tried <a data-analytics-id="inline-link" href="https://www.researchgate.net/publication/240395264_Reflective_multilayer_optics_for_67_nm_wavelength_radiation_sources_and_next_generation_lithography">multiple methods</a> of <a data-analytics-id="inline-link" href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-33-4-8806&id=568351">generating 6.7 nm wavelength radiation</a> (e.g., gadolinium laser-produced plasma), but there is no industry-standard approach. Secondly, these shorter wavelengths — due to their high photon energy — interact poorly with traditional photoresist materials used in chipmaking. Thirdly, because 6.5nm ~ 6.7nm wavelength light is absorbed rather than reflected by pretty much everything, multilayer-coated mirrors for this type of radiation haven't been produced before.</p><div ><table><tbody><tr><td class="firstcol " ><p>Lithography Type</p></td><td
><p>Wavelength</p></td><td
><p>Achievable Resolution</p></td><td
><p>Photon Energy</p></td><td
><p>Numerical Aperture (NA)</p></td><td
><p>Notes </p></td></tr><tr><td class="firstcol " ><p>g-line (Pre-DUV)</p></td><td
><p>436 nm</p></td><td
><p>500 nm</p></td><td
><p>2.84 eV</p></td><td
><p>0.3</p></td><td
><p>Uses mercury vapor lamps; legacy nodes; low resolution. </p></td></tr><tr><td class="firstcol " ><p>i-line (Pre-DUV)</p></td><td
><p>365 nm</p></td><td
><p>350 nm</p></td><td
><p>3.40 eV</p></td><td
><p>0.3</p></td><td
><p>Used for early CMOS. </p></td></tr><tr><td class="firstcol " ><p>KrF DUV</p></td><td
><p>248 nm</p></td><td
><p>90 nm</p></td><td
><p>5.00 eV</p></td><td
><p>0.7 - 1.0</p></td><td
><p>Used from ~130 nm to 90 nm; excimer laser source; still used in backend layers. </p></td></tr><tr><td class="firstcol " ><p>ArF DUV</p></td><td
><p>193 nm</p></td><td
><p>65 nm (dry) - 45 nm (immersion + multipatterning)</p></td><td
><p>6.42 eV</p></td><td
><p>Up to 1.35 (immersion)</p></td><td
><p>Most advanced DUV; still essential in multi-patterned 7 nm–5 nm nodes; used for many layers in 2nm nodes. </p></td></tr><tr><td class="firstcol " ><p>EUV</p></td><td
><p>13.5 nm</p></td><td
><p>13 nm (native), 8 nm (multi-patterning)</p></td><td
><p>92 eV</p></td><td
><p>0.33</p></td><td
><p>In volume production for 5nm - 2nm nodes. Will be used for years to come. </p></td></tr><tr><td class="firstcol " ><p>High-NA EUV</p></td><td
><p>13.5 nm</p></td><td
><p>8 nm (native), 5 nm (extended)</p></td><td
><p>92 eV</p></td><td
><p>0.55</p></td><td
><p>First tools: ASML EXE:5200B; targets beyond 2 nm-class nodes; reduced field size, higher cost. </p></td></tr><tr><td class="firstcol " ><p>Hyper-NA EUV (future)</p></td><td
><p>13.5 nm</p></td><td
><p>4 nm or better (theoretical)</p></td><td
><p>92 eV</p></td><td
><p>0.75 or more</p></td><td
><p>Future tech; requires exotic mirrors and ultra-high precision engineering. </p></td></tr><tr><td class="firstcol " ><p>Soft X-ray / B-EUV</p></td><td
><p>6.5 nm - 6.7 nm</p></td><td
><p>less than 5 nm (theoretical)</p></td><td
><p>185-190 eV</p></td><td
><p>0.3 - 0.5 (expected)</p></td><td
><p>Experimental; high-energy photons; new metal-organic resist chemistries under test.</p></td></tr></tbody></table></div><p>Finally, these lithography tools must be designed from scratch, and currently, there is no ecosystem to support the designs with components and consumables. To sum up, building a B-EUV machine (or Soft X-ray machine?) requires breakthroughs in light sources, projection mirrors, resists, and even consumables like pellicles or photomasks.</p><h2 id="solving-challenges-one-at-a-time-2">Solving challenges one at a time</h2><p>Researchers at Johns Hopkins University, led by Professor Michael Tsapatsis, explored how certain metals can improve the interaction between B-EUV (around 6 nm wavelength) light and resist materials used in chipmaking (i.e., they did not work on other challenges associated with Soft X-rays).</p><p>The team discovered that metals like zinc are able to absorb B-EUV light and emit electrons, which then trigger chemical reactions in organic compounds called imidazoles. These reactions make it possible to etch very fine patterns onto semiconductor wafers.</p><p>Interestingly, while zinc performs poorly with traditional 13.5nm EUV light, it becomes highly effective at shorter wavelengths, highlighting how important it is to match the material with the right wavelength.</p><p>To apply these metal–organic compounds to silicon wafers, the researchers developed a technique called chemical liquid deposition (CLD). This method creates thin, mirror-like layers of a material called aZIF (amorphous zeolitic imidazolate frameworks), growing at a rate of 1nm per second. CLD also allows for fast testing of different metal–imidazole combinations, making it easier to discover the best pairings for different lithography wavelengths. While zinc is well suited for B-EUV, the team noted that other metals might perform better at different wavelengths, offering flexibility for future chipmaking technologies.</p><p>This approach gives manufacturers a toolbox of at least 10 metal elements and hundreds of organic ligands to create custom resists tailored to specific lithography platforms, the researchers disclosed.</p><h2 id="summary-2">Summary</h2><p>Although the researchers did not solve the full stack of B-EUV challenges (e.g., source power, masks), they advanced one of the most critical bottlenecks: finding resist materials that can work with 6nm wavelength light. They created the CLD process to apply thin, uniform films of amorphous zeolitic imidazolate frameworks (aZIFs) onto silicon wafers. They experimentally showed that certain metals (like zinc) can absorb Soft X-ray light and emit electrons that trigger chemical reactions in imidazole-based resists.</p><p>There are plenty of challenges to be solved with B-EUV, and the technology doesn't have a clear path to the mass market. However, the CLD process can be used pretty widely, both in semiconductor and non-semiconductor applications.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/semiconductors/beyond-euv-chipmaking-tech-pushes-soft-x-ray-lithography-closer-to-challenging-hyper-na-euv-b-euv-uses-new-resist-chemistry-to-make-smaller-chips
Researchers at Johns Hopkins University have developed a new resist chemistry and deposition method optimized for 6.5 nm B-EUV light, marking a key step toward future Soft X-ray lithography. However, major challenges like light sources and tool infrastructure remain unresolved.
PnxwP79RqxCPRMQwwEebJP
Wed, 17 Sep 2025 11:38:04 +0000 Semiconductors
Tech Industry
Manufacturing
ashilov@gmail.com (Anton Shilov)
Anton Shilov
Micron
Micron
Micron
<![CDATA[ China bans its biggest tech companies from acquiring Nvidia chips, says report — Beijing claims its homegrown AI processors now match H20 and RTX Pro 6000D ]]>
<p>The Cyberspace Administration of China (CAC), the country’s top internet regulator, has reportedly banned its biggest tech companies, including ByteDance and Alibaba, from buying Nvidia’s AI chips. According to the <a data-analytics-id="inline-link" href="https://www.ft.com/content/12adf92d-3e34-428a-8d61-c9169511915c"><em>Financial Times</em></a>, the CAC said that these institutions should stop testing the new RTX Pro 6000D and cancel their orders, even though several companies had already indicated their interest in purchasing tens of thousands of these GPUs, which were <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-rtx-pro-6000d-b40-blackwell-gpus-reportedly-set-to-supersede-banned-h20-accelerators-in-china">set to replace the H20</a> after it was banned (but before it was unbanned again). This goes against the initial reports that <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidias-china-exclusive-rtx-6000d-reportedly-gets-lukewarm-reception-in-china-due-to-hobbled-performance-could-leave-nvidia-with-huge-backlog-of-unwanted-gpus">reception for the more affordable AI China-specific GPU was lukewarm</a> —
instead, it turns out that the central government was blocking the purchase of these graphics cards. This new ban comes just weeks after companies were directed to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/china-tells-tech-giants-to-halt-nvidia-h20-orders-after-u-s-officials-addiction-remark-chinese-leaders-call-lutnicks-comments-insulting">stop ordering Nvidia H20 chips</a>, too.</p><p>Beijing reportedly believes that homegrown AI chip makers, like Huawei and Cambricon, now produce chips that have comparable performance to Nvidia’s China-only products. And although Team Green might still have an advantage with its software stack, other Chinese tech giants like Tencent are <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/semiconductors/tencent-goes-public-with-pivot-to-chinese-chips">pushing to build their own infrastructure</a> to replace that. Because of these developments, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/chinas-chip-champions-ramp-up-production-of-ai-accelerators-at-domestic-fabs-but-hbm-and-fab-production-capacity-are-towering-bottlenecks">China’s chip makers are ramping production</a> in anticipation of the glut of orders coming from companies that need AI chips but can’t purchase Nvidia products.</p><p>When approached for comment, Nvidia directed us to remarks made by CEO Jensen Huang in London on Wednesday morning. "We can only be in service of a market if the country wants us to be," adding, "I’m disappointed with what I see. But they have larger agendas to work out, between China and the US, and I’m understanding of that. We are patient about it. We’ll continue to be supportive of the Chinese government and Chinese companies as they wish.” Huang told reporters in London on Wednesday that he hopes to discuss Nvidia's ability to do business in China with President Trump during the latter's state visit to the UK.</p><p>This news comes soon after the country accused Nvidia of breaking its anti-monopoly law, with the chipmaker facing fines of up to 10% of its China revenue. However, some also believe that Beijing is making these moves to get a more favorable deal from the U.S. in trade negotiations, especially as <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-shows-china-specific-b30-chips-with-80-percent-of-the-performance-of-the-standard-blackwell-gpu-to-the-u-s-government-nvidia-ceo-says-approval-is-still-up-in-the-air">export approval for its Blackwell-based B30 chips</a>, which perform up to 80% of its latest products, is still up in the air.</p><p>On the other hand, some Chinese industry leaders believe that this move is part of the central government’s effort to break free from American technology and boost its homegrown semiconductor industry. “The message is now loud and clear,” one executive told the <em>Financial Times</em>. “Earlier, people had hopes of renewed Nvidia supply if the geopolitical situation improves. Now it’s all hands on deck to build the domestic system.”</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><u><em>Tom's Hardware on Google News</em></u></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><u><em>add us as a preferred source</em></u></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/artificial-intelligence/china-bans-its-biggest-tech-companies-from-acquiring-nvidia-chips-says-report-beijing-claims-its-homegrown-ai-processors-now-match-h20-and-rtx-pro-6000d
Beijing is reportedly telling its biggest tech companies to buy local instead of purchasing Nvidia's latest China-specific AI chips.
XGT2o274VFJMkM9nsr3MzD
Wed, 17 Sep 2025 11:26:57 +0000 Artificial Intelligence
Tech Industry
editors@tomshardware.com (Jowi Morales)
Jowi Morales
Shutterstock
China
China
<![CDATA[ These are the top free Windows tools that I use on a daily basis to boost my productivity ]]>
<p>You don’t need to spend big money for the best applications, and here is proof! I’ve collated a list of the best free applications for creating content, writing code, making bootable USB drives, and, most essential of all, entertainment.</p><p>These are all of the apps that I commonly use when making content on <em>Tom’s Hardware,</em> and sometimes my overly neglected blog. All of these applications are for Windows-based machines, but many are also available for Linux and macOS devices.</p><p>So let's dive in and learn a little more about the apps that I use.</p><h3 class="article-body__section" id="section-creative-tools"><span>Creative Tools</span></h3><h2 id="screentogif-2">ScreenToGIF</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1762px;"><p class="vanilla-image-block" style="padding-top:66.40%;"><img id="nJvYp4UeJUjfbejaaLpRfS" name="screentogif.JPG" alt="Windows Tools" src="https://cdn.mos.cms.futurecdn.net/nJvYp4UeJUjfbejaaLpRfS.jpg" mos="" align="middle" fullscreen="" width="1762" height="1170" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>If you’ve seen a GIF in any of my content, <a data-analytics-id="inline-link" href="https://www.screentogif.com/"><u>ScreenToGIF</u></a> made it. This free tool can capture entire screens or portions of them and render them as an animated GIF or a video file. Not only does ScreenToGIF capture media for my GIFs, but I can also edit them from the expansive menu. Scaling, trimming, optimizing, and features such as watermarks, text, and special effects are just a click away.</p><p>I use ScreenToGIF a lot! Mainly for GIFs, but it works extremely well as a quick way to capture videos for IT support tickets when I need to prove that I found an issue.</p><p>ScreenToGIF can also be used to convert a video to GIF. Just make sure that it isn’t an MOV file; for that, you will need to convert the file using Handbrake, which is also on this list! In ScreenToGIF, I can remove frames, set the FPS, add effects, scan for duplicates, resize the images, and more.</p><p>Output options are as a video file and, of course, GIF. With GIF, there are multiple encoding options and encoders to get the best desired output for your creations.</p><p><a data-analytics-id="inline-link" href="https://www.screentogif.com/"><u>ScreenToGIF</u></a> is awesome and I’ve been using it for over four years now and it has proven to be a solid performer for my workflow.</p><h2 id="gimp-2">GIMP</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:970px;"><p class="vanilla-image-block" style="padding-top:54.74%;"><img id="4Li4w4g66Ng3SCc7Cg7wbS" name="gimp" alt="Windows Tools" src="https://cdn.mos.cms.futurecdn.net/4Li4w4g66Ng3SCc7Cg7wbS.jpg" mos="" align="middle" fullscreen="" width="970" height="531" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>I’ve used <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/video-editing-graphic-design/five-best-photoshop-alternatives-tested-image-editing-for-free" target="_blank"><u>many different bitmap image editors;</u></a> heck, I trained on Adobe Photoshop back in the 1990s, but something always brings me back to <a data-analytics-id="inline-link" href="https://www.gimp.org/" target="_blank"><u>GIMP</u></a>. It's not the name, clearly. But the GNU Image Manipulation Programme has plenty of features for this writer.</p><p>First of all, this isn’t Photoshop. You can make it appear to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/make-gimp-look-and-feel-like-photoshop"><u>work like Photoshop</u></a>, but I’d recommend learning to use the default interface, unless you are transitioning from Photoshop. GIMP can do it all — you just have to learn how it works, but it doesn’t take too long. GIMP feels a little slower than Photoshop; that could be a mix of performance and user expectation.</p><p>GIMP supports Photoshop file formats, along with all the usual suspects, including RAW images. We can work with multiple layers, effects, and filters to tweak our compositions. You can write your own plugins using Python or C/C++ and call GIMP from the command line/terminal to batch edit images without the GUI.</p><p><a data-analytics-id="inline-link" href="https://www.gimp.org/"><u>GIMP</u></a> is free and well worth investing your time and effort into. If not, there are <a data-analytics-id="inline-link" href="https://www.tomshardware.com/software/video-editing-graphic-design/five-best-photoshop-alternatives-tested-image-editing-for-free"><u>plenty of other image editors</u></a> that you can try out.</p><h2 id="inkscape-2">Inkscape</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1326px;"><p class="vanilla-image-block" style="padding-top:103.32%;"><img id="BFvmF9ypbdfEbgVUoNNndS" name="inkscape.JPG" alt="Windows Tools" src="https://cdn.mos.cms.futurecdn.net/BFvmF9ypbdfEbgVUoNNndS.jpg" mos="" align="middle" fullscreen="" width="1326" height="1370" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Adobe Illustrator is where I learned my craft back in the 1990s. Along with Adobe Photoshop and Aldus Pagemaker, it's no wonder that I seemed on track for a career in journalism. But the problem with Adobe software is the cost. If you are a professional, then the cost is a business expense, but if you are only an occasional user, then that cost is hard to swallow.</p><p><a data-analytics-id="inline-link" href="https://inkscape.org/"><u>Inkscape</u></a> is a viable and, dare I say, excellent alternative to Illustrator. It works with Illustrator files and SVG (Scalable Vector Graphics) files that are commonly used on the web. Inkscape is a joy to use, and I can annotate images, create diagrams ,and even prepare SVG files that I ultimately <a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/design-and-3d-print-custom-cases"><u>use to make 3D printable objects.</u></a></p><p><a data-analytics-id="inline-link" href="https://inkscape.org/"><u>Inkscape</u></a> is a wonderful tool, and you should add it to your setup right away.</p><h3 class="article-body__section" id="section-useful-tools"><span>Useful Tools</span></h3><h2 id="notepad-2">Notepad++</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2144px;"><p class="vanilla-image-block" style="padding-top:52.19%;"><img id="2LJ5L7c8mbshqrzsotwbeS" name="notepad++.JPG" alt="Windows Tools" src="https://cdn.mos.cms.futurecdn.net/2LJ5L7c8mbshqrzsotwbeS.jpg" mos="" align="middle" fullscreen="" width="2144" height="1119" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>For this entry, I have Avram Piltch to thank. <a data-analytics-id="inline-link" href="https://notepad-plus-plus.org/"><u>Notepad++</u></a> was something that I was aware of, but never really used. You see, I used to swear by Microsoft’s Visual Studio Code for all of my coding projects, but it is a little overkill for some.<br>Notepad++, as its name suggests, is as light as the built-in Notepad, but it has an excellent awareness of different programming languages, and it makes short work of opening the majority of text files, including my personal favorite, Markdown.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:628px;"><p class="vanilla-image-block" style="padding-top:49.04%;"><img id="hiKhPokdEMMtZTPk5iZvaS" name="notepad-syntax" alt="Windows Tools" src="https://cdn.mos.cms.futurecdn.net/hiKhPokdEMMtZTPk5iZvaS.jpg" mos="" align="middle" fullscreen="" width="628" height="308" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Using a plugin for Markdown, I can preview what I am writing, before it gets published to my blog. When coding in other languages, Notepad++ provides syntax highlighting, a handy feature for most coders.</p><p><a data-analytics-id="inline-link" href="https://notepad-plus-plus.org/"><u>Notepad++ </u></a>is more than a note taking tool, it is a development tool that no coder should do without.</p><h2 id="rufus-2">Rufus</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:474px;"><p class="vanilla-image-block" style="padding-top:151.69%;"><img id="Tsdw4kePT39GZ7XZaSJxaS" name="rufus.JPG" alt="Windows Tools" src="https://cdn.mos.cms.futurecdn.net/Tsdw4kePT39GZ7XZaSJxaS.jpg" mos="" align="middle" fullscreen="" width="474" height="719" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Over the years, I have made a lot of bootable USB sticks for single-board computers such as the Raspberry Pi, LattePanda, and, of course, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/why-i-still-use-a-lenovo-thinkpad-that-debuted-in-2011-the-x220-helps-me-stay-focused-on-my-most-important-tasks" target="_blank"><u>my elderly Lenovo X220</u></a>. To make these bootable sticks, I’ve used Raspberry Pi Imager, Etcher, and even the dd Linux command. But Rufus is the one app that I keep coming back to.</p><p><a data-analytics-id="inline-link" href="https://rufus.ie/en/"><u>Rufus</u></a> is a free download, and it can write an ISO / IMG file to a USB stick for later use on a machine. It can also be used to <a data-analytics-id="inline-link" href="https://www.tomshardware.com/how-to/clean-install-windows-11"><u>customize a Windows 11 install disk</u></a> to bypass TPM requirements.</p><p>Simple to use, Rufus hides a lot of features in plain sight, activating specific features according to your needs. Take, for example, the extended Windows 11 tweaks for TPM, RAM requirements, and so on.</p><p><a data-analytics-id="inline-link" href="https://rufus.ie/en/"><u>Download Rufus</u></a> and throw it on a spare USB flash drive, for those times when you really need to make a bootable USB disk, without the hassle of dd. Alternatively, take a look at <a data-analytics-id="inline-link" href="https://www.tomshardware.com/desktops/pc-building/i-created-a-pc-repair-kit-that-i-can-take-on-the-road-here-are-the-tools-that-come-in-handy-when-youre-in-a-pinch"><u>Ventoy</u></a>, which can store multiple bootable operating systems on a single USB drive.</p><h3 class="article-body__section" id="section-entertainment-and-media"><span>Entertainment and Media</span></h3><h2 id="vlc-2">VLC</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1913px;"><p class="vanilla-image-block" style="padding-top:61.63%;"><img id="FD9uKJDJ5qNhmWWbiiStgS" name="vlc" alt="Windows Tools" src="https://cdn.mos.cms.futurecdn.net/FD9uKJDJ5qNhmWWbiiStgS.jpg" mos="" align="middle" fullscreen="" width="1913" height="1179" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><a data-analytics-id="inline-link" href="https://www.videolan.org/"><u>VLC</u></a> is the backbone of my productivity. After ripping my CDs to FLAC, I throw them onto my NAS, and from there, my Windows desktop, Linux laptop, and Android smartphone can access the files via VLC.</p><p>Video playback via VLC is always a joy, and the interface is simple, but to the point where I can use it in my sleep. I even use VLC's screengrab tool to screenshot images from a video capture card that I use to record installing different operating systems on single-board computers.</p><p>Got a malformed media file? VLC doesn’t care; it will play it. YouTube link or BBC iPlayer stream? VLC will play it. An obscure web format from 20 years ago? Sure. VLC just gets on with it. Still got your physical media? VLC will play your CDs and DVDs</p><p>Without VLC, I’d probably get less work done; without my music, there is no work! So <a data-analytics-id="inline-link" href="https://www.videolan.org/"><u>download it for yourself</u></a> right now.</p><h2 id="handbrake-2">Handbrake</h2><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1081px;"><p class="vanilla-image-block" style="padding-top:74.01%;"><img id="bPNsLpb9UgZGxog4zWtWdS" name="handbrake" alt="Windows Tools" src="https://cdn.mos.cms.futurecdn.net/bPNsLpb9UgZGxog4zWtWdS.jpg" mos="" align="middle" fullscreen="" width="1081" height="800" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p><a data-analytics-id="inline-link" href="https://handbrake.fr/"><u>Handbrake </u></a>has saved my bacon on more than a few occasions. This is the tool that you need to transcode audio/video media when you want it to play on an older device, a device that has a specialized hardware decoder, or you want to publish to YouTube.</p><p>Handbrake has a frightening amount of options, but stick with the presets and you’ll be good.</p><p>Handbrake can be used on single files, or multiple files can be added to a queue, each with its own specific configuration. Then we just run Handbrake over the queued files and let it do its thing.</p><p>Advanced users can integrate Handbrake’s CLI interface to automate batch processes, handy for when we are backing up our DVD collection to a NAS. Users have created their own batch scripts to run Handbrake and download cover art and subtitles for their content, automating the process of archiving their physical media to a NAS.</p><p>Handbrake is one of those apps that we don’t always need, but when we need to convert video files, everybody uses Handbrake, so <a data-analytics-id="inline-link" href="https://handbrake.fr/"><u>grab a free copy now</u></a>.</p>
https://www.tomshardware.com/software/windows/these-are-the-top-six-free-windows-tools-that-i-use-on-a-daily-basis
You don’t need to spend any money nor commit piracy to get great software, and here is the proof!
eUL33SezjxRQ42M9zZoRUd
Wed, 17 Sep 2025 11:16:58 +0000 Windows
Software
Operating Systems
Les Pounder
Pexels OpenClipArt
Windows Tools
Windows Tools
<![CDATA[ Apple prepping touchscreen OLED MacBook Pro for 2026 — new report claims model will incorporate on-cell touch tech for the first time ]]>
<p>A new report from top Apple insider Ming-Chi Kuo <a data-analytics-id="inline-link" href="https://x.com/mingchikuo/status/1968249865940709538" target="_blank">claims</a> that the company will finally introduce touchscreen technology to its Mac lineup, beginning with an OLED MacBook Pro set to enter mass production in late 2026.</p><p>Kuo made the revelation on X early on Wednesday morning. "MacBook models will feature a touch panel for the first time, further blurring the line with the iPad,"
he noted. "This shift appears to reflect Apple’s long-term observation of iPad user behavior, indicating that in certain scenarios, touch controls can enhance both productivity and the overall user experience."</p><p>According to Kuo, a new OLED MacBook Pro is expected to enter mass production "by late 2026." The information — which, of course, is unofficial — could indicate a launch in Q4 2026 or possibly early 2027. The former seems likely, given that Apple generally launches new Mac models in the fall and has been previously tipped to be exploring an annual release cycle for Macs more in line with its iPhone launch schedule.</p><p>Kuo further notes that Apple is planning a more affordable MacBook powered by an A-series iPhone processor. He says this is slated for mass production in 4Q25, which could indicate a launch very soon. He notes a second-generation version of this MacBook could come in 2027 and that Apple is considering touch screen support for that model.</p><p>Apple's current MacBook Pro family was unveiled in October 2024 and sports the M4 family of Apple silicon chips. While its Liquid Retina XDR display offers 120Hz refresh rates, OLED technology remains a key tech that is set to grace the Apple MacBook lineup. Further previous leaks from the supply chain have hinted at MacBook Pro models with OLED displays coming in 2026, lining up nicely with Kuo's latest prediction.</p><p>Apple has famously eschewed touchscreen tech on its laptops, an aversion that dates back to Steve Jobs' tenure as CEO. Jobs famously blasted the concept, telling an audience in 2010 that "touch surfaces don't want to be vertical," citing user fatigue and terrible ergonomics.</p><p>If Kuo is right about the touch screen MacBook in 2026 (his track record is extremely solid), then that would signal a true change in Mac design philosophy from Apple, which many users have been clamoring for over generations of MacBook launches.</p><p>Apple's M5 MacBook series is next in the lineup. Previously, industry sources had tipped them for a release at the end of this year; however, latterly Kuo has pointed to a <a data-analytics-id="inline-link" href="https://mingchikuo.craft.me/UeiC07s0qRrd1L" target="_blank">2026 launch</a>. It's possible Kuo's OLED touch screen predictions relate to this lineup, but it seems more likely the change will be reserved for the next generation M6 processors.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/laptops/macbooks/apple-prepping-touchscreen-oled-macbook-pro-for-2026-new-report-claims-new-model-will-incorporate-on-cell-touch-tech-for-the-first-time
A new inside report claims Apple will release a touch screen MacBook in late 2026 or early 2027.
mqTnkMyrir3KkhBvQuV6SS
Wed, 17 Sep 2025 11:13:07 +0000 Macbooks
Laptops
stephen.warwick@futurenet.com (Stephen Warwick)
Stephen Warwick
Apple
Apple MacBook Pro
Apple MacBook Pro
<![CDATA[ How TSMC managed to increase efficiency of ASML's EUV tools: System-level optimizations and in-house pellicles —chipmaker boosted EUV-driven wafer production by 30x over six years while reducing power consumption by 24% ]]>
<p>TSMC is by far the largest operator of ASML's EUV lithography tools in the industry, with a second-to-none supply chain of both hardware and raw materials. The company's requirements for EUV pellicles (a thin membrane that protects the photomask, which acts as a stencil for chip patterns) have gotten incredibly high. The requirements are so extreme that TSMC intends to retrofit one of its 200-mm fabs to produce proprietary EUV pellicles exclusively, according to <a data-analytics-id="inline-link" href="https://www.digitimes.com.tw/tech/dt/n/shwnws.asp?CnlID=1&Cat=40&id=0000732130_PCY2S1112LWX501INK4C1">DigiTimes</a>.</p><p>However, TSMC's proprietary pellicles, which outperform ASML's own offering, are just the tip of the iceberg, as TSMC pushes efforts to improve the efficiency of its fabs and ASML's EUV lithography tools.</p><p>When TSMC first deployed EUV in 2019 on its N7+ process for Huawei's HiSilicon Kirin 9000-series processors for smartphones, it already controlled 42% of the global install base of EUV lithography machines,
despite TSMC not being the first foundry to formally announce the use of EUV tools.</p><p>By 2020, as ASML accelerated shipments and introduced its N5 process technology, which uses EUV for several layers, TSMC captured 50% market share of all EUV machines; this increased to 56% in mid-2024, with 130 machines to its name. Now, TSMC likely controls around 200 EUV machines globally, across many of its fabs.</p><p>While the number of EUV systems increased at TSMC by over 10 times compared to 2019, the number of wafers the company processes increased by over 30 times over six years, which indicates that TSMC did an incredible job to reduce downtime and service time, while increasing the throughput of its EUV scanners, which ultimately leads to higher productivity.</p><h2 id="tool-productivity-2">Tool productivity</h2><p>TSMC said at its Technology Symposium in mid-2024 that it had doubled the wafer output per EUV tool per day since 2019 by fine-tuning both the <a data-analytics-id="inline-link" href="https://semiwiki.com/lithography/304326-the-challenge-of-working-with-euv-doses/">exposure dose</a> and the photoresist materials used in its lithography process. But, to define how TSMC has managed such efficiency, we speculate how the company may have achieved this.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="MqQcLuXtcS9FPhQiZeDavC" name="NXE3400_Simplify_seq15_5k.jpg" alt="ASML" src="https://cdn.mos.cms.futurecdn.net/MqQcLuXtcS9FPhQiZeDavC.jpg" mos="" align="middle" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: ASML)</span></figcaption></figure><p>On the exposure side, TSMC refined dose-to-size and dose-to-clear thresholds to reduce scanner dwell time per exposure field, while preserving critical dimension (CD) uniformity and ensuring line-edge roughness (LER) according to spec, which enables faster patterning without compromising yield in EUV lithography.</p><p>Typically, lowering these doses reduces the scanner's dwell time per field, enabling more wafers to be processed each day. However, this must be carefully balanced to maintain pattern fidelity, including CD control and LER. TSMC has succeeded, at least if we decoded its description correctly. Since TSMC has not disclosed the exact details, this is speculation, based on publicly available information.</p><p>On the materials side, TSMC upgraded its photoresist systems, possibly adopting high-sensitivity chemically amplified resists (CARs) and potentially incorporating metal-oxide resists (MORs) to improve absorption at 13.5 nm wavelength. These materials can enable lower exposure doses without degrading resolution. However, the company has yet to disclose which materials were used and exactly how they may have been implemented.</p><p>In parallel, TSMC also improved scanner utilization efficiency by deploying <a data-analytics-id="inline-link" href="https://primavera-project.com/wp-content/uploads/2023/02/3.-ASML-Prima-Vera-1Feb2023.pdf">predictive maintenance models</a>, optimizing job scheduling, <a data-analytics-id="inline-link" href="https://pure.tue.nl/ws/files/46934826/846650-1.pdf">servicing tools in advance</a>, <a data-analytics-id="inline-link" href="https://patents.google.com/patent/US10386716B2/en">enhancing vibration control</a>, and improving <a data-analytics-id="inline-link" href="https://www.mdpi.com/2072-666X/16/8/880">cooling performance</a>. These changes reduced unplanned downtime and increased daily tool availability, enabling each EUV system to process more wafers in a stable environment.</p><h2 id="particular-pellicles-2">Particular Pellicles</h2><p>One of the most important advances has been in pellicle technology. These thin membranes protect EUV photomasks (reticles) from contamination, but have long been a bottleneck due to durability and defect issues. ASML itself developed two generations of its reticles, but it appears as though TSMC has managed to outpace them.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1280px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="Vu6N9RDjut8Yy6FiGKNSCB" name="Photomask 1.png" alt="Mask" src="https://cdn.mos.cms.futurecdn.net/Vu6N9RDjut8Yy6FiGKNSCB.png" mos="" align="middle" fullscreen="" width="1280" height="720" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Intel)</span></figcaption></figure><p>Back in mid-2024, TSMC reported dramatic improvements of its in-house developed EUV pellicle performance, including a four times longer lifespan, four and a half times more wafer output per pellicle, and an 80x reduction in defects. This likely points to TSMC's full-stack engineering approach that spans materials, mechanics, and fab integration.</p><p>On the materials side, TSMC may have adopted advanced pellicle films such as ultra-thin silicon-based membranes (e.g., <a data-analytics-id="inline-link" href="https://research.ibm.com/publications/fabrication-of-a-full-size-euv-pellicle-based-on-silicon-nitride">SiNx</a>, <a data-analytics-id="inline-link" href="https://pdfs.semanticscholar.org/753d/05ecc56f3f5bd63e942e45418078bb295150.pdf">ZrSi<sub>2</sub></a>) or hybrid multilayers with optimized EUV transmittance, thermal stability, and mechanical strength. These materials withstand EUV radiation, minimize thermal deformation, and reduce outgassing. To further suppress particle adhesion and resist contamination-induced failures, TSMC likely treated the surface with anti-reflective coatings or plasma passivation; however, this is speculation and not officially confirmed.</p><p>There are additional ways to improve the lifespan of pellicles and reduce defects. For example, tighter cleanroom protocols would reduce the chance of particle transfer onto pellicles or reticles. However, TSMC has yet to disclose these methods.</p><h2 id="photomasks-2">Photomasks</h2><p>In addition to pellicles that protect photomasks, TSMC is also refining the photomasks themselves. To meet the A14 node's lithography demands, TSMC improved mask accuracy and defect control to reduce defect density, boost yield, and ultimately increase throughput.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="iHJSp7cXP3UeHRdZqukkZQ" name="intel-ims-photomask-wafer-semiconductor-hero.jpg" alt="Intel" src="https://cdn.mos.cms.futurecdn.net/iHJSp7cXP3UeHRdZqukkZQ.jpg" mos="" align="middle" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Intel)</span></figcaption></figure><p>TSMC claims that its engineers had improved CD uniformity, pattern fidelity, and overlay precision for curvilinear features by modifying EUV mask blanks, increasing the resolution of multi-beam writers, and optimizing mask fabrication processes. These steps ensured a more consistent pattern transfer and better alignment across layers.</p><p>Defect control was a major focus, too. TSMC strengthened its pellicle inspection, reticle cleanliness, and developer rinse chemistry to suppress defects like bridging and pattern collapse. TSMC deployed advanced e-beam inspection techniques to detect sub-visible membrane defects and degradation early, enabling predictive maintenance and proactive replacement before catastrophic failures, which improves yields and lowers performance variability.</p><p>In the future, TSMC plans to develop next-generation blank materials and new process flows to support future EUV requirements.</p><h2 id="planarization-2">Planarization</h2><p>Improving photomasks and pellicles are not the only ways to lower defect density, increase yields, and reduce performance variability, particularly for 2nm and sub-2nm-class process technologies, according to TSMC. The company is working to <a data-analytics-id="inline-link" href="https://investor.tsmc.com/sites/ir/annual-report/2024/2024%20Annual%20Report.E.pdf">improve polarization for its A16 and A14 fabrication processes</a>.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="YAQU3zRSbuCHbUYm5L4oD4" name="tsmc-semiconductor-fab-wafer-hero.jpg" alt="TSMC" src="https://cdn.mos.cms.futurecdn.net/YAQU3zRSbuCHbUYm5L4oD4.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: TSMC)</span></figcaption></figure><p>Planarization is crucial in advanced EUV lithography because it ensures a uniformly flat wafer surface, which is critical for maintaining focus and pattern fidelity at sub-2nm nodes like TSMC's <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/tsmcs-1-6nm-node-to-be-production-ready-in-late-2026-roadmap-remains-on-track">A16</a> and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/tsmc-unveils-1-4nm-technology-2nd-gen-gaa-transistors-full-node-advantages-coming-in-2028">A14</a>. EUV systems have an extremely shallow depth of focus, which is why any topographical variation could be a source of defocus, CD variation, or LER. Uneven surfaces also lead to non-uniform resist thickness, which affects dose absorption and etch uniformity. Also, overlay errors increase if subsequent layers are patterned on non-planar foundations.</p><p>TSMC likely improves this through advanced chemical mechanical planarization (CMP), optimizing slurry chemistry, pressure profiles, and endpoint detection to achieve tight within-wafer and wafer-to-wafer planarity control.</p><h2 id="energy-efficiency-2">Energy efficiency</h2><p>EUV scanners are known for heavy energy use, and here too TSMC has made progress. It says it reduced power consumption of EUV tools by 24% 'through innovative energy saving techniques.' The company's future target is a 1.5 times improvement in energy efficiency per wafer by 2030.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:64.06%;"><img id="sF5kjc768gySpL2e9YSSqA" name="Engineer-checking-assembly-instructions_48554.jpg" alt="ASML" src="https://cdn.mos.cms.futurecdn.net/sF5kjc768gySpL2e9YSSqA.jpg" mos="" align="middle" fullscreen="" width="2560" height="1640" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: ASML)</span></figcaption></figure><p>Reduction in EUV scanner power consumption was likely achieved through a combination of hardware-level and system-level optimizations, though which optimizations were made has not been disclosed. For example, TSMC could enhance the laser-to-EUV conversion efficiency, where a significant amount of energy is lost. Another key area is thermal management. TSMC likely refined liquid cooling systems, optimized coolant flow rates, and improved heat exchanger design to lower auxiliary power consumption while maintaining thermal stability.</p><p>On the system side of things, firmware and scheduler optimizations may have reduced idle-state energy use and improved synchronization between subsystems, reducing power draw during non-exposure operations. Predictive maintenance and better utilization analytics would help avoid performance degradation from inaccurate positioning, synchronization, or overheating, which prevents inefficient tool operation.</p><h2 id="tsmc-s-position-strengthens-2">TSMC's position strengthens</h2><p>Although TSMC was not the first chipmaker to adopt EUV lithography, it is currently the largest operator of EUV tools in the industry. Since 2019, the company has doubled wafer throughput per tool, reduced scanner power consumption by 24%, and achieved major gains in pellicle performance, including four times the lifespan and 80 times lower defects. To support these advancements, TSMC plans to retrofit a 200 mm fab to manufacture proprietary EUV pellicles that may surpass ASML's own. These efforts reflect TSMC’s broader strategy to control the full EUV stack—from materials to tool optimization.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/tech-industry/semiconductors/how-tsmc-managed-to-increase-efficiency-of-asmls-euv-tools-system-level-optimizations-and-in-house-pellicles-chipmaker-boosted-euv-driven-wafer-production-by-30x-over-six-years-while-reducing-power-consumption-by-24-percent
TSMC has dramatically boosted EUV scanner throughput, pellicle performance, and energy efficiency through deep in-house innovations.
Ei96WAsmMnfG9jHWyhL2vX
Wed, 17 Sep 2025 11:10:06 +0000 Semiconductors
Tech Industry
Manufacturing
ashilov@gmail.com (Anton Shilov)
Anton Shilov
Taiwan Semiconductor Manufacturing Co., Ltd.
TSMC Lobby
TSMC Lobby
<![CDATA[ Latest FSR 4 source code 'leak' lets you run AMD's AI upscaling tech on nearly any GPU — no Linux required ]]>
<p>The latest version of AMD's FidelityFX, typically known as FSR 4, delivers a markedly superior result to FSR 3, making it a big win for those who can run it. But that privileged group is limited to folks with AMD Radeon RX 9000 series GPUs based on the company's RDNA 4 architecture. Or is it? As it turns out, <a data-analytics-id="inline-link" href="https://www.reddit.com/r/radeon/comments/1nhxgd0/fsr_4_working_in_cyberpunk_2077_on_rdna3_7900xtx/" target="_blank">you can actually run FSR 4</a> on nearly any GPU, thanks to AMD itself <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amd-accidentally-marks-fsr-4-open-source-source-code-reveals-potential-support-for-older-radeon-gpus">leaking the source code</a> last month.</p><p>Strictly speaking, this isn't exactly 'new' news. As far back as June of this year, people were <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/enthusiast-hacks-fsr-4-onto-rx-7000-series-gpu-without-official-amd-support-returns-better-quality-but-slightly-lower-fps-than-fsr-3-1" target="_blank">hacking FSR4 onto last-generation</a> Radeon RX 7000 GPUs, but that trick was fragile and required Linux. Today's method is quite easy and should, in theory, work on virtually any modern GPU in the vast majority of DirectX 12, DirectX 11, and Vulkan games. We'll get to the specifics in a moment, but we should explain exactly what's going on here.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:2560px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="xZUHUWsCEP2CtHjhdSLuA3" name="fsr4-artifacts" alt="A screenshot of Lizzie's Bar in Cyberpunk 2077." src="https://cdn.mos.cms.futurecdn.net/xZUHUWsCEP2CtHjhdSLuA3.jpg" mos="" align="middle" fullscreen="" width="2560" height="1440" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">Several motion artifacts are visible in this FSR4 screenshot. </span><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware / CDPR)</span></figcaption></figure><p>When AMD open-sourced the FSR SDK, including FSR 4, it <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amds-already-taken-down-mistakenly-released-fsr-4-source-code-but-the-internet-never-forgets-forked-github-repositories-remain-accessible" target="_blank">mistakenly published the full source</a> of FSR 4, not just the SDK portion of it. That meant that anyone could take the FSR 4 code and do whatever they wanted with it, because the source was published under a highly permissive MIT license. Notably, alongside the FP8 version of FSR 4 — that is, the standard version that <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amd-radeon-rx-9070-xt-review" target="_blank">the Radeon RX 9000 cards</a> normally use — there was also a version built to use the INT8 datatype. INT8 is supported on virtually all modern GPUs, so it is much more compatible.</p><p>That was a source release, so it took some hero to come along and compile the source into a binary form that gamers could actually use. That hero turns out to be /u/AthleteDependent926 on Reddit, who provided the compiled DLL file that users can simply drop into games with FSR 3 support to enable FSR 4.</p><p>It takes a bit of doing; in our testing, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/you-can-upgrade-fsr-3-1-games-to-fsr-4-with-manual-dll-swapping-github-community-discovers-fsr-swapping-works-similar-to-dlss-upgrades" target="_blank">simply swapping the files</a> won't enable FSR 4 the way you might do with DLSS. However, using the OptiScaler mod, you can specifically select FSR 4.0.2 in the mod's UI.</p><p><a data-analytics-id="inline-link" href="https://github.com/optiscaler/OptiScaler" target="_blank">OptiScaler</a> is a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amds-fsr-4-gets-a-big-boost-in-compatibility-as-optiscaler-now-supports-upconverting-any-modern-upscaler-to-fsr-4-with-frame-gen-as-long-as-the-game-isnt-vulkan-based-or-has-anti-cheat" target="_blank">multi-game mod</a> similar to something like ReShade or Special K. Install OptiScaler to the game's executable directory, run the "setup_windows.bat" or "setup_linux.sh" depending on your operating system, and then replace the "amd_fidelityfx_upscaler_dx12.dll" with the one from <a data-analytics-id="inline-link" href="https://www.reddit.com/r/radeon/comments/1nhkkr8/fsr_sdk_leak_contained_fsr_4_files_that_work_on/" target="_blank">/u/AthleteDependent926's Reddit post</a>. After doing so, launch your game, press Insert to open the OptiScaler UI, select "FSR 3.X" as your upscaler, and then in the "FFX Settings," select FSR 4.0.2. It's a little unintuitive, but it absolutely works.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:3840px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="Xbsun7xvfQ6v5P5t4yxLKE" name="cyberpunk-2077-fsr4-hack-fsr3-compare" alt="A side-by-side comparison of Cyberpunk 2077 rendered using AMD's FSR3 and FSR4 upscalers." src="https://cdn.mos.cms.futurecdn.net/Xbsun7xvfQ6v5P5t4yxLKE.png" mos="" align="middle" fullscreen="" width="3840" height="2160" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">It's hard to appreciate in a screenshot, but AMD's FSR4 is sharper and less artifact-prone. </span><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware / CDPR)</span></figcaption></figure><p>How do I know? Well, I tested it. First, on a Radeon RX 7800 XT connected to a 4K display, and then on <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-boasts-its-ryzen-ai-max-395-is-up-to-12-2x-faster-than-lunar-lake-in-ai-workloads" target="_blank">a Ryzen AI Max+ 395</a>'s integrated Radeon 8060S connected to a 1440p display. In both cases, performance is a little rough; we saw about 4.1 ms to upscale to 4K on the RX 7800 XT, while the Radeon 8060S takes about 2.3 ms to upscale to 1440p. For those unfamiliar with frame times, 60 FPS equates to a frame time of 16.7 ms. Tacking on an extra 4.1ms for the upscale drops you from 60 to about 48 FPS, but we didn't see that kind of performance from either GPU because we were testing <em>Cyberpunk 2077</em> in RT Ultra mode on Radeon hardware.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:869px;"><p class="vanilla-image-block" style="padding-top:44.42%;"><img id="RhAtg5vhvjTwv2vkiBh3TM" name="fsr4-fsr3-xess-upscalers-performance" alt="Three OptiScaler overlay screenshots showing the performance difference in various upscalers on a Radeon RX 7800 XT." src="https://cdn.mos.cms.futurecdn.net/RhAtg5vhvjTwv2vkiBh3TM.png" mos="" align="middle" fullscreen="" width="869" height="386" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">FSR4 is significantly slower than FSR3, but also offers much better image quality. </span><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Still, performance remained broadly playable on both GPUs, and the final image quality with FSR 4, while decidedly inferior to DLSS 4, is nonetheless an undeniable step up from FSR 3, and in fact also superior to Intel's XeSS—at least, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/intel-xess-technology-demo-and-overview" target="_blank">the DP4a path available to non-Intel GPUs</a>. In <em>Cyberpunk 2077</em>, FSR 4 clearly has fewer artifacts and less aliasing, although it's not flawless; we still saw some trailing on distant objects, and animated textures still throw it for a loop. Only NVIDIA's transformer-based DLSS 4 has resolved those issues.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="DZadCkjLoTnybBf95uq8XV" name="cyberpunk2077-graphics-settings-menu-fsr-30" alt="A screenshot of the graphics options menu in Cyberpunk 2077, showing FSR 3.0 engaged." src="https://cdn.mos.cms.futurecdn.net/DZadCkjLoTnybBf95uq8XV.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="caption-text">To use FSR 4 with OptiScaler, you'll enable FSR 3 in the game options. </span><span class="credit" itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><p>Of course, some of our problems could be down to the fact that this is <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/amd-accidentally-marks-fsr-4-open-source-source-code-reveals-potential-support-for-older-radeon-gpus" target="_blank">a rather hacky way</a> of implementing a fully unsupported upscaler. But regardless, this does seem like a great option to have in the toolbox of Radeon and Arc gamers who don't have access to the latest DLSS models. A great many games have implemented FSR 3 upscaling, and the ability to simply replace that with FSR 4 could be an excellent option if you're already flush with a fine frame rate. Kudos to the enthusiasts and modders who made this trick possible.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/gpus/latest-fsr-4-source-code-leak-lets-you-run-amds-ai-upscaling-tech-on-nearly-any-gpu-no-linux-required
As it turns out, with some tweaking, you can actually run FSR 4 on nearly any recent GPU thanks to AMD itself leaking the source code last month.
EtBzqtHyEFibTfT8siuYke
Tue, 16 Sep 2025 17:47:11 +0000 GPUs
PC Components
Zak Killian
AMD
A marketing image for AMD&#039;s FidelityFX Super Resolution
A marketing image for AMD&#039;s FidelityFX Super Resolution
<![CDATA[ Looking Glass demos Hololuminescent Display monitors — sizes range from 16 to 85 inches, starting at $1,500 ]]>
<p>Looking Glass has <a data-analytics-id="inline-link" href="https://lookingglassfactory.com/hld-overview" target="_blank">taken the wraps off</a> its new monitors with Hololuminescent Display (HLD) technology. The firm has been in the holographic displays market for a decade, but it believes its new HLD monitors, which are just 1-inch thick and deliver up to a 4K resolution, can deliver “magical holograms that can be deployed anywhere.” HLD will allow the firm’s immersive light field to be rolled out anywhere standard video screens are used today.</p><div class="youtube-video" data-nosnippet ><div class="video-aspect-box"><iframe data-lazy-priority="low" data-lazy-src="https://www.youtube.com/embed/EtG9O4YGjfw" allowfullscreen></iframe></div></div><p>According to Looking Glass, HLDs “create an immersive three-dimensional stage for all types of content -— without the complexity of traditional 3D pipelines.” They do not need <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/dying-light-eye-tracking-tobii,33473.html">eye tracking technology</a> or special viewing lenses, so they are suitable for viewing by groups. This is a significant quality that could potentially help adoption in traditional digital signage devices.</p><p>For content creators, HLDs are said to work with standard 2D video workflows mixing real-world footage, animation, interactive applications, and AI generation. A demonstration workflow with a green screen, character, and Adobe Premiere Pro is outlined on the HLD <a data-analytics-id="inline-link" href="https://lookingglassfactory.com/how-it-works">How It Works</a> page.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:600px;"><p class="vanilla-image-block" style="padding-top:56.33%;"><img id="BDHhViLYJHQbDgj9hup7Z9" name="HLD_Headphones_1" alt="Looking Glass demos new Hololuminescent Display technology" src="https://cdn.mos.cms.futurecdn.net/BDHhViLYJHQbDgj9hup7Z9.gif" mos="" align="middle" fullscreen="" width="600" height="338" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: Looking Glass )</span></figcaption></figure><p>Here's how Looking Glass’ prior Light Field Displays (LFD) compare to these new HLD models.</p><div ><table><thead><tr><th class="firstcol " ><p><strong>Feature</strong></p></th><th
><p><strong>Hololuminescent Displays (HLD)</strong></p></th><th
><p><strong>Light Field Displays (LFD)</strong></p></th></tr></thead><tbody><tr><td class="firstcol " ><p>Content Creation</p></td><td
><p>Standard 2D content pipelines</p></td><td
><p>Specialized 3D software</p></td></tr><tr><td class="firstcol " ><p>Setup Complexity</p></td><td
><p>Easy to intermediate, Plug and Play</p></td><td
><p>Intermediate to advanced 3D software</p></td></tr><tr><td class="firstcol " ><p>Ideal Content</p></td><td
><p>People, products, characters</p></td><td
><p>CAD, medical scans, terrain maps, data visualization</p></td></tr><tr><td class="firstcol " ><p>3D Effect</p></td><td
><p>Fixed holographic stage for holographic depth cues</p></td><td
><p>Multi-view parallax</p></td></tr><tr><td class="firstcol " ><p>Eye Tracking</p></td><td
><p>None</p></td><td
><p>None</p></td></tr><tr><td class="firstcol " ><p>Best For</p></td><td
><p>Digital signage, retail, experiential displays</p></td><td
><p>R&D, medical, engineering, 3D art</p></td></tr><tr><td class="firstcol " ><p>Sizes Available</p></td><td
><p>16”, 27”, 86”</p></td><td
><p>6”, 16”, 27”</p></td></tr></tbody></table></div><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1578px;"><p class="vanilla-image-block" style="padding-top:69.58%;"><img id="5o3t3eR3opjFYSHHsHXVc8" name="HLD-ease-of-use" alt="Looking Glass demos new Hololuminescent Display technology" src="https://cdn.mos.cms.futurecdn.net/5o3t3eR3opjFYSHHsHXVc8.jpg" mos="" link="" align="" fullscreen="" width="1578" height="1098" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Looking Glass )</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="4HAo4qoUA7Si7DTt9ZCRc8" name="HLD-side" alt="Looking Glass demos new Hololuminescent Display technology" src="https://cdn.mos.cms.futurecdn.net/4HAo4qoUA7Si7DTt9ZCRc8.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Looking Glass )</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="cPaVMcfhq4kJiS3QxG9Vc8" name="HLD-hero2" alt="Looking Glass demos new Hololuminescent Display technology" src="https://cdn.mos.cms.futurecdn.net/cPaVMcfhq4kJiS3QxG9Vc8.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Looking Glass )</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="Vcx2LYATG6YoBTu9DMsJc8" name="HLD-hero" alt="Looking Glass demos new Hololuminescent Display technology" src="https://cdn.mos.cms.futurecdn.net/Vcx2LYATG6YoBTu9DMsJc8.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Looking Glass )</span></figcaption></figure></div></div></div><p>Looking Glass’s Hololuminescent Displays will be available in Q4, and will start at $1,500 for an FHD 16-inch display (pre-order offer). A 27-inch 4K HLD will ship in November and December of this year, and 86-inch 4K displays will roll out in February 2026, says the firm, but pricing is yet to be disclosed.</p><p>Looking Glass <a data-analytics-id="inline-link" href="https://lookingglassfactory.com/displays-overview" target="_blank">Light Field Displays</a>, for advanced 3D visualization, interaction, and research, will continue to be offered to teams who work in 3D R&D and industrial visualization.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/monitors/looking-glass-demos-hololuminescent-display-monitors-sizes-range-from-16-to-85-inches-starting-at-usd1-500
Looking Glass has taken the wraps off its 'magical' and slim new monitors with Hololuminescent Display (HLD) technology. Models will be available soon, starting from $1,500.
8Ay5ytotN2WBEyh5zMqrY7
Tue, 16 Sep 2025 17:46:18 +0000 Monitors
Mark Tyson
Looking Glass
Looking Glass demos new Hololuminescent Display technology
Looking Glass demos new Hololuminescent Display technology
<![CDATA[ AMD launches four new Ryzen CPUs, including cut-down Zen 4 and Zen 3 models — most only available in global markets ]]>
<p>AMD has quietly released four new Ryzen CPUs without any official acknowledgement beyond adding the specs of each CPU to its website. The new chips are the eight-core Ryzen 7 9700F, six-core Ryzen 5 9500F, six-core Ryzen 5 7400, and six-core Ryzen 5 5600F.</p><p>The Ryzen 7 9700F and Ryzen 5 9500F are new chiplet-style<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-dishes-more-zen-5-details-compact-core-is-25-smaller-than-the-normal-core-new-soc-architecture-disclosed"> Zen 5</a> processors that lack integrated graphics. The 9700F is an eight-core chip featuring 32MB of L3 cache, 65W TDP, 3.8GHz base clock, and 5.5GHz peak boost clock. The 9500F features six Zen 5 CPU cores, 32MB of L3 cache, 65W TDP, a base clock of 3.8 GHz, and a peak boost clock of 5.2 GHz.</p><div ><table><tbody><tr><td class="firstcol empty" ></td><td
><p>Architecture</p></td><td
><p>Cores / Threads</p></td><td
><p>L3 Cache / TDP</p></td><td
><p>Base / Boost Clocks</p></td></tr><tr><td class="firstcol " ><p>Ryzen 7 9700F</p></td><td
><p>Zen 5</p></td><td
><p>8 / 16</p></td><td
><p>32 MB / 65W</p></td><td
><p>3.8 GHz / 5.5 GHz</p></td></tr><tr><td class="firstcol " ><p>Ryzen 5 9500F</p></td><td
><p>Zen 5</p></td><td
><p>6 / 12</p></td><td
><p>32 MB / 65W</p></td><td
><p>3.8 GHz / 5 GHz</p></td></tr><tr><td class="firstcol " ><p>Ryzen 5 7400</p></td><td
><p>Zen 4</p></td><td
><p>6 / 12</p></td><td
><p>16 MB / 65W</p></td><td
><p>3.3 GHz / 4.3 GHz</p></td></tr><tr><td class="firstcol " ><p>Ryzen 5 5600F</p></td><td
><p>Zen 3</p></td><td
><p>6 / 12</p></td><td
><p>32 MB / 65W</p></td><td
><p>3.0 GHz / 4.0 GHz</p></td></tr></tbody></table></div><p>The two chips are essentially F-series versions of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-ryzen-5-9600x-cpu-review">Ryzen 7 9700X </a>and Ryzen 5 9600/9600X that lack integrated graphics. This is especially the case with the 9700F, as it shares the same specs as the 9700X from the cores and cache, all the way down to the base and boost clocks as well. The Ryzen 5 9500F is also very similar to the Ryzen 5 9600/9600X, but both have higher clock speeds, with the 9600 boasting a 200MHz higher clock speed than the 9500F.</p><p>The Ryzen 5 7400 is a new entry-level Zen-4 CPU featuring six cores, 16 MB of L3 cache, 65W TDP, a base clock of 3.3GHz, and a maximum boost clock of 4.3GHz. This CPU is arguably one of the most unorthodox chips to come out from AMD, being one of the very first chips to come out <em>after</em> its F-series counterpart (the<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-silently-introduces-the-ryzen-5-7400f-based-on-raphael-six-zen-4-cores-a-boost-clock-of-4-7-ghz-and-global-availability"> Ryzen 5 7400F </a>has been on the market for several months).</p><p>What's even stranger is the CPU's L3 cache configuration of just 16MB. Traditionally, the 16MB cache limit is targeted at AMD's monolithic APUs, which physically don't hold more than 16 MB of L3 cache. However, the Ryzen 5 7400 is classified with the Raphael codename, meaning it takes advantage of AMD's chiplet-style design, which incorporates 32MB of L3 cache. Apparently, AMD has opted to disable half the L3 cache on this chip, probably as a method of reducing waste on potentially defective Zen 4 dies with defective L3 cache.</p><p>This strange configuration also makes the Ryzen 5 7400 a significantly different processor than its twin-by-name-alone, the Ryzen 5 7400F. While the 7400F lacks integrated graphics, it is a noticeably better processor, boasting a 400MHz higher base clock and boost clock, and 32MB of L3 cache. It could be argued that the Ryzen 5 7400 has more in common with the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/alleged-ryzen-8000g-apu-details-leak-zen-4-zen-4c-and-rdna-3">Ryzen 5 8500G</a>, which has just 16MB of L3 cache, integrated graphics, and six cores, though those cores consist of Zen 4 and<a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/amds-epyc-bergamo-and-zen-4c-detailed-same-as-zen-4-but-denser"> Zen 4c </a>cores.</p><p>The Ryzen 5 5600F is yet another Zen 3 part coming to the market. As the name states, the 5600F is another variant of the 5600/5600X featuring six Zen 3 cores, 32MB of L3 cache, 65W TDP, a 3.5GHz base clock, and a 4.4GHz peak boost clock. This chip is also somewhat unorthodox, sporting the F-series nomenclature. Chips with this nomenclature usually have disabled integrated graphics, but the Ryzen 5000 series does not support integrated graphics at all.</p><p>Apparently, the "F" in this case denotes reduced performance compared to its vanilla counterpart. The Ryzen 5 5600 features a 500MHz higher base clock and 400MHz higher boost clock than the 5600F.</p><p>The new Zen 3 chips are not the only recent re-use of old silicon. Intel has also re-released its <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intels-14nm-desktop-cpus-are-making-a-comeback-chipmaker-inexplicably-resurrects-comet-lake-from-five-years-ago-with-new-core-i5-110">10th Gen Core "Comet Lake"</a> silicon as the Core i5-110 in its Ultra Series 1 line.</p><p>These CPUs, specifically the 7400 and 5600F, continue to demonstrate AMD's commitment to providing as many solutions as possible. The main reason most of the more unorthodox chips exist is for global markets, particularly Asia or Latin America (like the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-quietly-launches-a-budget-gaming-beast-ryzen-5-5500x3d-arrives-for-the-latin-american-market">Ryzen 5 5500X3D</a>). The Ryzen 5 7400F is regionally exclusive to China and other Asian markets, the Ryzen 5 5600F is locked to the Asia-Pacific / Japan region, and the Ryzen 7 9700F is locked to North America. The Ryzen 5 9500F is the only chip of the bunch that boasts regional availability.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/cpus/amd-launches-four-new-ryzen-cpus-including-cutdown-zen-4-and-zen-3-models-most-only-available-in-global-markets
AMD has quietly released four new CPUs on three different architectures featuring the Ryzen 7 9700F, Ryzen 5 9500F, Ryzen 5 7400, and Ryzen 5 5600F.
p56z4iiRX8fFLAY3xUTgXG
Tue, 16 Sep 2025 17:25:38 +0000 CPUs
PC Components
editors@tomshardware.com (Aaron Klotz)
Aaron Klotz
AMD
Ryzen 9000
Ryzen 9000
<![CDATA[ Gearbox CEO channels inner Claptrap, offers himself up as personal tech support over rampant Borderlands 4 PC performance issues — 'Would you like help tuning with your personal specification?' ]]>
<p>In the weekend and change since its launch, Borderlands 4 has quickly become notorious as a PC resource hog the likes of which we have never seen before. Now, after days of personally crusading against complaints of poor optimization and performance issues, Gearbox CEO Randy Pitchford has taken to offering personal tech support to players struggling for performance.</p><p>After early reports at the weekend that <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/borderlands-4-launch-marred-by-performance-issues">Borderlands 4 was crushing even the most powerful PCs</a> — the 5090 can't manage native 60fps in some scenarios — publisher 2K put out an <a data-analytics-id="inline-link" href="https://www.tomshardware.com/video-games/pc-gaming/is-borderlands-4-the-new-crysis-official-gpu-setting-recommendations-peg-4k-performance-with-the-rtx-5090-at-60-fps-with-dlss-and-frame-generation-enabled">extensive list of recommended graphics settings for a variety of resolutions on both AMD and Nvidia cards</a>.</p><p>Pitchford's own contribution has been decidedly more controversial. "Every PC gamer must accept the reality of the relationship between their hardware and what the software they are running is doing," he posted on Monday in an X post, which received so much backlash that it got a community note.</p><div class="see-more see-more--clipped"><blockquote class="twitter-tweet hawk-ignore" data-lang="en"><p lang="en" dir="ltr">Every PC gamer must accept the reality of the relationship between their hardware and what the software they are running is doing.<a href="https://twitter.com/cantworkitout/status/1967424826697781446">September 15, 2025</a></p></blockquote><div class="see-more__filter"></div></div><p>In his less philosophical tweets, Pitchford has insisted that many of the Borderlands 4 players experiencing performance issues are simply demanding too much of their hardware, and that many would benefit from turning down the resolution or dialling up frame generation tech like DLSS. However, that hasn't stopped him from moving to offering personal tech support to people struggling.</p><p>"Would you like help tuning with your personal specification?," he asked one user with an RTX 5080 in a tone that will be all too familiar to fans of the franchise. "You should be in good shape," he replied. "Run on that 1440p monitor.
Recommend two inputs into that G8 so you're running it split screen. What FPS are you getting? Vsync off. Disable anti-aliasing. Use DLSS. Multi-frame generation at 4x. nVIDIA Reflex: On. Match your frame rate limiter to your monitor's refresh rate.
You can set everything to very high (but you can gain some more frames if you turn down or off the volumetric fog). Let me know what FPS you get with those settings."</p><p>"At 1440p you should be blazing fast," he continued. Pitchford has responded to plenty of comments about the game's poor performance and optimization with offers of personal tech support, not all of them landing well. "You are being unbelievably insufferable, Randy," one user retorted to his offer. "I'm trying to be helpful," the beleaguered CEO replied, before asking: "What do you think would be helpful? What would you have me do?"</p><p>Pitchford's messaging is certainly more than a little mixed and likely not helping the reception to the game. On the one hand, he has insisted the game is built well and that users are demanding too much of their hardware. On the other hand, Gearbox has pushed out a couple of PC performance patches, and the CEO says they are continuing work <a data-analytics-id="inline-link" href="https://x.com/DuvalMagic/status/1967835152861798890" target="_blank">behind the scenes.</a> Fans of the tenured looter shooter would be remiss not to admit that there's more than a little hint of the desperation that made its one-wheeled protagonist Claptrap an infamous mainstay of the series in his posts.</p><p>Borderlands 4 is the latest installment in Gearbox's popular looter shooter franchise, and in many ways is a hefty departure from previous games. It eschews the setting of homeworld Pandora for the first time, pitting players into a fight for liberation from the Timekeeper, the overlord of the planet Kairos.</p><p>Performance issues aside, fans can expect a return of many of Borderlands' most popular mechanics, including randomly generated loot, humorous dialogue, and prolific violence.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/video-games/pc-gaming/gearbox-ceo-channels-inner-claptrap-offers-himself-up-as-personal-tech-support-over-rampant-borderlands-4-pc-performance-issues-would-you-like-help-tuning-with-your-personal-specification
Gearbox CEO Randy Pitchford has taken to X in a bit to offer personal tech support to Borderlands 4 players struggling for PC performance.
HiKEBAcX9FxsafmfHCJK4R
Tue, 16 Sep 2025 17:17:33 +0000 PC Gaming
Video Games
stephen.warwick@futurenet.com (Stephen Warwick)
Stephen Warwick
Gearbox
Borderlands 4
Borderlands 4
<![CDATA[ Corsair launches gargantuan 3,000W power supply for $599.99 — comes with four native 12V‑2x6 600W GPU cables ]]>
<p>Corsair has introduced the brand's first power supply exceeding 1,600W. The latest WS3000, featuring a capacity of 3,000W and 80 Plus Platinum-certified, is engineered to support systems and workstations equipped with multi-GPU configurations.</p><p>With dimensions measuring 6.9 x 5.9 x 3.4 inches (175 x 150 x 86 mm), the WS3000 constitutes a standard ATX 3.1 power supply that adheres to the PCIe 5.1 specification. Its length of 6.9 inches positions the WS3000 as potentially one of the most compact 3,000W power supplies available, facilitating installation within even conventional ATX cases. Like many contemporary high-end units, the WS3000 is equipped with a modular design, thus simplifying cable management.</p><p>The WS3000 boasts a power capacity of 3,000W and features a single-rail design. This configuration indicates that the power supply provides up to 250A on the +12V rail. It is essential to note that the WS3000 is designed for 220-240V operation; therefore, it is imperative to ensure that your residence is wired for 240V and has a circuit that complies with the specifications. Consequently, the WS3000 employs a C19 power cable, which has a physically larger connector and is rated for a higher current (16A compared to 10A), as opposed to the standard C13 power cable used with typical power supplies.</p><p>The WS3000 is not the first or only Corsair power supply to utilize the C19 power cable. Numerous high-capacity units from the brand, such as the HX1500i and AX1600i, have already employed the C19 power cable.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="FbZr6QT8GeZMHSAzGAv8QN" name="WS3000_BLACK_05_PERSP_REAR.width-540.format-webp" alt="Corsair WS3000" src="https://cdn.mos.cms.futurecdn.net/FbZr6QT8GeZMHSAzGAv8QN.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Corsair)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1955px;"><p class="vanilla-image-block" style="padding-top:56.27%;"><img id="5tfdkzyAmm2muRiQAcj3SN" name="ws3000_01" alt="Corsair WS3000" src="https://cdn.mos.cms.futurecdn.net/5tfdkzyAmm2muRiQAcj3SN.jpg" mos="" link="" align="" fullscreen="" width="1955" height="1100" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Corsair)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.30%;"><img id="JBHYWEJzjx62MwHVzeAcRN" name="WS3000_BLACK_04_PORTS.width-540.format-webp" alt="Corsair WS3000" src="https://cdn.mos.cms.futurecdn.net/JBHYWEJzjx62MwHVzeAcRN.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1081" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Corsair)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 4 of 4</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="e9o3k8wN8EETjenJSANwQN" name="WS3000_BLACK_13_CABLES.width-1000.format-webp" alt="Corsair WS3000" src="https://cdn.mos.cms.futurecdn.net/e9o3k8wN8EETjenJSANwQN.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: Corsair)</span></figcaption></figure></div></div></div><p>The WS3000 leverages a 140mm double-ball-bearing fan for active cooling. However, it does not have the Zero RPM mode found in consumer models. This is because Corsair considers noise reduction a lower priority for the WS3000's use case. Additionally, it does not support iCUE, so you won't be able to monitor or control the WS3000 through Corsair's software.</p><p>Designed to support multi-GPU configurations, the WS3000 is equipped with four native <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/16-pin-power-connector-gets-a-much-needed-revision-meet-the-new-12v-2x6-connector" target="_blank">12V-2x6 </a>power cables, specifically designed for contemporary graphics cards that utilize the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/pcie-5-power-connector-600w-next-gen-amd-nvidia-gpus">12VHPWR</a> power connector. Furthermore, the power supply provides four dual 8-pin PCIe power cables, totaling eight 8-pin PCIe power connectors. Additionally, it includes two 8-pin EPS power connectors intended for high-end or workstation motherboards with power-hungry processors.</p><p>Corsair backs the WS3000 (CP-9020312-NA) with a limited ten-year warranty. The power supply has an MSRP of $599.99; however, we've seen it listed at Central Computer, a U.S. retailer, for <a data-analytics-id="inline-link" href="https://www.centralcomputer.com/corsair-cp-9020312-na-ws3000-3000w-atx-3-1-fully-modular-workstation-power-supply-atx-3-1-80-plus-platinum-certified.html">$549.99,</a> so it's possible to acquire the power supply for below MSRP.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q=" target="_blank"><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/pc-components/power-supplies/corsair-launches-gargantuan-3-000w-power-supply-for-usd599-99-comes-with-four-native-12v-2x6-600w-gpu-cables
Corsair launches WS3000, a power supply with a 3,000W capacity for systems with multi-GPU setups.
hwCe3u47JJFicfNJqiJUgA
Tue, 16 Sep 2025 16:57:52 +0000 Power Supplies
PC Components
Zhiye Liu
Corsair
Corsair WS3000
Corsair WS3000
<![CDATA[ Japan to subsidize undersea cable vessels over 'very serious' national security concerns — will front up to half the cost for $300 million vessels bought by NEC ]]>
<p>Japanese officials are set to offer subsidies to NEC, one of the country's biggest tech companies, to purchase cable-laying ships capable of traversing oceans. According to the <a data-analytics-id="inline-link" href="https://www.ft.com/content/8bf18101-4afa-4110-aad8-66fc76c4c70a" target="_blank"><em>Financial Times</em></a>, Tokyo is willing to cover as much as half of the acquisition cost of these vessels. With each ship estimated to cost around $300 million, Japan is seemingly ready to spend half a billion dollars or more to allow NEC to have unfettered access to its own cable-laying vessels, a matter the government now considers of vital importance to national security.</p><p>NEC is one of the largest layers of undersea cables and the biggest one in Asia, having already installed over 400,000 km globally. It currently competes with US-based SubCom, the French government-owned Alcatel Submarine Networks, and China’s HMN Tech. All of these companies have their own fleets, with each of them owning between two and seven vessels. On the other hand, NEC does not own any cable-laying ships and instead leases them from other operators.</p><p>The company currently rents a subsea cable-laying vessel from a Norwegian corporation, with the charter expected to expire next year. Aside from that, it would contract various specialist ships as needed to serve its needs, especially as demand for undersea cables increases within the Indo-Pacific region. The company also occasionally rents smaller cable-laying vessels from Japanese companies NTT and KDDI, but they’re not equipped for traversing oceanic routes and can only work within regional waters.</p><p>Undersea cables are crucial for connecting countries to the rest of the world, and adversaries have increasingly been attacking and sabotaging this relatively unprotected infrastructure. Their placement in international waters means that their destruction isn’t automatically an overt act of war, and the responsible vessels often have murky ownership, making deniability easy. It could also often be chalked up as an accident, so prosecuting offending ships and crews is difficult, if not politically sensitive.</p><p>It has gotten to the point that the Taiwanese Coast Guard is <a data-analytics-id="inline-link" href="https://www.tomshardware.com/networking/taiwan-increases-undersea-cable-protection-patrols-closely-monitoring-96-blacklisted-china-linked-boats">conducting patrols to protect the 24 undersea cables connecting the island</a> to the rest of the world. This came within a year after several high-profile incidents of cuts to undersea internet cables were reported across the globe, starting with a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/two-undersea-internet-cables-connecting-finland-and-sweden-to-europe-have-been-cut-eu-leaders-suspect-sabotage">disruption between Finland and Sweden in November 2024</a>, followed by <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/undersea-power-cable-connecting-finland-and-estonia-experiences-outage-capacity-reduced-to-35-percent-as-finnish-authorities-investigate">outages between Finland and Estonia in December</a>. Then, in January, a Chinese freighter was suspected of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/networking/chinese-freighter-suspected-of-damaging-undersea-internet-cable-connecting-the-us-and-taiwan">damaging an internet cable that links the U.S. and Taiwan</a>, with another ship suspected of <a data-analytics-id="inline-link" href="https://www.tomshardware.com/networking/taiwanese-authorities-detain-fishy-chinese-freighter-suspected-of-cutting-undersea-internet-cable">cutting another cable the following month</a>.</p><p>Because of these incidents, the Japanese government thought that it would be better for NEC to have its own ships. That way, it does not have to rely on charters and rentals, which can bottleneck its operations and prevent it from responding as quickly as possible to cable-laying and cable-repair operations, the report cites officials who say the current lack of ships is a national security risk. "The Japanese government thinks this situation is very serious, so we are thinking we need to make some intervention," one official told <em>FT</em>.</p><p>Nevertheless, NEC purchasing its own vessels, even at subsidized cost, is still a hefty outlay for the company. “Owning a vessel is a huge fixed cost — that’s all well when the market is growing, but when the tech bubble bursts like it did in 2000, then it becomes simply a big cost,” NEC Senior Director for the Submarine Network Division Takahisa Ohta told the <em>Financial Times.</em> “Thankfully, the market is booming now, so one option is to acquire our own ship, and it’s something we’re considering.”</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB" target="_blank"><em>Tom's Hardware on Google News</em></a><em> to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.</em></p>
https://www.tomshardware.com/networking/japan-to-subsidize-undersea-cable-vessels-over-very-serious-national-security-concerns-will-front-up-to-half-the-cost-for-usd300-million-vessels-bought-by-nec
Japan wants NEC to buy its own undersea cable-laying ships and is considering covering nearly half the purchase cost of these vessels.
PGSEW7ekd7mZ4s374hymu3
Tue, 16 Sep 2025 15:31:32 +0000 Networking
editors@tomshardware.com (Jowi Morales)
Jowi Morales
Shutterstock
Undersea cable vessel
Undersea cable vessel
<![CDATA[ Asus' dual-mode 32-inch dual-mode gaming monitor is on sale for just $500 — 4K and refresh rates up to 320 Hz for $100 off ]]>
<p>Asus is one of the most reliable names when it comes to computer hardware and peripherals. They make excellent gaming monitors, and today's deal is no exception. If you've been looking to get a high refresh rate monitor without sacrificing on sharpness, <a data-analytics-id="inline-link" href="https://www.amazon.com/ASUS-Strix-Gaming-Monitor-XG32UCG/dp/B0F23BB8TW" target="_blank">Asus' XG32UCG is on sale for just $500</a> on Amazon. This is a 4K 32" dual-mode monitor that has a native refresh rate of 160 Hz, which doubles to 320 Hz when you select the FHD (1080p) mode. It usually retails for more, but you can avail a $100 discount right now and snag a great deal.</p><ul><li><a href="https://www.amazon.com/s?k=4k+gaming+monitor&i=electronics&crid=6AQS6C8BGPKR&sprefix=4k+gaming+monito%2Celectronics%2C327" target="_blank">Check out 4K gaming monitors on Amazon</a></li></ul><div class="product star-deal"><a data-dimension112="4bc04008-5581-4d94-9974-8d731d86ffba" data-action="Star Deal Block" data-label="Enjoy the best of both worlds with this dual-mode monitor that can refresh your games at up to 320 times a second at 1080p, or 160 times at 4K. With excellent build quality, customary RGB lighting, and solid connectivity, the XG32UCG is a great gaming monitor that's now available at a matching great price." data-dimension48="Enjoy the best of both worlds with this dual-mode monitor that can refresh your games at up to 320 times a second at 1080p, or 160 times at 4K. With excellent build quality, customary RGB lighting, and solid connectivity, the XG32UCG is a great gaming monitor that's now available at a matching great price." data-dimension25="$499" href="https://www.amazon.com/ASUS-Strix-Gaming-Monitor-XG32UCG/dp/B0F23BB8TW" target="_blank" rel="nofollow"><figure class="van-image-figure "
><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1280px;"><p class="vanilla-image-block" style="padding-top:85.94%;"><img id="nUkXoFxtLjfkpGYtDZkUCF" name="145507" caption="" alt="" src="https://cdn.mos.cms.futurecdn.net/nUkXoFxtLjfkpGYtDZkUCF.webp" mos="" align="middle" fullscreen="" width="1280" height="1100" attribution="" endorsement="" credit="" class=""></p></div></div></figure></a><div><span class="product__star-deal-label">Dual-Mode</span><p>Enjoy the best of both worlds with this dual-mode monitor that can refresh your games at up to 320 times a second at 1080p, or 160 times at 4K. With excellent build quality, customary RGB lighting, and solid connectivity, the XG32UCG is a great gaming monitor that's now available at a matching great price.<a class="view-deal button" href="https://www.amazon.com/ASUS-Strix-Gaming-Monitor-XG32UCG/dp/B0F23BB8TW" target="_blank" rel="nofollow" data-dimension112="4bc04008-5581-4d94-9974-8d731d86ffba" data-action="Star Deal Block" data-label="Enjoy the best of both worlds with this dual-mode monitor that can refresh your games at up to 320 times a second at 1080p, or 160 times at 4K. With excellent build quality, customary RGB lighting, and solid connectivity, the XG32UCG is a great gaming monitor that's now available at a matching great price." data-dimension48="Enjoy the best of both worlds with this dual-mode monitor that can refresh your games at up to 320 times a second at 1080p, or 160 times at 4K. With excellent build quality, customary RGB lighting, and solid connectivity, the XG32UCG is a great gaming monitor that's now available at a matching great price." data-dimension25="$499">View Deal</a></p></div></div><p>The XG32UCG uses a Fast IPS panel that has a 0.3ms (claimed) response time. It also supports G-Sync and AMD's FreeSync. Moreover, it features ELMB-Sync, a rare feature that allows backlight strobing to work with VRR by synchronizing both. That means no weird ghosting or flickering amid fluctuating frame rates. Of course, the hallmark feature here is the dual-mode support, which can be toggled with a simple hotkey button, allowing you to prioritize visual clarity (4K 160Hz) or smoothness (1080p 320Hz) at will.</p><p>Despite lacking local dimming, there's support for HDR400 here, but don't expect striking contrast or blinding brightness since this is not OLED or Mini-LED. Still, you do get 95% coverage of the DCI-P3 color gamut, along with 130% coverage of sRGB. There's plenty of ports onboard too, including DisplayPort 1.4, HDMI 2.1, USB-C with 15W power delivery, and a headphone jack.</p><p>Another highlight of this monitor is the DisplayWidget Center app that allows you to control the OSD right inside Windows, eliminating the need to reach for physical buttons. Asus also markets a few AI features, such as "Dynamic Crosshair" and "Dynamic Shadow Boos,t" but even without these, the XG32UCG is a great all-rounder, made even better at its discounted price.</p><p><em>If you're looking for more savings, check out our </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-deals-on-tech"><em>Best PC Hardware deals</em></a><em> for a range of products, or dive deeper into our specialized </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-deals-on-ssds"><em>SSD and Storage Deals,</em></a><em> </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/ssds/best-hard-drive-deals"><em>Hard Drive Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-computer-monitor-deals"><em>Gaming Monitor Deals</em></a><em>, </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/best-graphics-card-deals-now"><em>Graphics Card Deals</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://www.tomshardware.com/features/best-cpu-deals"><em>CPU Deals</em></a><em> pages.</em></p>
https://www.tomshardware.com/pc-components/asus-dual-mode-32-inch-dual-mode-gaming-monitor-is-on-sale-for-just-usd500-4k-and-refresh-rates-up-to-320-hz-for-usd100-off
If you're looking for a monitor that's both sharp and fast, Asus has got you covered with its XG32UCG. This dual-mode gaming monitor can display the best visuals at 4K 160Hz—which is plenty fast already—or give you a genuine competitive edge at 1080p 320Hz. Get it for a discounted price of just $500 now.
ipGm4ce3AmCDe4HRTE5Zoe
Tue, 16 Sep 2025 15:03:43 +0000 PC Components
editors@tomshardware.com (Hassam Nasir)
Hassam Nasir
Asus / Future
Asus ROG Strix XG32UCG gaming monitor on sale
Asus ROG Strix XG32UCG gaming monitor on sale
<![CDATA[ Norbauer's $8,000 keyboard waitlist climbs to 9 months — the world's most expensive keyboard is perpetually out of stock ]]>
<p>The self-proclaimed “keyboard dream from the future,” the Norbauer Seneca mechanical keyboard, is one of the most expensive production input devices we have seen. Despite its sky-high price range, spanning $3,600 to $8,090 plus extras, the Seneca is currently out of stock. Moreover, the wait list is a luxury sports car-esque six to nine months, according to the <a data-analytics-id="inline-link" href="https://www.norbauer.co/products/the-seneca?s=31" target="_blank">official site</a>.</p><div class="inlinegallery
carousel-layout"><div class="inlinegallery-wrap" style="display:flex; flex-flow:row nowrap;"><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 1 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1600px;"><p class="vanilla-image-block" style="padding-top:63.13%;"><img id="VZaaiyW3PcpWXqnVhSokBN" name="seneca-main" alt="Norbauer Seneca First Edition" src="https://cdn.mos.cms.futurecdn.net/VZaaiyW3PcpWXqnVhSokBN.jpg" mos="" link="" align="" fullscreen="" width="1600" height="1010" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: <a href="https://www.norbauer.co/products/the-seneca?s=31" target="_blank">Norbauer</a>)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 2 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="7VdghzCdgKyKWC4UDMjhDN" name="titanium-finish" alt="Norbauer Seneca First Edition" src="https://cdn.mos.cms.futurecdn.net/7VdghzCdgKyKWC4UDMjhDN.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: <a href="https://www.norbauer.co/products/the-seneca?s=31" target="_blank">Norbauer</a>)</span></figcaption></figure></div><div class="inlinegallery-item" style="flex: 0 0 auto;"><span class="slidecount">Image 3 of 3</span><figure class="van-image-figure " data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="YqQyWndfPdre7bNMRLtvBN" name="assembled" alt="Norbauer Seneca First Edition" src="https://cdn.mos.cms.futurecdn.net/YqQyWndfPdre7bNMRLtvBN.jpg" mos="" link="" align="" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=""><span class="credit" itemprop="copyrightHolder">(Image credit: <a href="https://www.norbauer.co/products/the-seneca?s=31" target="_blank">Norbauer</a>)</span></figcaption></figure></div></div></div><p>We first <a data-analytics-id="inline-link" href="Norbauer’s">reported on the Seneca</a> back in May 2024, ahead of the First Edition’s touted release schedule. At that time, we summed up this premium keyboard’s design as offering a retro-futuristic electro-capacitive keyboard with purportedly silent stabilizers – at a price. We also deep dived into Ryan Norbauer’s quest to optimize key stabilizers, without any trade-offs. And, of course, we stared slack-jawed at the cost of this thing. Who would have guessed that Norbauer & Co. would end up not being able to keep up with the pace of demand?</p><div class="youtube-video" data-nosnippet ><div class="video-aspect-box"><iframe data-lazy-priority="low" data-lazy-src="https://www.youtube.com/embed/N3FEv1qw4_w" allowfullscreen></iframe></div></div><p><em>Above: Adam Savage of MythBusters fame recently interviewed Ryan Norbauer.</em></p><p>So, to help fill the time while you wait for your Seneca order to be fulfilled, let’s take a look at what your (potentially) high-four-figure outlay gets you. In addition to the underlying keyswitch and stabilizer tech, which is a signature feature of the Seneca, Norbauer doesn’t shy away from the use of premium materials and manufacturing methods.</p><p>There are four finish options for the First Edition TKL keyboard, and they are all based on a metal chassis. Three aluminum finishes are available, and these are Oxide Gray, Travertine, and Heatshield. They all look matte and are fingerprint-resistant, providing a solid case for the keyboard mechanism. If you want to push the boat out further, there’s also a raw, uncoated titanium chassis option, with a sandblasted surface.</p><p>For keycaps, Norbauer has looked to the aesthetics of the first personal computers, with a slightly sculpted finish, which is also known as MTNU. The material of choice is double-shot PBT, as used on many of the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/peripherals/gaming-keyboards/best-gaming-keyboards">best mechanical keyboards</a> we see in our test labs.</p><p>All these material choices and refinements sound appealing, but even at this astronomical price, there seem to be some compromises that might mean the money-is-no-object Norbauer Seneca isn’t for you.</p><h2 id="missing-features-2">Missing features</h2><p>Let’s start by considering whether you might like to tilt the keyboard forward a little. Would you type more comfortably by increasing the keyboard rake? Norbauer eschews adjustable feet built into the keyboard, and instead offers a “beautiful teak wood” riser that slots under the keyboard, adding a 3° incline and $290. It is claimed this sliver of wood is "precision CNC machined in South Africa." In terms of woodcraft, CNC may be accurate, but it isn't a premium production method (hello IKEA). Norbauer's keyboard riser is basically a kitchen chopping board with an incline.</p><figure class="van-image-figure
inline-layout" data-bordeaux-image-check ><div class='image-full-width-wrapper'><div class='image-widthsetter' style="max-width:1920px;"><p class="vanilla-image-block" style="padding-top:56.25%;"><img id="7ynbreeC8ePFQLFQzGqHBN" name="290-dollars" alt="Norbauer Seneca First Edition" src="https://cdn.mos.cms.futurecdn.net/7ynbreeC8ePFQLFQzGqHBN.jpg" mos="" align="middle" fullscreen="" width="1920" height="1080" attribution="" endorsement="" class=""></p></div></div><figcaption itemprop="caption description" class=" inline-layout"><span class="credit" itemprop="copyrightHolder">(Image credit: <a href="https://www.norbauer.co/products/the-seneca?s=31" target="_blank">Norbauer</a>)</span></figcaption></figure><p>We also need to consider several modern comforts that the Norbauer Seneca is missing. Many are accustomed to keyboards with <a data-analytics-id="inline-link" href="https://www.tomshardware.com/best-picks/best-wireless-keyboards">wireless connectivity options</a>, some level of key backlighting, and additional media keys and knobs for quick and intuitive adjustments. The Seneca doesn’t offer any of these common frills. Also, numpad lovers will be steering clear of this input device.</p><p>Before we go, considering the price and lack of features that some modern computer users might find essential, it might be tempting to assume Norbauer is purposely slow in its production process. The less cynical might suggest this is to ensure consistency and quality, but others may conclude it is a tactic to develop the panache of artificial scarcity.</p><p><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLAgKIiZDQklTRmdnTWFoSUtFSFJ2YlhOb1lYSmtkMkZ5WlM1amIyMG9BQVAB"><em>Tom's Hardware on Google News</em></a><em>, or </em><a data-analytics-id="inline-link" href="https://google.com/preferences/source?q="><em>add us as a preferred source</em></a><em>, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!</em></p>
https://www.tomshardware.com/peripherals/mechanical-keyboards/norbauers-usd8-000-keyboard-waitlist-climbs-to-9-months-the-worlds-most-expensive-keyboard-is-perpetually-out-of-stock
The premium Norbauer Seneca mechanical keyboard is one of the most expensive input devices we have seen. Despite its sky-high price range, spanning $3,600 to $8,090 plus extras, device buyers face up to a nine-month wait.
ErWaCEQrXG3R9ALpzWG7nF
Tue, 16 Sep 2025 14:24:25 +0000 Mechanical Keyboards
Peripherals
Keyboards
Mark Tyson
Norbauer
Norbauer Seneca First Edition
Norbauer Seneca First Edition