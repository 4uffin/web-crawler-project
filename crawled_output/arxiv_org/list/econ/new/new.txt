Economics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
econ
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Economics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Tuesday, 16 September 2025
Total of 38 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 16 of 16 entries)
[1]
arXiv:2509.10548
[pdf, html, other]
Title:
The Economics and Game Theory of OSINT Frontline Photography: Risk, Attention, and the Collective Dilemma
Jonathan Teagan
Subjects:
General Economics (econ.GN)
This paper develops an economic model of the Open Source Intelligence (OSINT) attention economy in contemporary armed conflict. We conceptualize attention (e.g. social media views, followers, likes) as revenue, and time and risk spent in analysis as costs. Using utility functions and simple game theoretic setups, we show how OSINT actors (amateurs, journalists, analysts, and state operatives) allocate effort to maximize net attention benefit. We incorporate strategic behaviors such as a first mover advantage (racing to publish) and prisoner's dilemma scenarios (to share information or hold it back). In empirical case studies, especially the Ukraine conflict actors like the UAV unit Madyar's Birds and volunteer channels like Kavkazfighter, illustrate how battlefront reporting translates into digital revenue (attention) at real cost. We draw on recent literature and data (e.g., public follower counts, viral posts) to examine trends such as OSINT virality. Finally, we discuss policy implications for balancing transparency with operational security, citing calls for verification ethics and attention sustaining narratives. Our analysis bridges conflict studies and economics, highlighting OSINT as both a public good and a competitive product in today's information war.
[2]
arXiv:2509.10567
[pdf, other]
Title:
Choice Paralysis in Evolutionary Games
Brendon G. Anderson
Subjects:
Theoretical Economics (econ.TH); Computer Science and Game Theory (cs.GT); Dynamical Systems (math.DS)
In this paper, we consider finite-strategy approximations of infinite-strategy evolutionary games. We prove that such approximations converge to the true dynamics over finite-time intervals, under mild regularity conditions which are satisfied by classical examples, e.g., the replicator dynamics. We identify and formalize novel characteristics in evolutionary games: choice mobility, and its complement choice paralysis. Choice mobility is shown to be a key sufficient condition for the long-time limiting behavior of finite-strategy approximations to coincide with that of the true infinite-strategy game. An illustrative example is constructed to showcase how choice paralysis may lead to the infinite-strategy game getting "stuck," even though every finite approximation converges to equilibrium.
[3]
arXiv:2509.10788
[pdf, html, other]
Title:
Choquet rank-dependent utility
Zachary Van Oosten, Ruodu Wang
Subjects:
Theoretical Economics (econ.TH)
We propose a new decision model under ambiguity, called the Choquet rank-dependent utility model. The model extends the Choquet expected utility model by allowing for the reduction to the rank-dependent utility model in the absence of ambiguity, rather than to the expected utility model. The model has three major components: a utility function $u$ and a probability distortion $g$, which together capture the risk component of the preferences, and generalized probabilistic beliefs $\nu$, which captures the ambiguity component of the preferences. The representation takes the form $X\succsim Y\iff \int_{\Omega} u(X)d(g\circ\nu)\iff \int_{\Omega} u(Y)d(g\circ\nu).$ To obtain the axiomatization, we work in the uncertainty setting of Savage with a non-ambiguous source. Afterwards, we discuss ambiguity attitudes and their representation with respect to the generalized probabilistic beliefs, along with conditions for a robust representation.
[4]
arXiv:2509.10795
[pdf, other]
Title:
The impact on health system expenditure in Australia and OECD countries from accelerated NCD mortality decline through prevention or treatment strategies to achieve Sustainable Development Goal Target 3.4
Bibha Dhungel, Jingjing Yang, Tim Wilson, Samantha Grimshaw, Emily Bourke, Stephanie Khuu, Tony Blakely
Subjects:
General Economics (econ.GN)
Background: It is unclear what the relative impacts of prevention or treatment of NCDs are on future health system expenditure. First, we estimated expenditure in Australia for prevention vs treatment pathways to achieve SDG target 3.4. Second, we applied the method to 34 other OECD countries.
Methods: We used GBD data to estimate average annual percentage changes in disease incidence, remission, and CFRs from 1990-2021, and projected to 2030 to estimate business-as-usual (BAU) reductions in NCD mortality risk (40q30). For countries not on track to meet SDG3.4 under BAU, we modelled two intervention scenarios commencing in 2022 to achieve SDG3.4: (1) prevention via accelerated incidence reduction; (2) treatment via accelerated increases in remission and decreases in CFRs. Australian disease expenditure data were input into a PMSLT model to estimate expenditure changes from 2022 to 2040. Assuming similar expenditure patterns, the method was applied across OECD countries.
Findings: In Australia, current trends project a 25% reduction in 40q30 by 2030, short of the 33.3% SDG3.4 target. Achieving this requires a 2.53 percentage point (pp) annual acceleration in incidence decline (prevention) or 1.56pp acceleration in CFR reduction and remission increase (treatment). Prevention reduces disease expenditure by 0.72%-3.17% by 2030 and 2040; treatment initially increase expenditure by 0.16%, before reducing it by 0.98%. A treatment scenario reducing only CFRs increased expenditure initially; increasing remission alone achieved savings similar to prevention. Only Sweden, Ireland, and South Korea were on track to meet SDG3.4. Other OECD countries showed similar expenditure impacts to Australia.
Interpretation: Whether reducing NCD mortality saves money depends on pathway taken (prevention or treatment). Care is needed when linking NCD mortality reduction to health system savings.
[5]
arXiv:2509.10933
[pdf, html, other]
Title:
Bailouts and Redistribution
Mikayel Sukiasyan
Comments:
Job market paper; May 2021
Subjects:
General Economics (econ.GN)
What is the best macroprudential regulation when households differ in their exposure to profits from the financial sector? To answer the question, I study a real business cycle model with household heterogeneity and market incompleteness. In the model, shocks are amplified in states with high leverage, leading to lower investment. I consider the problem of a Ramsey planner who can finance transfers with a distortive tax on labor and levy taxes on the balance sheet components of experts. I show that the optimal tax on capital purchases is zero and the optimal policy relies mostly on a tax on deposit issuance. The latter redistributes between agents by affecting the equilibrium rate on deposits. The welfare gains from optimal policy are due to both redistribution and insurance and are larger the more unequal the initial distribution is. A simple tax rule that targets a level of leverage can achieve most of the welfare gains from optimal policy.
[6]
arXiv:2509.10942
[pdf, html, other]
Title:
Matching to two sides
Chao Huang
Subjects:
Theoretical Economics (econ.TH)
This paper studies a matching problem in which a group of agents cooperate with agents on two sides. In environments with either nontransferable or transferable utilities, we demonstrate that a stable outcome exists when cooperations exhibit same-side complementarity and cross-side substitutability. Our results apply to pick-side matching problems and membership competition in online duopoly markets.
[7]
arXiv:2509.11060
[pdf, html, other]
Title:
Large-Scale Curve Time Series with Common Stochastic Trends
Degui Li, Yu-Ning Li, Peter C.B. Phillips
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
This paper studies high-dimensional curve time series with common stochastic trends. A dual functional factor model structure is adopted with a high-dimensional factor model for the observed curve time series and a low-dimensional factor model for the latent curves with common trends. A functional PCA technique is applied to estimate the common stochastic trends and functional factor loadings. Under some regularity conditions we derive the mean square convergence and limit distribution theory for the developed estimates, allowing the dimension and sample size to jointly diverge to infinity. We propose an easy-to-implement criterion to consistently select the number of common stochastic trends and further discuss model estimation when the nonstationary factors are cointegrated. Extensive Monte-Carlo simulations and two empirical applications to large-scale temperature curves in Australia and log-price curves of S&P 500 stocks are conducted, showing finite-sample performance and providing practical implementations of the new methodology.
[8]
arXiv:2509.11271
[pdf, html, other]
Title:
Out-of-sample gravity predictions and trade policy counterfactuals
Nicolas Apfel, Holger Breinlich, Nick Green, Dennis Novy, J.M.C. Santos Silva, Tom Zylkin
Subjects:
General Economics (econ.GN)
Gravity equations are often used to evaluate counterfactual trade policy scenarios, such as the effect of regional trade agreements on trade flows. In this paper, we argue that the suitability of gravity equations for this purpose crucially depends on their out-of-sample predictive power. We propose a methodology that compares different versions of the gravity equation, both among themselves and with machine learning-based forecast methods such as random forests and neural networks. We find that the 3-way gravity model is difficult to beat in terms of out-of-sample average predictive performance, further justifying its place as the predominant tool for applied trade policy analysis. However, when the goal is to predict individual bilateral trade flows, the 3-way model can be outperformed by an ensemble machine learning method.
[9]
arXiv:2509.11501
[pdf, html, other]
Title:
The Price of Disaster: Estimating the Impact of Hurricane Harvey on the Texas Construction Labor Market
Kartik Ganesh
Comments:
16 pages, 6 figures, 4 tables
Subjects:
General Economics (econ.GN); Computation (stat.CO)
This paper estimates the effect of Hurricane Harvey on wages and employment in the construction labor industry across impacted counties in Texas. Based on data from the Quarterly Census of Employment and Wages (QCEW) for the period 2016-2019, I adopted a difference-in-differences event study approach by comparing results in 41 FEMA-designated disaster counties with a set of unaffected southern control counties. I find that Hurricane Harvey had a large and long-lasting impact on labor market outcomes in the construction industry. More precisely, average log wages in treated counties rose by around 7.2 percent compared to control counties two quarters after the hurricane and remained high for the next two years. Employment effects were more gradual, showing a statistically significant increase only after six quarters, in line with the lagged nature of large-scale reconstruction activities. These results imply that natural disasters can generate persistent labor demand shocks to local construction markets, with policy implications for disaster recovery planning and workforce mobilization.
[10]
arXiv:2509.11538
[pdf, html, other]
Title:
The Dynamics of the Profit Rate in an Extended Okishio Framework
Jihyuan Liuh
Subjects:
Theoretical Economics (econ.TH)
Building on the classical Okishio theorem, we construct a two-sector, multi-period dynamic model that relaxes the rigid assumption of a fixed real wage and introduces an endogenous wage-growth mechanism together with a process of technology diffusion. Through analytical derivations and numerical simulations we find that the long-run trajectory of the profit rate is not unique: it hinges on the relative strength of the speed of wage adjustment and the potency of technical progress. (1) When wage adjustment is relatively sluggish, the technical effect dominates and the profit rate trends upward. (2) When wage adjustment proceeds at a moderate pace, the profit rate first rises and then falls. (3) When wage adjustment is extremely rapid, the wage effect dominates and the profit rate declines continuously. The results offer a new theoretical lens on the intricate interplay among technical change, wage dynamics and profitability.
[11]
arXiv:2509.11562
[pdf, html, other]
Title:
Human or Robot? Evidence from Last-Mile Delivery Service
Baorui Li, Xincheng Ma, Brian Rongqing Han, Daizhong Tang, Lei Fu
Subjects:
General Economics (econ.GN)
As platforms increasingly deploy robots alongside human labor in last-mile logistics, little is known about how contextual features like product attributes, environmental conditions, and psychological mechanisms shape consumer preference in real-world settings. To address this gap, this paper conducts an empirical study on consumer choice between human versus robot service, analyzing 241,517 package-level choices from Alibaba's last-mile delivery stations. We identify how product privacy sensitivity, product value, and environmental complexity affect consumer preference. Our findings reveal that consumers are significantly more likely to choose robot delivery for privacy-sensitive packages (11.49%) and high-value products (0.97% per 1% increase in value), but prefer human couriers under adverse weather conditions (1.63%). These patterns are robust to alternative specifications and controls. These results also underscore that delivery choices are shaped not only by functional considerations but also by psychological concerns, highlighting the need for context-aware service design that aligns strategies with consumer perceptions.
[12]
arXiv:2509.11660
[pdf, html, other]
Title:
Partially rational preferences under ambiguity
Kensei Nakamura, Shohei Yanagita
Subjects:
Theoretical Economics (econ.TH)
Completeness and transitivity are standard rationality conditions in economics. However, under ambiguity, decision makers sometimes violate these requirements because of the difficulty of forming accurate predictions about ambiguous events. Motivated by this, we study various ambiguity preferences that partially satisfy completeness and transitivity. Our characterization results show that completeness and a novel yet natural weakening of transitivity correspond to two opposite ways of using multiple probability distributions in mind; that is, these two axioms have dual implications at the level of cognitive processes for ambiguity.
[13]
arXiv:2509.11673
[pdf, html, other]
Title:
Grabbing the Forbidden Fruit: Restriction-Sensitive Choice
Niels Boissonnet, Alexis Ghersengorin
Subjects:
Theoretical Economics (econ.TH)
Restricting individuals' access to some opportunities may steer their desire toward their substitutes, a phenomenon known as the forbidden fruit effect. We axiomatize a choice model named restriction-sensitive choice (RSC), which rationalizes the forbidden fruit effect and is compatible with the prominent psychological explanations: reactance theory and commodity theory. The model is identifiable from choice data, specifically from the observation of choice reversals caused by the removal of options. We conduct a normative analysis both in terms of the agent's freedom and welfare. We apply our model to shed light on two phenomena: the backfire effect of beliefs and the backlash of integration policies targeted towards minorities.
[14]
arXiv:2509.12084
[pdf, other]
Title:
Geopolitical Barriers to Globalization
Tianyu Fan, Mai Wo, Wei Xiang
Subjects:
General Economics (econ.GN)
This paper systematically estimates and quantifies how geopolitical alignment shapes global trade across three distinct eras: the Cold War, hyper-globalization, and contemporary fragmentation. We construct a novel measure of bilateral alignment using large language models to compile and analyze 833,485 political events spanning 193 countries from 1950 to 2024. Our analysis reveals that trade flows systematically track geopolitical alignment in both bilateral relationships and aggregate patterns. Using local projections within a gravity framework, we estimate that a one-standard-deviation improvement in geopolitical alignment increases bilateral trade by 20 percent over ten years. Integrating these elasticities into a quantitative general equilibrium model, we find that deteriorating geopolitical relations have reduced global trade by 7 percentage points between 1995 and 2020. Our findings provide empirical benchmarks for evaluating the costs of geopolitical fragmentation in an era of renewed great power competition.
[15]
arXiv:2509.12119
[pdf, html, other]
Title:
Fairness-Aware and Interpretable Policy Learning
Nora Bearth, Michael Lechner, Jana Mareckova, Fabian Muny
Subjects:
Econometrics (econ.EM); Applications (stat.AP)
Fairness and interpretability play an important role in the adoption of decision-making algorithms across many application domains. These requirements are intended to avoid undesirable group differences and to alleviate concerns related to transparency. This paper proposes a framework that integrates fairness and interpretability into algorithmic decision making by combining data transformation with policy trees, a class of interpretable policy functions. The approach is based on pre-processing the data to remove dependencies between sensitive attributes and decision-relevant features, followed by a tree-based optimization to obtain the policy. Since data pre-processing compromises interpretability, an additional transformation maps the parameters of the resulting tree back to the original feature space. This procedure enhances fairness by yielding policy allocations that are pairwise independent of sensitive attributes, without sacrificing interpretability. Using administrative data from Switzerland to analyze the allocation of unemployed individuals to active labor market programs (ALMP), the framework is shown to perform well in a realistic policy setting. Effects of integrating fairness and interpretability constraints are measured through the change in expected employment outcomes. The results indicate that, for this particular application, fairness can be substantially improved at relatively low cost.
[16]
arXiv:2509.12195
[pdf, html, other]
Title:
Optimal Savings with Preference for Wealth
Qingyin Ma, Alexis Akira Toda
Subjects:
Theoretical Economics (econ.TH); Optimization and Control (math.OC)
The consumption function maps current wealth and the exogenous state to current consumption. We prove the existence and uniqueness of a consumption function when the agent has a preference for wealth. When the period utility functions are restricted to power functions, we prove that the consumption function is asymptotically linear as wealth tends to infinity and provide a complete characterization of the asymptotic slopes. When the risk aversion with respect to wealth is less than that for consumption, the asymptotic slope is zero regardless of other model parameters, implying wealthy households save a large fraction of their income, consistent with empirical evidence.
Cross submissions (showing 4 of 4 entries)
[17]
arXiv:2509.10465
(cross-list from math.OC)
[pdf, html, other]
Title:
Bilevel subsidy-enabled mobility hub network design with perturbed utility coalitional choice-based assignment
Hai Yang, Joseph Y. J. Chow
Subjects:
Optimization and Control (math.OC); Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT); General Economics (econ.GN)
Urban mobility is undergoing rapid transformation with the emergence of new services. Mobility hubs (MHs) have been proposed as physical-digital convergence points, offering a range of public and private mobility options in close proximity. By supporting Mobility-as-a-Service, these hubs can serve as focal points where travel decisions intersect with operator strategies. We develop a bilevel MH platform design model that treats MHs as control levers. The upper level (platform) maximizes revenue or flow by setting subsidies to incentivize last-mile operators; the lower level captures joint traveler-operator decisions with a link-based Perturbed Utility Route Choice (PURC) assignment, yielding a strictly convex quadratic program. We reformulate the bilevel problem to a single-level program via the KKT conditions of the lower level and solve it with a gap-penalty method and an iterative warm-start scheme that exploits the computationally cheap lower-level problem. Numerical experiments on a toy network and a Long Island Rail Road (LIRR) case (244 nodes, 469 links, 78 ODs) show that the method attains sub-1% optimality gaps in minutes. In the base LIRR case, the model allows policymakers to quantify the social surplus value of a MH, or the value of enabling subsidy or regulating the microtransit operator's pricing. Comparing link-based subsidies to hub-based subsidies, the latter is computationally more expensive but offers an easier mechanism for comparison and control.
[18]
arXiv:2509.10977
(cross-list from cs.MA)
[pdf, html, other]
Title:
Statistical Model Checking of NetLogo Models
Marco Pangallo, Daniele Giachini, Andrea Vandin
Subjects:
Multiagent Systems (cs.MA); General Economics (econ.GN)
Agent-based models (ABMs) are gaining increasing traction in several domains, due to their ability to represent complex systems that are not easily expressible with classical mathematical models. This expressivity and richness come at a cost: ABMs can typically be analyzed only through simulation, making their analysis challenging. Specifically, when studying the output of ABMs, the analyst is often confronted with practical questions such as: (i) how many independent replications should be run? (ii) how many initial time steps should be discarded as a warm-up? (iii) after the warm-up, how long should the model run? (iv) what are the right parameter values? Analysts usually resort to rules of thumb and experimentation, which lack statistical rigor. This is mainly because addressing these points takes time, and analysts prefer to spend their limited time improving the model. In this paper, we propose a methodology, drawing on the field of Statistical Model Checking, to automate the process and provide guarantees of statistical rigor for ABMs written in NetLogo, one of the most popular ABM platforms. We discuss MultiVeStA, a tool that dramatically reduces the time and human intervention needed to run statistically rigorous checks on ABM outputs, and introduce its integration with NetLogo. Using two ABMs from the NetLogo library, we showcase MultiVeStA's analysis capabilities for NetLogo ABMs, as well as a novel application to statistically rigorous calibration. Our tool-chain makes it immediate to perform statistical checks with NetLogo models, promoting more rigorous and reliable analyses of ABM outputs.
[19]
arXiv:2509.11089
(cross-list from stat.AP)
[pdf, other]
Title:
What is in a Price? Estimating Willingness-to-Pay with Bayesian Hierarchical Models
Srijesh Pillai, Rajesh Kumar Chandrawat
Comments:
7 pages, 6 figures, 1 table. Accepted for publication in the proceedings of the 2025 Advances in Science and Engineering Technology International Conferences (ASET)
Subjects:
Applications (stat.AP); Machine Learning (cs.LG); Econometrics (econ.EM); Machine Learning (stat.ML)
For premium consumer products, pricing strategy is not about a single number, but about understanding the perceived monetary value of the features that justify a higher cost. This paper proposes a robust methodology to deconstruct a product's price into the tangible value of its constituent parts. We employ Bayesian Hierarchical Conjoint Analysis, a sophisticated statistical technique, to solve this high-stakes business problem using the Apple iPhone as a universally recognizable case study. We first simulate a realistic choice based conjoint survey where consumers choose between different hypothetical iPhone configurations. We then develop a Bayesian Hierarchical Logit Model to infer consumer preferences from this choice data. The core innovation of our model is its ability to directly estimate the Willingness-to-Pay (WTP) in dollars for specific feature upgrades, such as a "Pro" camera system or increased storage. Our results demonstrate that the model successfully recovers the true, underlying feature valuations from noisy data, providing not just a point estimate but a full posterior probability distribution for the dollar value of each feature. This work provides a powerful, practical framework for data-driven product design and pricing strategy, enabling businesses to make more intelligent decisions about which features to build and how to price them.
[20]
arXiv:2509.11381
(cross-list from math.ST)
[pdf, html, other]
Title:
The Honest Truth About Causal Trees: Accuracy Limits for Heterogeneous Treatment Effect Estimation
Matias D. Cattaneo, Jason M. Klusowski, Ruiqi Rae Yu
Subjects:
Statistics Theory (math.ST); Econometrics (econ.EM); Methodology (stat.ME); Machine Learning (stat.ML)
Recursive decision trees have emerged as a leading methodology for heterogeneous causal treatment effect estimation and inference in experimental and observational settings. These procedures are fitted using the celebrated CART (Classification And Regression Tree) algorithm [Breiman et al., 1984], or custom variants thereof, and hence are believed to be "adaptive" to high-dimensional data, sparsity, or other specific features of the underlying data generating process. Athey and Imbens [2016] proposed several "honest" causal decision tree estimators, which have become the standard in both academia and industry. We study their estimators, and variants thereof, and establish lower bounds on their estimation error. We demonstrate that these popular heterogeneous treatment effect estimators cannot achieve a polynomial-in-$n$ convergence rate under basic conditions, where $n$ denotes the sample size. Contrary to common belief, honesty does not resolve these limitations and at best delivers negligible logarithmic improvements in sample size or dimension. As a result, these commonly used estimators can exhibit poor performance in practice, and even be inconsistent in some settings. Our theoretical insights are empirically validated through simulations.
Replacement submissions (showing 18 of 18 entries)
[21]
arXiv:2305.09352
(replaced)
[pdf, other]
Title:
Health Impacts of Public Pawnshops in Industrializing Tokyo
Tatsuki Inoue
Comments:
63 pages, 7 tables, 3 figures, Appendicies
Subjects:
General Economics (econ.GN)
This study is the first to investigate whether pawnshops, financial institutions for low-income populations, have contributed to the decline in mortality in the early twentieth century. Using ward-level panel data from Tokyo City, this study revealed that the popularity of public pawnshops was associated with a 4% and 5% decrease in infant mortality and fetal death rates, respectively, during 1927-1935. The historical context implies that the potential channels of the relationships were improving nutrition and hygiene and covering childbirth costs. Moreover, a cost-effectiveness calculation highlighted that the establishment of public pawnshops was a cost-effective public investment for better public health. Contrarily, for-profit private pawnshops showed no significant association with health improvements.
[22]
arXiv:2401.04200
(replaced)
[pdf, html, other]
Title:
Teacher bias or measurement error?
Thomas van Huizen, Madelon Jacobs, Matthijs Oosterveen
Subjects:
Econometrics (econ.EM)
Subjective teacher evaluations play a key role in shaping students' educational trajectories. Previous studies have shown that students of low socioeconomic status (SES) receive worse subjective evaluations than their high SES peers, even when they score similarly on objective standardized tests. This is often interpreted as evidence of teacher bias. Measurement error in test scores challenges this interpretation. We discuss how both classical and non-classical measurement error in test scores generate a biased coefficient of the conditional SES gap, and consider three empirical strategies to address this bias. Using administrative data from the Netherlands, where secondary school track recommendations are pivotal teacher judgments, we find that measurement error explains 35 to 43% of the conditional SES gap in track recommendations.
[23]
arXiv:2404.06471
(replaced)
[pdf, other]
Title:
Regression Discontinuity Design with Spillovers
Eric Auerbach, Yong Cai, Ahnaf Rafi
Subjects:
Econometrics (econ.EM)
This paper studies regression discontinuity designs (RDD) when linear-in-means spillovers occur between units that are close in their running variable. We show that the RDD estimand depends on the ratio of two terms: (1) the radius over which spillovers occur and (2) the choice of bandwidth used for the local linear regression. RDD estimates direct treatment effect when radius is of larger order than the bandwidth and total treatment effect when radius is of smaller order than the bandwidth. When the two are of similar order, the RDD estimand need not have a causal interpretation. To recover direct and spillover effects in the intermediate regime, we propose to incorporate estimated spillover terms into local linear regression. Our estimator is consistent and asymptotically normal and we provide bias-aware confidence intervals for direct treatment effects and spillovers. In the setting of Gonzalez (2021), we detect endogenous spillovers in voter fraud during the 2009 Afghan Presidential election. We also clarify when the donut-hole design addresses spillovers in RDD.
[24]
arXiv:2404.08105
(replaced)
[pdf, html, other]
Title:
Uniform Inference in High-Dimensional Threshold Regression Models
Jiatong Li, Hongqiang Yan
Subjects:
Econometrics (econ.EM)
We develop a uniform inference theory for high-dimensional slope parameters in threshold regression models, allowing for either cross-sectional or time series data. We first establish oracle inequalities for prediction errors, and L1 estimation errors for the Lasso estimator of the slope parameters and the threshold parameter, accommodating heteroskedastic non-subgaussian error terms and non-subgaussian covariates. Next, we derive the asymptotic distribution of tests involving an increasing number of slope parameters by debiasing (or desparsifying) the Lasso estimator in cases with no threshold effect and with a fixed threshold effect. We show that the asymptotic distributions in both cases are the same, allowing us to perform uniform inference without specifying whether the model is a linear or threshold regression. Additionally, we extend the theory to accommodate time series data under the near-epoch dependence assumption. Finally, we identify statistically significant factors influencing cross-country economic growth and quantify the effects of military news shocks on US government spending and GDP, while also estimating a data-driven threshold point in both applications.
[25]
arXiv:2404.19707
(replaced)
[pdf, html, other]
Title:
Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models
Savi Virolainen
Subjects:
Econometrics (econ.EM); Statistics Theory (math.ST); Methodology (stat.ME)
We show that structural smooth transition vector autoregressive models are statistically identified if the shocks are mutually independent and at most one of them is Gaussian. This extends a known identification result for linear structural vector autoregressions to a time-varying impact matrix. We also propose an estimation method, show how a blended identification strategy can be adopted to address weak identification, and establish a sufficient condition for ergodic stationarity. The introduced methods are implemented in the accompanying R package sstvars. Our empirical application finds that a positive climate policy uncertainty shock reduces production and raises inflation under both low and high economic policy uncertainty, but its effects, particularly on inflation, are stronger during the latter.
[26]
arXiv:2410.07443
(replaced)
[pdf, html, other]
Title:
On the Lower Confidence Band for the Optimal Welfare in Policy Learning
Kirill Ponomarev, Vira Semenova
Subjects:
Econometrics (econ.EM); Statistics Theory (math.ST)
We study inference on the optimal welfare in a policy learning problem and propose reporting a lower confidence band (LCB). A natural approach to constructing an LCB is to invert a one-sided t-test based on an efficient estimator for the optimal welfare. However, we show that for an empirically relevant class of DGPs, such an LCB can be first-order dominated by an LCB based on a welfare estimate for a suitable suboptimal treatment policy. We show that such first-order dominance is possible if and only if the optimal treatment policy is not ``well-separated'' from the rest, in the sense of the commonly imposed margin condition. When this condition fails, standard debiased inference methods are not applicable. We show that uniformly valid and easy-to-compute LCBs can be constructed analytically by inverting moment-inequality tests with the maximum and quasi-likelihood-ratio test statistics. As an empirical illustration, we revisit the National JTPA study and find that the proposed LCBs achieve reliable coverage and competitive length.
[27]
arXiv:2411.17883
(replaced)
[pdf, html, other]
Title:
Independence and indifferent points imply continuity
Gerrit Bauch
Subjects:
Theoretical Economics (econ.TH)
I propose the new axiom of Indifferent Points (IP) that can replace continuity axioms in classical expected utility representations under the Independence Axiom over a finite set of prices. IP asserts the existence of a set of indifferent points that span a hyperplane. In the case of three prices, often used for illustrations, the decision maker only needs to show indifference between two distinct lotteries. IP does not imply any of the established continuity axioms and is even strictly weaker than mixture continuity and solvability.
[28]
arXiv:2501.02383
(replaced)
[pdf, html, other]
Title:
Strategic Investment to Mitigate Transition Risks
Jiayue Zhang, Tony S. Wirjanto, Lysa Porth, Ken Seng Tan
Subjects:
General Economics (econ.GN)
This paper investigates strategic investments needed to mitigate transition risks, particularly focusing on sectors significantly impacted by the shift to a low-carbon economy. It emphasizes the importance of tailored sector-specific strategies and the role of government interventions, such as carbon taxes and subsidies, in shaping corporate behavior. In providing a multi-period framework, this paper evaluates the economic and operational trade-offs companies face under four various decarbonization scenarios: immediate, quick, slow, and no transitions. The analysis provides practical insights for both policymakers and business leaders, demonstrating how regulatory frameworks and strategic investments can be aligned to manage transition risks while optimizing long-term sustainability effectively. The findings contribute to a deeper understanding of the economic impacts of regulatory policies and offer a comprehensive framework to navigate the complexities of transitioning to a low-carbon economy.
[29]
arXiv:2503.06646
(replaced)
[pdf, other]
Title:
Evaluating and Aligning Human Economic Risk Preferences in LLMs
Jiaxin Liu, Yixuan Tang, Yi Yang, Kar Yan Tam
Subjects:
General Economics (econ.GN); Computation and Language (cs.CL)
Large Language Models (LLMs) are increasingly used in decision-making scenarios that involve risk assessment, yet their alignment with human economic rationality remains unclear. In this study, we investigate whether LLMs exhibit risk preferences consistent with human expectations across different personas. Specifically, we assess whether LLM-generated responses reflect appropriate levels of risk aversion or risk-seeking behavior based on individual's persona. Our results reveal that while LLMs make reasonable decisions in simplified, personalized risk contexts, their performance declines in more complex economic decision-making tasks. To address this, we propose an alignment method designed to enhance LLM adherence to persona-specific risk preferences. Our approach improves the economic rationality of LLMs in risk-related applications, offering a step toward more human-aligned AI decision-making.
[30]
arXiv:2504.03228
(replaced)
[pdf, html, other]
Title:
Weak instrumental variables due to nonlinearities in panel data: A Super Learner Control Function estimator
Monika Avila Marquez
Subjects:
Econometrics (econ.EM); Machine Learning (stat.ML)
A triangular structural panel data model with additive separable individual-specific effects is used to model the causal effect of a covariate on an outcome variable when there are unobservable confounders with some of them time-invariant. In this setup, a linear reduced-form equation might be problematic when the conditional mean of the endogenous covariate and the instrumental variables is nonlinear. The reason is that ignoring the nonlinearity could lead to weak instruments As a solution, we propose a triangular simultaneous equation model for panel data with additive separable individual-specific fixed effects composed of a linear structural equation with a nonlinear reduced form equation. The parameter of interest is the structural parameter of the endogenous variable. The identification of this parameter is obtained under the assumption of available exclusion restrictions and using a control function approach. Estimating the parameter of interest is done using an estimator that we call Super Learner Control Function estimator (SLCFE). The estimation procedure is composed of two main steps and sample splitting. We estimate the control function using a super learner using sample splitting. In the following step, we use the estimated control function to control for endogeneity in the structural equation. Sample splitting is done across the individual dimension. We perform a Monte Carlo simulation to test the performance of the estimators proposed. We conclude that the Super Learner Control Function Estimators significantly outperform Within 2SLS estimators.
[31]
arXiv:2506.12979
(replaced)
[pdf, other]
Title:
Who and How? Adverse Selection and flexible Moral Hazard
Henrique Castro-Pires, Deniz Kattwinkel, Jan Knoepfle
Subjects:
Theoretical Economics (econ.TH)
We characterize incentive compatible mechanisms in environments with hidden types and flexible hidden actions. Our approach introduces extended recommendation schedules that specify prescribed actions also off-path, after misreports. This approach yields a tractable and complete characterization of incentive compatibility, which includes a generalized integral monotonicity condition capturing the interaction between adverse selection and moral hazard. We demonstrate the usefulness of the characterization across a range of contracting problems.
[32]
arXiv:2507.16078
(replaced)
[pdf, other]
Title:
Automation, AI, and the Intergenerational Transmission of Knowledge
Enrique Ide
Subjects:
General Economics (econ.GN)
Recent advances in Artificial Intelligence (AI) have sparked expectations of unprecedented economic growth. Yet, by enabling senior workers to accomplish more tasks independently, AI may inadvertently reduce entry-level opportunities, raising concerns about how future generations will acquire essential expertise. This paper develops a model to examine how advanced automation affects the intergenerational transmission of tacit knowledge -- practical insights that resist codification and are critical for workplace success. The analysis shows that the competitive equilibrium features socially excessive automation of early-career tasks and reveals a critical trade-off: while such automation delivers immediate productivity gains, it can undermine long-term growth by hindering younger workers' acquisition of tacit skills. Back-of-the-envelope calculations suggest AI-driven entry-level automation could lower the long-run annual growth rate of U.S. per capita output by 0.05 to 0.35 percentage points, depending on its scale. The analysis further shows that AI co-pilots -- systems providing access to tacit-like expertise once obtained only through direct experience -- can partially offset these losses by assisting individuals who fail to develop adequate skills early in their careers. However, co-pilots are not always beneficial, as they may also weaken junior workers' incentives to engage in hands-on learning. These findings challenge the view that AI will automatically lead to higher economic growth, highlighting the need to safeguard -- or deliberately create -- entry-level opportunities to fully realize AI's potential.
[33]
arXiv:2508.17095
(replaced)
[pdf, html, other]
Title:
Axiomatizations of a simple Condorcet voting method for Final Four and Final Five elections
Wesley H. Holliday
Comments:
28 pages, 4 figures, 2 tables. Added Footnote 5 on tie-breaking
Subjects:
Theoretical Economics (econ.TH); Computer Science and Game Theory (cs.GT)
Proponents of Condorcet voting face the question of what to do in the rare case when no Condorcet winner exists. Recent work provides compelling arguments for the rule that should be applied in three-candidate elections, but already with four candidates, many rules appear reasonable. In this paper, we consider a recent proposal of a simple Condorcet voting method for Final Four political elections. Our question is what normative principles could support this simple form of Condorcet voting. When there is no Condorcet winner, one natural principle is to pick the candidate who is closest to being a Condorcet winner. Yet there are multiple plausible ways to define closeness, leading to different results. Here we take the following approach: identify a relatively uncontroversial sufficient condition for one candidate to be closer than another to being a Condorcet winner; then use other principles to help settle who wins in cases when that condition alone does not. We prove that our principles uniquely characterize the simple Condorcet voting method for Final Four elections. This analysis also points to a new way of extending the method to elections with five or more candidates that is simpler than an extension previously considered. The new proposal is to elect the candidate with the most head-to-head wins, and if multiple candidates tie for the most wins, then elect the one who has the smallest head-to-head loss. We provide additional principles sufficient to characterize this simple method for Final Five elections.
[34]
arXiv:2509.04529
(replaced)
[pdf, other]
Title:
Analysis and Study of Smart Growth
Rongyan Chen, Ci Chen, Ziyang Yan
Comments:
Paper needs major revision for content and results
Subjects:
General Economics (econ.GN)
In the mid-1990s, the concept of smart growth emerged in the United States as a critical response to the phenomenon of suburban sprawl. To promote sustainable urban development, it is necessary to further investigate the principles and applications of smart growth. In this paper, we propose a Smart Growth Index (SGI) as a standard for measuring the degree of responsible urban development. Based on this index, we construct a comprehensive 3E evaluation model (covering economic prosperity, social equity, and environmental sustainability) to systematically assess the level of smart growth. For empirical analysis, we selected two medium-sized cities from different continents: Wuhu County, China, and Colima, Mexico. Using an improved entropy method, we evaluated the degree of smart growth in recent years and analyzed the contributions of various policies to sustainable urban development. Guided by the ten principles of smart growth, we further linked theoretical insights to practical challenges and formulated a development plan for both cities. To forecast long-term trends, we employed trend extrapolation based on historical data, enabling the prediction of SGI values for 2020, 2030, and 2050. The results indicate that Wuhu demonstrates greater potential for smart growth compared with Colima. We also simulated a scenario in which the population of both cities increased by 50 percent and re-evaluated the SGI. The analysis suggests that while rapid population growth tends to slow the pace of smart growth, it does not necessarily exert a negative impact on the overall trajectory of sustainable development. Finally, we conducted a study on the application of Transit-Oriented Development (TOD) theory in Wuhu County and proposed several policy recommendations aimed at enhancing the city's sustainable urban development.
[35]
arXiv:2509.07145
(replaced)
[pdf, html, other]
Title:
Efficient Defection: Overage-Proportional Rationing Attains the Cooperative Frontier
Florian Lengyel
Comments:
10 pages, remark on uniqueness at the boundary in Appendix A, expanded proofs in Appendices B and C, numeric bibliography
Subjects:
Theoretical Economics (econ.TH); Optimization and Control (math.OC)
We study a noncooperative $n$-player game of slack allocation in which each player $j$ has entitlement $L_j>0$ and chooses a claim $C_j\ge0$. Let $v_j=(C_j-L_j)_+$ (overage) and $s_j=(L_j-C_j)_+$ (slack); set $X=\sum_j v_j$ and $I=\sum_j s_j$. At the end of the period an overage-proportional clearing rule allocates cooperative surplus $I$ to defectors in proportion to $v_j$; cooperators receive $C_j$. We show: (i) the selfish outcome reproduces the cooperative payoff vector $(L_1,\dots,L_n)$; (ii) with bounded actions, defection is a weakly dominant strategy; (iii) within the $\alpha$-power family, the linear rule ($\alpha=1$) is the unique boundary-continuous member; and (iv) the dominant-strategy outcome is Strong Nash under transferable utility and hence coalition-proof (Bernheim et al., 1987). We give a policy interpretation for carbon rationing with a penalty collar.
[36]
arXiv:2402.09321
(replaced)
[pdf, html, other]
Title:
Collusion-Resilience in Transaction Fee Mechanism Design
Hao Chung, Tim Roughgarden, Elaine Shi
Subjects:
Computer Science and Game Theory (cs.GT); Theoretical Economics (econ.TH)
Users bid in a transaction fee mechanism (TFM) to get their transactions included and confirmed by a blockchain protocol. Roughgarden (EC'21) initiated the formal treatment of TFMs and proposed three requirements: user incentive compatibility (UIC), miner incentive compatibility (MIC), and a form of collusion-resilience called OCA-proofness. Ethereum's EIP-1559 mechanism satisfies all three properties simultaneously when there is no contention between transactions, but loses the UIC property when there are too many eligible transactions to fit in a single block. Chung and Shi (SODA'23) considered an alternative notion of collusion-resilience, called $c$-side-contract-proofness ($c$-SCP), and showed that, when there is contention between transactions, no TFM can satisfy UIC, MIC, and $c$-SCP for any $c\geq 1$. OCA-proofness asserts that the users and a miner should not be able to "steal from the protocol." On the other hand, the $c$-SCP condition requires that a coalition of a miner and a subset of users should not be able to profit through strategic deviations (whether at the expense of the protocol or of the users outside the coalition).
Our main result is the first proof that, when there is contention between transactions, no (possibly randomized) TFM in which users are expected to bid truthfully satisfies UIC, MIC, and this http URL result resolves the main open question in Roughgarden (EC'21). We also suggest several relaxations of the basic model that allow our impossibility result to be circumvented.
[37]
arXiv:2412.02065
(replaced)
[pdf, other]
Title:
Leveraging Large Language Models to Democratize Access to Costly Datasets for Academic Research
Julian Junyan Wang, Victor Xiaoqi Wang
Comments:
58 pagegs, 5 figures, 5 tables
Subjects:
General Finance (q-fin.GN); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); General Economics (econ.GN)
Unequal access to costly datasets essential for empirical research has long hindered researchers from disadvantaged institutions, limiting their ability to contribute to their fields and advance their careers. Recent breakthroughs in Large Language Models (LLMs) have the potential to democratize data access by automating data collection from unstructured sources. We develop and evaluate a novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation (RAG) framework to collect data from corporate disclosures. Our approach achieves human-level accuracy in collecting CEO pay ratios from approximately 10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000 10-K filings, with LLM processing times of 9 and 40 minutes respectively, each at a cost under US $10. This stands in stark contrast to the hundreds of hours needed for manual collection or the thousands of dollars required for commercial database subscriptions. To foster a more inclusive research community by empowering researchers with limited resources to explore new avenues of inquiry, we share our methodology and the resulting datasets.
[38]
arXiv:2508.10663
(replaced)
[pdf, html, other]
Title:
Higher-order Gini indices: An axiomatic approach
Xia Han, Ruodu Wang, Qinyu Wu
Subjects:
Mathematical Finance (q-fin.MF); Econometrics (econ.EM); Statistics Theory (math.ST)
Via an axiomatic approach, we characterize the family of n-th order Gini deviation, defined as the expected range over n independent draws from a distribution, to quantify joint dispersion across multiple observations. This family extends the classical Gini deviation, which relies solely on pairwise comparisons. The normalized version is called a high-order Gini coefficient. The generalized indices grow increasingly sensitive to tail inequality as n increases, offering a more nuanced view of distributional extremes. The higher-order Gini deviations admit a Choquet integral representation, inheriting the desirable properties of coherent deviation measures. Furthermore, we show that both the n-th order Gini deviation and the n-th order Gini coefficient are statistically n-observation elicitable, allowing for direct computation through empirical risk minimization. Data analysis using World Inequality Database data reveals that higher-order Gini coefficients capture disparities that the classical Gini coefficient may fail to reflect, particularly in cases of extreme income or wealth concentration.
Total of 38 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack