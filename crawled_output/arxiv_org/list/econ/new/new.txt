Economics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
econ
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Economics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Friday, 19 September 2025
Total of 16 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 8 of 8 entries)
[1]
arXiv:2509.14396
[pdf, html, other]
Title:
Friend or Foe: Delegating to an AI Whose Alignment is Unknown
Drew Fudenberg, Annie Liang
Subjects:
Theoretical Economics (econ.TH); Computer Science and Game Theory (cs.GT)
AI systems have the potential to improve decision-making, but decision makers face the risk that the AI may be misaligned with their objectives. We study this problem in the context of a treatment decision, where a designer decides which patient attributes to reveal to an AI before receiving a prediction of the patient's need for treatment. Providing the AI with more information increases the benefits of an aligned AI but also amplifies the harm from a misaligned one. We characterize how the designer should select attributes to balance these competing forces, depending on their beliefs about the AI's reliability. We show that the designer should optimally disclose attributes that identify \emph{rare} segments of the population in which the need for treatment is high, and pool the remaining patients.
[2]
arXiv:2509.14422
[pdf, other]
Title:
Aggregating Epigenetic Clocks to Study Human Capital Formation
Giorgia Menta, Pietro Biroli, Divya Mehta, Conchita D'Ambrosio, Deborah Cobb-Clark
Subjects:
General Economics (econ.GN)
Epigenetics is the study of how people's behavior and environments influence the way their genes are expressed, even though their DNA sequence is itself unchanged. By aggregating age-related epigenetic markers, epigenetic 'clocks' have become the leading tool for studying biological aging. We make an important contribution by developing a novel, integrated measure of epigenetic aging--the Multi EpiGenetic Age (MEGA) clock--which combines several existing epigenetic clocks to reduce measurement error and improve estimation efficiency. We use the MEGA clock in three empirical contexts to show that: i) accelerated epigenetic aging in adolescence is associated with worse educational, mental-health, and labor market outcomes in early adulthood; ii) exposure to child maltreatment before adolescence is associated with half a year higher epigenetic aging; and iii) that entering school one year later accelerates epigenetic aging by age seven, particularly among disadvantaged children. The MEGA clock is robust to alternative methods for constructing it, providing a flexible and interpretable approach for incorporating epigenetic data into a wide variety of settings.
[3]
arXiv:2509.14645
[pdf, html, other]
Title:
Are Final Market Prices Sufficient for Information Aggregation? Evidence from Last-Minute Dynamics in Parimutuel Betting
Hiroaki Hanyu, Shunsuke Ishii, Suguru Otani, Kazuhiro Teramoto
Comments:
18 pages with 8 pages appendix
Subjects:
General Economics (econ.GN)
Most betting market models employ static frameworks that condition decisions on final odds. Using a unique dataset of interim odds from Japanese horse racing, this study examines the validity of such static analyses by asking whether there is a systematic relationship between expected returns and the trajectory of odds. We find that returns are negatively related to last-minute changes in odds, and that these late movements attenuate the favorite-longshot bias by weakening the correlation between final odds and returns. These patterns suggest that informed bettors place wagers at the final stage based on private signals, leaving surprises in final odds.
[4]
arXiv:2509.14732
[pdf, html, other]
Title:
Outside options and risk attitude
Gregorio Curello, Ludvig Sinander, Mark Whitmeyer
Subjects:
Theoretical Economics (econ.TH)
We uncover a close link between outside options and risk attitude: when a decision-maker gains access to an outside option, her behaviour becomes less risk-averse, and conversely, any observed decrease of risk-aversion can be explained by an outside option having been made available. We characterise the comparative statics of risk-aversion, delineating how effective risk attitude (i.e. actual choice among risky prospects) varies with the outside option and with the decision-maker's 'true' risk attitude. We prove that outside options are special: among transformations of a decision problem, those that amount to adding an outside option are the only ones that always reduce risk-aversion.
[5]
arXiv:2509.14766
[pdf, html, other]
Title:
An Implementation Relaxation Approach to Principal-Agent Problems
Hang Jiang
Subjects:
Theoretical Economics (econ.TH)
The first-order approach (FOA) is the standard tool for solving principal-agent problems, replacing the incentive compatibility (IC) constraint with its first-order condition to obtain a relaxed problem. We show that FOA is not a valid relaxation when the support of the outcome distribution shifts with the agent's effort, as in well-studied additive-noise models. In such cases, the optimal effort may occur at a kink point that the first-order condition cannot capture, causing FOA to miss optimal contracts, including widely adopted bonus schemes. Motivated by this limitation, we introduce the Implementation Relaxation Approach (IRA), which relaxes the set of agent actions and payoffs that feasible contracts can induce, rather than directly relaxing IC. IRA accommodates non-differentiable optima and is straightforward to apply across settings, particularly for deriving optimality conditions for simple contracts. Using IRA, we derive an optimality condition for quota-bonus contracts that is more general, encompassing a broader range of scenarios than FOA-based conditions, including those established in the literature under fixed-support assumptions. This also fills a gap where the optimality of quota-bonus contracts in shifting-support settings has been examined only under endogenous assumptions, and it highlights the broader applicability of IRA as a methodological tool.
[6]
arXiv:2509.14795
[pdf, html, other]
Title:
Paradoxes of the public sector productivity measurement
Timo Kuosmanen, Xun Zhou
Subjects:
General Economics (econ.GN)
This paper critically investigates standard total factor productivity (TFP) measurement in the public sector, where output information is often incomplete or distorted. The analysis reveals fundamental paradoxes under three common output measurement conventions. When cost-based value added is used as the aggregate output, measured TFP may paradoxically decline as a result of genuine productivity-enhancing changes such as technical progress and improved allocative and scale efficiencies, as well as reductions in real input prices. We show that the same problems carry over to the situation where the aggregate output is constructed as the cost-share weighted index of outputs. In the case of distorted output prices, measured TFP may move independently of any productivity changes and instead reflect shifts in pricing mechanisms. Using empirical illustrations from the United Kingdom and Finland, we demonstrate that such distortions are not merely theoretical but are embedded in widely used public productivity statistics. We argue that public sector TFP measurement requires a shift away from cost-based aggregation of outputs and toward non-market valuation methods grounded in economic theory.
[7]
arXiv:2509.15165
[pdf, html, other]
Title:
Invariant Modeling for Joint Distributions
Christopher P. Chambers, Yusufcan Masatlioglu, Ruodu Wang
Subjects:
Theoretical Economics (econ.TH)
A common theme underlying many problems in statistics and economics involves the determination of a systematic method of selecting a joint distribution consistent with a specified list of categorical marginals, some of which have an ordinal structure. We propose guidance in narrowing down the set of possible methods by introducing Invariant Aggregation (IA), a natural property that requires merging adjacent categories in one marginal not to alter the joint distribution over unaffected values. We prove that a model satisfies IA if and only if it is a copula model. This characterization ensures i) robustness against data manipulation and survey design, and ii) allows seamless incorporation of new variables. Our results provide both theoretical clarity and practical safeguards for inference under marginal constraints.
[8]
arXiv:2509.15169
[pdf, html, other]
Title:
Monetary Policy and Exchange Rate Fluctuations
Yongheng Hu
Comments:
23 pages, 10 figures
Subjects:
Econometrics (econ.EM)
In this paper, we model USD-CNY bilateral exchange rate fluctuations as a general stochastic process and incorporate monetary policy shock to examine how bilateral exchange rate fluctuations affect the Revealed Comparative Advantage (RCA) index. Numerical simulations indicate that as the mean of bilateral exchange rate fluctuations increases, i.e., currency devaluation, the RCA index rises. Moreover, smaller bilateral exchange rate fluctuations after the policy shock cause the RCA index to gradually converge toward its mean level. For the empirical analysis, we select the USD-CNY bilateral exchange rate and provincial manufacturing industry export competitiveness data in China from 2008 to 2021. We find that in the short term, when exchange rate fluctuations stabilize within a range less than 0.2 RMB depreciation will effectively boost export competitiveness. Then, the 8.11 exchange rate policy reversed the previous linear trend of the CNY, stabilizing it within a narrow fluctuation range over the long term. This policy leads to a gradual convergence of provincial RCA indices toward a relatively high level, which is commensurate with our numerical simulations, and indirectly enhances provincial export competitiveness.
Cross submissions (showing 4 of 4 entries)
[9]
arXiv:2509.14466
(cross-list from cs.GT)
[pdf, html, other]
Title:
Optimal Algorithms for Bandit Learning in Matching Markets
Tejas Pagare, Agniv Bandyopadhyay, Sandeep Juneja
Subjects:
Computer Science and Game Theory (cs.GT); Theoretical Economics (econ.TH); Systems and Control (eess.SY); Dynamical Systems (math.DS)
We study the problem of pure exploration in matching markets under uncertain preferences, where the goal is to identify a stable matching with confidence parameter $\delta$ and minimal sample complexity. Agents learn preferences via stochastic rewards, with expected values indicating preferences. This finds use in labor market platforms like Upwork, where firms and freelancers must be matched quickly despite noisy observations and no prior knowledge, in a stable manner that prevents dissatisfaction. We consider markets with unique stable matching and establish information-theoretic lower bounds on sample complexity for (1) one-sided learning, where one side of the market knows its true preferences, and (2) two-sided learning, where both sides are uncertain. We propose a computationally efficient algorithm and prove that it asymptotically ($\delta\to 0$) matches the lower bound to a constant for one-sided learning. Using the insights from the lower bound, we extend our algorithm to the two-sided learning setting and provide experimental results showing that it closely matches the lower bound on sample complexity. Finally, using a system of ODEs, we characterize the idealized fluid path that our algorithm chases.
[10]
arXiv:2509.14532
(cross-list from cs.CY)
[pdf, html, other]
Title:
Leveraging Artificial Intelligence as a Strategic Growth Catalyst for Small and Medium-sized Enterprises
Oluwatosin Agbaakin (Indiana University Indianapolis)
Comments:
14 pages, 2 figures. A review and strategic framework for AI adoption in SMEs
Subjects:
Computers and Society (cs.CY); Artificial Intelligence (cs.AI); General Economics (econ.GN)
Artificial Intelligence (AI) has transitioned from a futuristic concept reserved for large corporations to a present-day, accessible, and essential growth lever for Small and Medium-sized Enterprises (SMEs). For entrepreneurs and business leaders, strategic AI adoption is no longer an option but an imperative for competitiveness, operational efficiency, and long-term survival. This report provides a comprehensive framework for SME leaders to navigate this technological shift, offering the foundational knowledge, business case, practical applications, and strategic guidance necessary to harness the power of AI. The quantitative evidence supporting AI adoption is compelling; 91% of SMEs using AI report that it directly boosts their revenue. Beyond top-line growth, AI drives profound operational efficiencies, with studies showing it can reduce operational costs by up to 30% and save businesses more than 20 hours of valuable time each month. This transformation is occurring within the context of a seismic economic shift; the global AI market is projected to surge from $233.46 Billion in 2024 to an astonishing $1.77 Trillion by 2032. This paper demystifies the core concepts of AI, presents a business case based on market data, details practical applications, and lays out a phased, actionable adoption strategy.
[11]
arXiv:2509.14805
(cross-list from stat.AP)
[pdf, html, other]
Title:
Forecasting in small open emerging economies Evidence from Thailand
Paponpat Taveeapiradeecharoen, Nattapol Aunsri
Subjects:
Applications (stat.AP); Econometrics (econ.EM)
Forecasting inflation in small open economies is difficult because limited time series and strong external exposures create an imbalance between few observations and many potential predictors. We study this challenge using Thailand as a representative case, combining more than 450 domestic and international indicators. We evaluate modern Bayesian shrinkage and factor models, including Horseshoe regressions, factor-augmented autoregressions, factor-augmented VARs, dynamic factor models, and Bayesian additive regression trees.
Our results show that factor models dominate at short horizons, when global shocks and exchange rate movements drive inflation, while shrinkage-based regressions perform best at longer horizons. These models not only improve point and density forecasts but also enhance tail-risk performance at the one-year horizon.
Shrinkage diagnostics, on the other hand, additionally reveal that Google Trends variables, especially those related to food essential goods and housing costs, progressively rotate into predictive importance as the horizon lengthens. This underscores their role as forward-looking indicators of household inflation expectations in small open economies.
[12]
arXiv:2509.15090
(cross-list from cs.LG)
[pdf, html, other]
Title:
Emergent Alignment via Competition
Natalie Collina, Surbhi Goel, Aaron Roth, Emily Ryu, Mirah Shi
Subjects:
Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT); Theoretical Economics (econ.TH)
Aligning AI systems with human values remains a fundamental challenge, but does our inability to create perfectly aligned models preclude obtaining the benefits of alignment? We study a strategic setting where a human user interacts with multiple differently misaligned AI agents, none of which are individually well-aligned. Our key insight is that when the users utility lies approximately within the convex hull of the agents utilities, a condition that becomes easier to satisfy as model diversity increases, strategic competition can yield outcomes comparable to interacting with a perfectly aligned model. We model this as a multi-leader Stackelberg game, extending Bayesian persuasion to multi-round conversations between differently informed parties, and prove three results: (1) when perfect alignment would allow the user to learn her Bayes-optimal action, she can also do so in all equilibria under the convex hull condition (2) under weaker assumptions requiring only approximate utility learning, a non-strategic user employing quantal response achieves near-optimal utility in all equilibria and (3) when the user selects the best single AI after an evaluation period, equilibrium guarantees remain near-optimal without further distributional assumptions. We complement the theory with two sets of experiments.
Replacement submissions (showing 4 of 4 entries)
[13]
arXiv:2407.17589
(replaced)
[pdf, html, other]
Title:
Diversity in Choice as Majorization
Federico Echenique, Teddy Mekonnen, M. Bumin Yenmez
Subjects:
Theoretical Economics (econ.TH)
We propose a framework that uses majorization to model diversity and representativeness in school admissions. We generalize the standard notion of majorization to accommodate arbitrary distributional targets, such as a student body that reflects the population served by the school. Building on this framework, we introduce and axiomatically characterize the $r$-targeting Schur choice rule, which balances diversity and priority in admissions. We show that this rule is optimal: any alternative rule must either leave seats unfilled, reduce diversity, or admit lower-priority students. The rule satisfies path independence (and substitutability), which guarantees desirable outcomes in matching markets. Our work contributes to the ongoing discourse on market design by providing a new and flexible framework for improving diversity and representation.
[14]
arXiv:2502.19525
(replaced)
[pdf, html, other]
Title:
Privacy-Aware Sequential Learning
Yuxin Liu, M. Amin Rahimian
Comments:
An extended abstract of this work appeared at the 6th Symposium on Foundations of Responsible Computing (FORC 2025). See this https URL. Code and Data: this https URL
Subjects:
Theoretical Economics (econ.TH)
In settings like vaccination registries, individuals act after observing others, and the resulting public records can expose private information. We study privacy-preserving sequential learning, where agents add endogenous noise to their reported actions to conceal private signals. Efficient social learning relies on information flow, seemingly in conflict with privacy. Surprisingly, with continuous signals and a fixed privacy budget $(\varepsilon)$, the optimal randomization strategy balances privacy and accuracy, accelerating learning to $\Theta_{\varepsilon}(\log n)$, faster than the nonprivate $\Theta(\sqrt{\log n})$ rate. In the nonprivate baseline, the expected time to the first correct action and the number of incorrect actions diverge; under privacy with sufficiently small $\varepsilon$, both are finite. Privacy helps because, under the false state, agents more often receive signals contradicting the majority; randomization then asymmetrically amplifies the log-likelihood ratio, enhancing aggregation. In heterogeneous populations, an order-optimal $\Theta(\sqrt{n})$ rate is achievable when a subset of agents have low privacy budgets. With binary signals, however, privacy reduces informativeness and impairs learning relative to the nonprivate baseline, though the dependence on $\varepsilon$ is nonmonotone. Our results show how privacy reshapes information dynamics and inform the design of platforms and policies.
[15]
arXiv:2504.05678
(replaced)
[pdf, html, other]
Title:
Equity in strategic exchange
Peng Liu, Huaxia Zeng
Subjects:
Theoretical Economics (econ.TH)
New fairness notions aligned with the merit principle are proposed for designing exchange rules. We show that for an obviously strategy-proof, efficient and individually rational rule, (i) an agent receives her favorite object when others unanimously perceive her object the best, if and only if preferences are single-peaked, and (ii) an upper bound on fairness attainable is that, if two agents' objects are considered the best by all agents partitioned evenly into two groups, it is guaranteed that one, not both, gets her favorite object. This indicates an unambiguous trade-off between incentives and fairness in the design of exchange rules.
[16]
arXiv:2509.14057
(replaced)
[pdf, html, other]
Title:
Machines are more productive than humans until they aren't, and vice versa
Riccardo Zanardelli
Comments:
Added more detailed characterization of the simulation setup in the conclusions; added more detailed considerations in the conclusion, no changes in results; minor typos corrected; improved the example in section A.4.1, with corrections to Table 17
Subjects:
General Economics (econ.GN); Artificial Intelligence (cs.AI)
With the growth of artificial skills, organizations may increasingly confront with the problem of optimizing skill policy decisions guided by economic principles. This paper addresses the underlying complexity of this challenge by developing an in-silico framework based on Monte Carlo simulations grounded in empirical realism to analyze the economic impact of human and machine skills, individually or jointly deployed, in the execution of tasks presenting varying levels of complexity. Our results provide quantitative support for the established notions that automation tends to be the most economically-effective strategy for tasks characterized by low-to-medium generalization difficulty, while automation may struggle to match the economic utility of human skills in more complex scenarios. Critically, our simulations highlight that combining human and machine skills can be the most effective strategy when a high level of generalization is required, but only if genuine augmentation is achieved. In contrast, when failing to realize this synergy, the human-machine policy is severely penalized by the inherent costs of its dual skill structure, causing it to destroy value and becoming the worst choice from an economic perspective. The takeaway for decision-makers is unambiguous: in contexts requiring high generalization capabilities, simply allocating human and machine skills to a task is insufficient, and a human-machine skill policy is neither a silver-bullet solution nor a low-risk compromise. Rather, it is a critical opportunity to boost competitiveness that demands a strong organizational commitment to enabling augmentation. Also, our findings show that improving the cost-effectiveness of machine skills over time, while useful, does not replace the fundamental need to focus on achieving augmentation.
Total of 16 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack