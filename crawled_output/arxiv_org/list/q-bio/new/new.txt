Quantitative Biology
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
q-bio
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Quantitative Biology
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Thursday, 18 September 2025
Total of 29 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 10 of 10 entries)
[1]
arXiv:2509.13344
[pdf, html, other]
Title:
Benchmarking Dimensionality Reduction Techniques for Spatial Transcriptomics
Md Ishtyaq Mahmud, Veena Kochat, Suresh Satpati, Jagan Mohan Reddy Dwarampudi, Kunal Rai, Tania Banerjee
Comments:
This paper is accepted to the 16th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB 2025), 10 page and have 4 figures
Subjects:
Genomics (q-bio.GN); Machine Learning (cs.LG)
We introduce a unified framework for evaluating dimensionality reduction techniques in spatial transcriptomics beyond standard PCA approaches. We benchmark six methods PCA, NMF, autoencoder, VAE, and two hybrid embeddings on a cholangiocarcinoma Xenium dataset, systematically varying latent dimensions ($k$=5-40) and clustering resolutions ($\rho$=0.1-1.2). Each configuration is evaluated using complementary metrics including reconstruction error, explained variance, cluster cohesion, and two novel biologically-motivated measures: Cluster Marker Coherence (CMC) and Marker Exclusion Rate (MER). Our results demonstrate distinct performance profiles: PCA provides a fast baseline, NMF maximizes marker enrichment, VAE balances reconstruction and interpretability, while autoencoders occupy a middle ground. We provide systematic hyperparameter selection using Pareto optimal analysis and demonstrate how MER-guided reassignment improves biological fidelity across all methods, with CMC scores improving by up to 12\% on average. This framework enables principled selection of dimensionality reduction methods tailored to specific spatial transcriptomics analyses.
[2]
arXiv:2509.13376
[pdf, other]
Title:
Unleashing the power of computational insights in revealing the complexity of biological systems in the new era of spatial multi-omics
Zhiwei Fan, Tiangang Wang, Kexin Huang, Binwu Ying, Xiaobo Zhou
Comments:
43 pages, 9 figures, 1 table
Subjects:
Quantitative Methods (q-bio.QM); Machine Learning (cs.LG)
Recent advances in spatial omics technologies have revolutionized our ability to study biological systems with unprecedented resolution. By preserving the spatial context of molecular measurements, these methods enable comprehensive mapping of cellular heterogeneity, tissue architecture, and dynamic biological processes in developmental biology, neuroscience, oncology, and evolutionary studies. This review highlights a systematic overview of the continuous advancements in both technology and computational algorithms that are paving the way for a deeper, more systematic comprehension of the structure and mechanisms of mammalian tissues and organs by using spatial multi-omics. Our viewpoint demonstrates how advanced machine learning algorithms and multi-omics integrative modeling can decode complex biological processes, including the spatial organization and topological relationships of cells during organ development, as well as key molecular signatures and regulatory networks underlying tumorigenesis and metastasis. Finally, we outline future directions for technological innovation and modeling insights of spatial omics in precision medicine.
[3]
arXiv:2509.13428
[pdf, other]
Title:
Autonomous Reporting of Normal Chest X-rays by Artificial Intelligence in the United Kingdom. Can We Take the Human Out of the Loop?
Katrina Nash, James Vaz, Ahmed Maiter, Christopher Johns, Nicholas Woznitza, Aditya Kale, Abdala Espinosa Morgado, Rhidian Bramley, Mark Hall, David Lowe, Alex Novak, Sarim Ather
Subjects:
Populations and Evolution (q-bio.PE); Computer Vision and Pattern Recognition (cs.CV)
Chest X-rays (CXRs) are the most commonly performed imaging investigation. In the UK, many centres experience reporting delays due to radiologist workforce shortages. Artificial intelligence (AI) tools capable of distinguishing normal from abnormal CXRs have emerged as a potential solution. If normal CXRs could be safely identified and reported without human input, a substantial portion of radiology workload could be reduced.
This article examines the feasibility and implications of autonomous AI reporting of normal CXRs. Key issues include defining normal, ensuring generalisability across populations, and managing the sensitivity-specificity trade-off. It also addresses legal and regulatory challenges, such as compliance with IR(ME)R and GDPR, and the lack accountability frameworks for errors. Further considerations include the impact on radiologists practice, the need for robust post-market surveillance, and incorporation of patient perspectives. While the benefits are clear, adoption must be cautious.
[4]
arXiv:2509.13459
[pdf, html, other]
Title:
Why all roads don't lead to Rome: Representation geometry varies across the human visual cortical hierarchy
Arna Ghosh, Zahraa Chorghay, Shahab Bakhtiari, Blake A. Richards
Comments:
9 pages, 4 figures
Subjects:
Neurons and Cognition (q-bio.NC); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)
Biological and artificial intelligence systems navigate the fundamental efficiency-robustness tradeoff for optimal encoding, i.e., they must efficiently encode numerous attributes of the input space while also being robust to noise. This challenge is particularly evident in hierarchical processing systems like the human brain. With a view towards understanding how systems navigate the efficiency-robustness tradeoff, we turned to a population geometry framework for analyzing representations in the human visual cortex alongside artificial neural networks (ANNs). In the ventral visual stream, we found general-purpose, scale-free representations characterized by a power law-decaying eigenspectrum in most areas. However, in certain higher-order visual areas did not have scale-free representations, indicating that scale-free geometry is not a universal property of the brain. In parallel, ANNs trained with a self-supervised learning objective also exhibited free-free geometry, but not after fine-tune on a specific task. Based on these empirical results and our analytical insights, we posit that a system's representation geometry is not a universal property and instead depends upon the computational objective.
[5]
arXiv:2509.13476
[pdf, html, other]
Title:
A Geometric Graph-Based Deep Learning Model for Drug-Target Affinity Prediction
Md Masud Rana, Farjana Tasnim Mukta, Duc D. Nguyen
Subjects:
Biomolecules (q-bio.BM); Machine Learning (cs.LG)
In structure-based drug design, accurately estimating the binding affinity between a candidate ligand and its protein receptor is a central challenge. Recent advances in artificial intelligence, particularly deep learning, have demonstrated superior performance over traditional empirical and physics-based methods for this task, enabled by the growing availability of structural and experimental affinity data. In this work, we introduce DeepGGL, a deep convolutional neural network that integrates residual connections and an attention mechanism within a geometric graph learning framework. By leveraging multiscale weighted colored bipartite subgraphs, DeepGGL effectively captures fine-grained atom-level interactions in protein-ligand complexes across multiple scales. We benchmarked DeepGGL against established models on CASF-2013 and CASF-2016, where it achieved state-of-the-art performance with significant improvements across diverse evaluation metrics. To further assess robustness and generalization, we tested the model on the CSAR-NRC-HiQ dataset and the PDBbind v2019 holdout set. DeepGGL consistently maintained high predictive accuracy, highlighting its adaptability and reliability for binding affinity prediction in structure-based drug discovery.
[6]
arXiv:2509.13481
[pdf, other]
Title:
Complex-valued Phase Synchrony Reveals Directional Coupling in FMRI and Tracks Medication Effects
Sir-Lord Wiafe, Najme Soleimani, Masoud Seraji, Bradley Baker, Robyn Miller, Ashkan Faghiri, Vince D. Calhoun
Comments:
5 pages, 3 Figures, conference
Subjects:
Neurons and Cognition (q-bio.NC)
Understanding interactions in complex systems requires capturing the directionality of coupling, not only its strength. Phase synchronization captures this timing, yet most methods either reduce phase to its cosine or collapse it into scaler indices such as phase-locking value, discarding directionality. We propose a complex-valued phase synchrony (CVPS) framework that estimates phase with an adaptive Gabor wavelet and preserves both cosine and sine components. Simulations confirm that CVPS recovers true phase offsets and tracks non-stationary dynamics more faithfully than Hilbert-based methods. Because antipsychotics are known to modulate the timing of cortical interactions, they provide a rigorous context to evaluate whether CVPS can capture such pharmacological effects. CVPS further reveals cortical neuro-hemodynamic drivers, with occipital-to-parietal and prefrontal-to-striatal lead-lag flows consistent with known receptor targets, confirming its ability to capture pharmacological timing. CVPS, therefore, offers a robust and generalizable framework for detecting directional coupling in complex systems such as the brain.
[7]
arXiv:2509.13612
[pdf, html, other]
Title:
Rest2Visual: Predicting Visually Evoked fMRI from Resting-State Scans
Chuyang Zhou, Ziao Ji, Daochang Liu, Dongang Wang, Chenyu Wang, Chang Xu
Subjects:
Neurons and Cognition (q-bio.NC); Computer Vision and Pattern Recognition (cs.CV)
Understanding how spontaneous brain activity relates to stimulus-driven neural responses is a fundamental challenge in cognitive neuroscience. While task-based functional magnetic resonance imaging (fMRI) captures localized stimulus-evoked brain activation, its acquisition is costly, time-consuming, and difficult to scale across populations. In contrast, resting-state fMRI (rs-fMRI) is task-free and abundant, but lacks direct interpretability. We introduce Rest2Visual, a conditional generative model that predicts visually evoked fMRI (ve-fMRI) from resting-state input and 2D visual stimuli. It follows a volumetric encoder--decoder design, where multiscale 3D features from rs-fMRI are modulated by image embeddings via adaptive normalization, enabling spatially accurate, stimulus-specific activation synthesis. To enable model training, we construct a large-scale triplet dataset from the Natural Scenes Dataset (NSD), aligning each rs-fMRI volume with stimulus images and their corresponding ve-fMRI activation maps. Quantitative evaluation shows that the predicted activations closely match ground truth across standard similarity and representational metrics, and support successful image reconstruction in downstream decoding. Notably, the predicted maps preserve subject-specific structure, demonstrating the model's capacity to generate individualized functional surrogates. Our results provide compelling evidence that individualized spontaneous neural activity can be transformed into stimulus-aligned representations, opening new avenues for scalable, task-free functional brain modeling.
[8]
arXiv:2509.13875
[pdf, other]
Title:
Personalized Detection of Stress via hdrEEG: Linking Neuro-markers to Cortisol, HRV, and Self-Report
N. B. Maimon, Ganit Baruchin, Itamar Grotto, Nathan Intrator, Talya Zeimer, Ofir Chibotero, Efrat Danino
Subjects:
Neurons and Cognition (q-bio.NC)
Chronic stress is a major risk factor for cognitive decline, neurodegenerative disease, and systemic health burden, underscoring the need for reliable individual-level biomarkers of stress reactivity. While cortisol, heart rate variability (HRV), and self-report measures are widely used, they provide limited insight into neural mechanisms. Here, we tested whether two single-channel hdrEEG biomarkers, ST4 and T2, serve as personalized indices of stress regulation by linking neural activity to validated physiological and subjective measures. We conducted two studies. Study 1 included 101 healthy adults (22-82 years) who completed questionnaires on resilience, burnout, and perceived stress, provided salivary cortisol, and underwent hdrEEG during resting, detection, n-back, lexical, emotional music, and startle tasks. Study 2 included 82 adults (19-42 years) who completed the State-Trait Anxiety Inventory, were monitored for HRV, and performed auditory, stress-inducing (job interview, arithmetic), and emotional tasks during hdrEEG recording. Across studies, ST4 reflected physiological arousal and cognitive strain, correlating positively with cortisol, subjective stress, and pulse pressure, and negatively with resilience and HRV indices. T2 showed a complementary profile, linking emotional and autonomic processes. T2 activity correlated with cortisol, heart rate, HRV measures, resilience, and trait anxiety, especially during lexical and emotional tasks. Together, ST4 and T2 capture distinct facets of the stress response: physiological arousal versus emotional-regulatory sensitivity. These findings highlight portable hdrEEG as a promising tool for personalized stress assessment, bridging neural, physiological, and subjective domains with implications for clinical and occupational monitoring.
[9]
arXiv:2509.14037
[pdf, html, other]
Title:
PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction
Ranga Baminiwatte, Kazi Jewel Rana, Aaron J. Masino
Subjects:
Genomics (q-bio.GN); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
Understanding disease similarity is critical for advancing diagnostics, drug discovery, and personalized treatment strategies. We present PhenoGnet, a novel graph-based contrastive learning framework designed to predict disease similarity by integrating gene functional interaction networks with the Human Phenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-view model that separately encodes gene and phenotype graphs using Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), and a cross view model implemented as a shared weight multilayer perceptron (MLP) that aligns gene and phenotype embeddings through contrastive learning. The model is trained using known gene phenotype associations as positive pairs and randomly sampled unrelated pairs as negatives. Diseases are represented by the mean embeddings of their associated genes and/or phenotypes, and pairwise similarity is computed via cosine similarity. Evaluation on a curated benchmark of 1,100 similar and 866 dissimilar disease pairs demonstrates strong performance, with gene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764, outperforming existing state of the art methods. Notably, PhenoGnet captures latent biological relationships beyond direct overlap, offering a scalable and interpretable solution for disease similarity prediction. These results underscore its potential for enabling downstream applications in rare disease research and precision medicine.
[10]
arXiv:2509.14166
[pdf, html, other]
Title:
Reaction-diffusion models of invasive tree pest spread: quantifying the spread of oak processionary moth in the UK
Jamie P. McKeown, Laura E. Wadkin, Nick G. Parker, Andrew Golightly, Andrew W. Baggaley
Comments:
19 pages, 8 figures, 2 tables
Subjects:
Populations and Evolution (q-bio.PE); Quantitative Methods (q-bio.QM)
UK woodlands, forests, and urban treescapes are under threat from invasive species, exacerbated by climate change, trade, and transport. Invasive tree pests debilitate their host and disrupt forest ecosystems, thus it is imperative to quantitatively model and predict their spread. Addressing this, we represent the spatial distribution of the pest as a population density field which evolves according to a spatiotemporal reaction-diffusion equation. We solve this intractable system of equations numerically and, from the solution, we determine first arrival times of the pest at locations in the field. The adopted model permits us to obtain the expansion rate of pest spread directly from the model parameters, which we infer in the Bayesian paradigm, using a Markov chain Monte Carlo scheme. We apply our framework to the ongoing spread of oak processionary moth in the UK, an outbreak which continues to grow despite management efforts. We demonstrate that our approach effectively captures the spread of the pest and that this has occurred at a non-constant expansion rate. The proposed framework is a powerful tool for quantitatively modelling the spread of an invasive tree pest and could underpin future prediction and management approaches.
Cross submissions (showing 7 of 7 entries)
[11]
arXiv:2509.13360
(cross-list from eess.IV)
[pdf, html, other]
Title:
PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma
L. Zimmer, J. Weidner, M. Balcerak, F. Kofler, I. Ezhov, B. Menze, B. Wiestler
Subjects:
Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)
Glioblastoma is the most prevalent primary brain malignancy, distinguished by its highly invasive behavior and exceptionally high rates of recurrence. Conventional radiation therapy, which employs uniform treatment margins, fails to account for patient-specific anatomical and biological factors that critically influence tumor cell migration. To address this limitation, numerous computational models of glioblastoma growth have been developed, enabling generation of tumor cell distribution maps extending beyond radiographically visible regions and thus informing more precise treatment strategies. However, despite encouraging preliminary findings, the clinical adoption of these growth models remains limited. To bridge this translational gap and accelerate both model development and clinical validation, we introduce PREDICT-GBM, a comprehensive integrated pipeline and dataset for modeling and evaluation. This platform enables systematic benchmarking of state-of-the-art tumor growth models using an expert-curated clinical dataset comprising 255 subjects with complete tumor segmentations and tissue characterization maps. Our analysis demonstrates that personalized radiation treatment plans derived from tumor growth predictions achieved superior recurrence coverage compared to conventional uniform margin approaches for two of the evaluated models. This work establishes a robust platform for advancing and systematically evaluating cutting-edge tumor growth modeling approaches, with the ultimate goal of facilitating clinical translation and improving patient outcomes.
[12]
arXiv:2509.13372
(cross-list from eess.IV)
[pdf, html, other]
Title:
Generative AI Pipeline for Interactive Prompt-driven 2D-to-3D Vascular Reconstruction for Fontan Geometries from Contrast-Enhanced X-Ray Fluoroscopy Imaging
Prahlad G Menon
Subjects:
Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Emerging Technologies (cs.ET); Quantitative Methods (q-bio.QM)
Fontan palliation for univentricular congenital heart disease progresses to hemodynamic failure with complex flow patterns poorly characterized by conventional 2D imaging. Current assessment relies on fluoroscopic angiography, providing limited 3D geometric information essential for computational fluid dynamics (CFD) analysis and surgical planning.
A multi-step AI pipeline was developed utilizing Google's Gemini 2.5 Flash (2.5B parameters) for systematic, iterative processing of fluoroscopic angiograms through transformer-based neural architecture. The pipeline encompasses medical image preprocessing, vascular segmentation, contrast enhancement, artifact removal, and virtual hemodynamic flow visualization within 2D projections. Final views were processed through Tencent's Hunyuan3D-2mini (384M parameters) for stereolithography file generation.
The pipeline successfully generated geometrically optimized 2D projections from single-view angiograms after 16 processing steps using a custom web interface. Initial iterations contained hallucinated vascular features requiring iterative refinement to achieve anatomically faithful representations. Final projections demonstrated accurate preservation of complex Fontan geometry with enhanced contrast suitable for 3D conversion. AI-generated virtual flow visualization identified stagnation zones in central connections and flow patterns in branch arteries. Complete processing required under 15 minutes with second-level API response times.
This approach demonstrates clinical feasibility of generating CFD-suitable geometries from routine angiographic data, enabling 3D generation and rapid virtual flow visualization for cursory insights prior to full CFD simulation. While requiring refinement cycles for accuracy, this establishes foundation for democratizing advanced geometric and hemodynamic analysis using readily available imaging data.
[13]
arXiv:2509.13556
(cross-list from physics.bio-ph)
[pdf, html, other]
Title:
Collective multicellular patterns arising from cadherin-linked cytoskeletal domains
XinXin Du, Ido Lavi, Michael J. Shelley
Comments:
4 Figures
Subjects:
Biological Physics (physics.bio-ph); Cell Behavior (q-bio.CB)
In multicellular systems, adhesion complexes, such as those composed of E-cadherin and associated catenins, mechanically couple neighboring cells by directly linking their actin-based cytoskeletal assemblies. However, the mechanics of how forces are transmitted across these adhesions remains largely unstudied. Here, we introduce a biophysical model that explicitly couples adhesion complex dynamics to intracellular mechanics across cell boundaries. A cadherin dimer plus associated catenins connecting two cells is represented as a spring whose ends experience drag with respect to the moving actin cytoskeleton. The cytoskeleton is modeled as a contractile gel driven by myosin activity in its bulk and forces from adhesion on its boundaries. Our model captures this bidirectional coupling via a coarse-grained continuum framework and reveals a range of observed cell- and tissue-scale behaviors. These include global cell polarization of the multicellular collective, other polarization patterns and oscillatory dynamics, spontaneously formed actin rings within cells, and supracellular stress chains. Many of these features arise from modeling the direct mechanical coupling between cytoskeleton and adhesion. This model can be extended to other adhesion-cytoskeleton feedback systems and used to advance our understanding of multicellular tissue dynamics, particularly during development.
[14]
arXiv:2509.13867
(cross-list from cond-mat.stat-mech)
[pdf, html, other]
Title:
Plasticity-induced multistability on fast and slow timescales enables optimal information encoding and spontaneous sequence discrimination
Giacomo Barzon, Daniel M. Busiello, Giorgio Nicoletti
Subjects:
Statistical Mechanics (cond-mat.stat-mech); Neurons and Cognition (q-bio.NC)
Neural circuits exhibit remarkable computational flexibility, enabling adaptive responses to noisy and ever-changing environmental cues. A fundamental question in neuroscience concerns how a wide range of behaviors can emerge from a relatively limited set of underlying biological mechanisms. In particular, the interaction between activities of neuronal populations and plasticity modulation of synaptic connections may endow neural circuits with a variety of functional responses when coordinated over different characteristic timescales. Here, we develop an information-theoretic framework to quantitatively explore this idea. We consider a stochastic model for neural activities that incorporates the presence of a coupled dynamic plasticity and time-varying stimuli. We show that long-term plasticity modulations play the functional role of steering neural activities towards a regime of optimal information encoding. By constructing the associated phase diagram, we demonstrate that either Hebbian or anti-Hebbian plasticity may become optimal strategies depending on how the external input is projected to the target neural populations. Conversely, short-term plasticity enables the discrimination of temporal ordering in sequences of inputs by navigating the emergent multistable attractor landscape. By allowing a degree of variability in external stimuli, we also highlight the existence of an optimal variability for sequence discrimination at a given plasticity strength. In summary, the timescale of plasticity modulation shapes how inputs are represented in neural activities, thereby fundamentally altering the computational properties of the system. Our approach offers a unifying information-theoretic perspective of the role of plasticity, paving the way for a quantitative understanding of the emergence of complex computations in coupled neuronal-synaptic dynamics.
[15]
arXiv:2509.14029
(cross-list from cs.LG)
[pdf, html, other]
Title:
Deep Learning-Driven Peptide Classification in Biological Nanopores
Samuel Tovey, Julian Hoßbach, Sandro Kuppel, Tobias Ensslen, Jan C. Behrends, Christian Holm
Comments:
29 pages (incl. references) 7 figures
Subjects:
Machine Learning (cs.LG); Signal Processing (eess.SP); Computational Physics (physics.comp-ph); Biomolecules (q-bio.BM)
A device capable of performing real time classification of proteins in a clinical setting would allow for inexpensive and rapid disease diagnosis. One such candidate for this technology are nanopore devices. These devices work by measuring a current signal that arises when a protein or peptide enters a nanometer-length-scale pore. Should this current be uniquely related to the structure of the peptide and its interactions with the pore, the signals can be used to perform identification. While such a method would allow for real time identification of peptides and proteins in a clinical setting, to date, the complexities of these signals limit their accuracy. In this work, we tackle the issue of classification by converting the current signals into scaleogram images via wavelet transforms, capturing amplitude, frequency, and time information in a modality well-suited to machine learning algorithms. When tested on 42 peptides, our method achieved a classification accuracy of ~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward practical peptide/protein diagnostics at the point of care. In addition, we demonstrate model transfer techniques that will be critical when deploying these models into real hardware, paving the way to a new method for real-time disease diagnosis.
[16]
arXiv:2509.14118
(cross-list from math.OC)
[pdf, html, other]
Title:
Multi-Source Neural Activity Indices and Spatial Filters for EEG/MEG Inverse Problem: An Extension to MNE-Python
Julia Jurkowska, Joanna Dreszer, Monika Lewandowska, Krzysztof Tołpa, Tomasz Piotrowski
Subjects:
Optimization and Control (math.OC); Neurons and Cognition (q-bio.NC)
Accurate EEG/MEG source localization is essential for understanding brain function, yet remains challenging because the inverse problem is inherently ill-posed. In spatial filtering (beamforming) approaches, single-source LCMV spatial filters, though widely used, suffer from source cancellation when sources are correlated - a common experimental scenario. Multi-source frameworks, such as the multi-source minimum-variance pseudo-unbiased reduced-rank (MV-PURE) method, offer improved reconstruction and robust neural activity indices, yet their adoption has been limited by incomplete theory and lack of accessible implementations. In this paper, we present a rigorous derivation of multi-source neural activity indices and spatial filters, establishing a complete analytical framework with automated parameter selection. The resulting compact algebraic forms enable straightforward implementation. To facilitate adoption, we provide a full implementation extending MNE-Python, along with an accompanying tutorial, and demonstrate its utility on EEG experimental data, highlighting the practical advantages of multi-source spatial filtering for source localization and reconstruction.
[17]
arXiv:2509.14167
(cross-list from cs.LG)
[pdf, html, other]
Title:
Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework
Md Rezwan Jaher, Abul Mukid Mohammad Mukaddes, A. B. M. Abdul Malek
Comments:
43 pages, 10 figures (including supplementary material)
Subjects:
Machine Learning (cs.LG); Quantitative Methods (q-bio.QM); Applications (stat.AP); Methodology (stat.ME)
Many critical healthcare decisions are challenged by the inability to measure key underlying parameters. Glaucoma, a leading cause of irreversible blindness driven by elevated intraocular pressure (IOP), provides a stark example. The primary determinant of IOP, a tissue property called trabecular meshwork permeability, cannot be measured in vivo, forcing clinicians to depend on indirect surrogates. This clinical challenge is compounded by a broader computational one: developing predictive models for such ill-posed inverse problems is hindered by a lack of ground-truth data and prohibitive cost of large-scale, high-fidelity simulations. We address both challenges with an end-to-end framework to noninvasively estimate unmeasurable variables from sparse, routine data. Our approach combines a multi-stage artificial intelligence architecture to functionally separate the problem; a novel data generation strategy we term PCDS that obviates the need for hundreds of thousands of costly simulations, reducing the effective computational time from years to hours; and a Bayesian engine to quantify predictive uncertainty. Our framework deconstructs a single IOP measurement into its fundamental components from routine inputs only, yielding estimates for the unmeasurable tissue permeability and a patient's outflow facility. Our noninvasively estimated outflow facility achieved excellent agreement with state-of-the-art tonography with precision comparable to direct physical instruments. Furthermore, the newly derived permeability biomarker demonstrates high accuracy in stratifying clinical cohorts by disease risk, highlighting its diagnostic potential. More broadly, our framework establishes a generalizable blueprint for solving similar inverse problems in other data-scarce, computationally-intensive domains.
Replacement submissions (showing 12 of 12 entries)
[18]
arXiv:2410.13034
(replaced)
[pdf, html, other]
Title:
Synthesis and Perceptual Scaling of High Resolution Naturalistic Images Using Stable Diffusion
Leonardo Pettini, Carsten Bogler, Christian Doeller, John-Dylan Haynes
Comments:
80 pages, 26 Figures, 6 tables
Subjects:
Neurons and Cognition (q-bio.NC); Computer Vision and Pattern Recognition (cs.CV)
Naturalistic scenes are of key interest for visual perception, but controlling their perceptual and semantic properties is challenging. Previous work on naturalistic scenes has frequently focused on collections of discrete images with considerable physical differences between stimuli. However, it is often desirable to assess representations of naturalistic images that vary along a continuum. Traditionally, perceptually continuous variations of naturalistic stimuli have been obtained by morphing a source image into a target image. This produces transitions driven mainly by low-level physical features and can result in semantically ambiguous outcomes. More recently, generative adversarial networks (GANs) have been used to generate continuous perceptual variations within a stimulus category. Here we extend and generalize this approach using a different machine learning approach, a text-to-image diffusion model (Stable Diffusion XL), to generate a freely customizable stimulus set of photorealistic images that are characterized by gradual transitions, with each image representing a unique exemplar within a prompted category. We demonstrate the approach by generating a set of 108 object scenes from 6 categories. For each object scene, we generate 10 variants that are ordered along a perceptual continuum. This ordering was first estimated using a machine learning model of perceptual similarity (LPIPS) and then subsequently validated with a large online sample of human participants. In a subsequent experiment we show that this ordering is also predictive of confusability of stimuli in a working memory experiment. Our image set is suited for studies investigating the graded encoding of naturalistic stimuli in visual perception, attention, and memory.
[19]
arXiv:2501.13628
(replaced)
[pdf, other]
Title:
Language modulates vision: Evidence from neural networks and human brain-lesion models
Haoyang Chen, Bo Liu, Shuyue Wang, Xiaosha Wang, Wenjuan Han, Yixin Zhu, Xiaochun Wang, Yanchao Bi
Subjects:
Neurons and Cognition (q-bio.NC)
Comparing information structures in between deep neural networks (DNNs) and the human brain has become a key method for exploring their similarities and differences. Recent research has shown better alignment of vision-language DNN models, such as CLIP, with the activity of the human ventral occipitotemporal cortex (VOTC) than earlier vision models, supporting the idea that language modulates human visual perception. However, interpreting the results from such comparisons is inherently limited due to the "black box" nature of DNNs. To address this, we combined model-brain fitness analyses with human brain lesion data to examine how disrupting the communication pathway between the visual and language systems causally affects the ability of vision-language DNNs to explain the activity of the VOTC. Across four diverse datasets, CLIP consistently captured unique variance in VOTC neural representations, relative to both label-supervised (ResNet) and unsupervised (MoCo) models. This advantage tended to be left-lateralized at the group level, aligning with the human language network. Analyses of 33 stroke patients revealed that reduced white matter integrity between the VOTC and the language region in the left angular gyrus was correlated with decreased CLIP-brain correspondence and increased MoCo-brain correspondence, indicating a dynamic influence of language processing on the activity of the VOTC. These findings support the integration of language modulation in neurocognitive models of human vision, reinforcing concepts from vision-language DNN models. The sensitivity of model-brain similarity to specific brain lesions demonstrates that leveraging manipulation of the human brain is a promising framework for evaluating and developing brain-like computer models.
[20]
arXiv:2503.09841
(replaced)
[pdf, html, other]
Title:
Maintaining diversity in structured populations
David A. Brewster, Jakub Svoboda, Dylan Roscow, Krishnendu Chatterjee, Josef Tkadlec, Martin A. Nowak
Comments:
43 pages
Journal-ref:
PNAS Nexus, Volume 4, Issue 8, August 2025, pgaf252
Subjects:
Populations and Evolution (q-bio.PE)
We examine population structures for their ability to maintain diversity in neutral evolution. We use the general framework of evolutionary graph theory and consider birth-death (bd) and death-birth (db) updating. The population is of size $N$. Initially all individuals represent different types. The basic question is: what is the time $T_N$ until one type takes over the population? This time is known as consensus time in computer science and as total coalescent time in evolutionary biology. For the complete graph, it is known that $T_N$ is quadratic in $N$ for db and bd. For the cycle, we prove that $T_N$ is cubic in $N$ for db and bd. For the star, we prove that $T_N$ is cubic for bd and quasilinear ($N\log N$) for db. For the double star, we show that $T_N$ is quartic for bd. We derive upper and lower bounds for all undirected graphs for bd and db. We also show the Pareto front of graphs (of size $N=8$) that maintain diversity the longest for bd and db. Further, we show that some graphs that quickly homogenize can maintain high levels of diversity longer than graphs that slowly homogenize. For directed graphs, we give simple contracting star-like structures that have superexponential time scales for maintaining diversity.
[21]
arXiv:2509.08179
(replaced)
[pdf, html, other]
Title:
Computational modelling of Parkinson's disease: A multiscale approach with deep brain stimulation and stochastic noise
Aaron Herrera, Hina Shaheen
Comments:
9 pages, 14 figures
Subjects:
Neurons and Cognition (q-bio.NC)
Multiscale modelling presents a multifaceted perspective into understanding the mechanisms of the brain and how neurodegenerative disorders like Parkinson's disease (PD) manifest and evolve over time. In this study, we propose a novel co-simulation multiscale approach that unifies both micro- and macroscales to more rigorously capture brain dynamics. The presented design considers the electrodiffusive activity across the brain and in the network defined by the cortex, basal ganglia, and thalamus that is implicated in the mechanics of PD, as well as the contribution of presynaptic inputs in the highlighted regions. The application of deep brain stimulation (DBS) and its effects, along with the inclusion of stochastic noise are also examined. We found that the thalamus exhibits large, fluctuating spiking in both the deterministic and stochastic conditions, suggesting that noise contributes primarily to neural variability, rather than driving the overall spiking activity. Ultimately, this work intends to provide greater insights into the dynamics of PD and the brain which can eventually be converted into clinical use.
[22]
arXiv:2509.10432
(replaced)
[pdf, other]
Title:
Standards in the Preparation of Biomedical Research Metadata: A Bridge2AI Perspective
Harry Caufield, Satrajit Ghosh, Sek Wong Kong, Jillian Parker, Nathan Sheffield, Bhavesh Patel, Andrew Williams, Timothy Clark, Monica C. Munoz-Torres
Subjects:
Other Quantitative Biology (q-bio.OT); Artificial Intelligence (cs.AI)
AI-readiness describes the degree to which data may be optimally and ethically used for subsequent AI and Machine Learning (AI/ML) methods, where those methods may involve some combination of model training, data classification, and ethical, explainable prediction. The Bridge2AI consortium has defined the particular criteria a biomedical dataset may possess to render it AI-ready: in brief, a dataset's readiness is related to its FAIRness, provenance, degree of characterization, explainability, sustainability, and computability, in addition to its accompaniment with documentation about ethical data practices.
To ensure AI-readiness and to clarify data structure and relationships within Bridge2AI's Grand Challenges (GCs), particular types of metadata are necessary. The GCs within the Bridge2AI initiative include four data-generating projects focusing on generating AI/ML-ready datasets to tackle complex biomedical and behavioral research problems. These projects develop standardized, multimodal data, tools, and training resources to support AI integration, while addressing ethical data practices. Examples include using voice as a biomarker, building interpretable genomic tools, modeling disease trajectories with diverse multimodal data, and mapping cellular and molecular health indicators across the human body.
This report assesses the state of metadata creation and standardization in the Bridge2AI GCs, provides guidelines where required, and identifies gaps and areas for improvement across the program. New projects, including those outside the Bridge2AI consortium, would benefit from what we have learned about creating metadata as part of efforts to promote AI readiness.
[23]
arXiv:1911.02526
(replaced)
[pdf, other]
Title:
Dynamic traversal of large gaps by insects and legged robots reveals a template
Sean W. Gart, Changxin Yan, Ratan Othayoth, Zhiyi Ren, Chen Li
Journal-ref:
Bioinspiration & Biomimetics, 13, 026006 (2018)
Subjects:
Biological Physics (physics.bio-ph); Systems and Control (eess.SY); Quantitative Methods (q-bio.QM)
It is well known that animals can use neural and sensory feedback via vision, tactile sensing, and echolocation to negotiate obstacles. Similarly, most robots use deliberate or reactive planning to avoid obstacles, which relies on prior knowledge or high-fidelity sensing of the environment. However, during dynamic locomotion in complex, novel, 3-D terrains such as forest floor and building rubble, sensing and planning suffer bandwidth limitation and large noise and are sometimes even impossible. Here, we study rapid locomotion over a large gap, a simple, ubiquitous obstacle, to begin to discover general principles of dynamic traversal of large 3-D obstacles. We challenged the discoid cockroach and an open-loop six-legged robot to traverse a large gap of varying length. Both the animal and the robot could dynamically traverse a gap as large as 1 body length by bridging the gap with its head, but traversal probability decreased with gap length. Based on these observations, we developed a template that well captured body dynamics and quantitatively predicted traversal performance. Our template revealed that high approach speed, initial body pitch, and initial body pitch angular velocity facilitated dynamic traversal, and successfully predicted a new strategy of using body pitch control that increased the robot maximal traversal gap length by 50%. Our study established the first template of dynamic locomotion beyond planar surfaces and is an important step in expanding terradynamics into complex 3-D terrains.
[24]
arXiv:1911.02527
(replaced)
[pdf, other]
Title:
Body-terrain interaction affects large bump traversal of insects and legged robots
Sean W. Gart, Chen Li
Journal-ref:
Bioinspiration & Biomimetics, 13, 026005 (2018)
Subjects:
Biological Physics (physics.bio-ph); Robotics (cs.RO); Quantitative Methods (q-bio.QM)
Small animals and robots must often rapidly traverse large bump-like obstacles when moving through complex 3-D terrains, during which, in addition to leg-ground contact, their body inevitably comes into physical contact with the obstacles. However, we know little about the performance limits of large bump traversal and how body-terrain interaction affects traversal. To address these, we challenged the discoid cockroach and an open-loop six-legged robot to dynamically run into a large bump of varying height to discover the maximal traversal performance, and studied how locomotor modes and traversal performance are affected by body-terrain interaction. Remarkably, during rapid running, both the animal and the robot were capable of dynamically traversing a bump much higher than its hip height (up to 4 times the hip height for the animal and 3 times for the robot, respectively) at traversal speeds typical of running, with decreasing traversal probability with increasing bump height. A stability analysis using a novel locomotion energy landscape model explained why traversal was more likely when the animal or robot approached the bump with a low initial body yaw and a high initial body pitch, and why deflection was more likely otherwise. Inspired by these principles, we demonstrated a novel control strategy of active body pitching that increased the robot maximal traversable bump height by 75%. Our study is a major step in establishing the framework of locomotion energy landscapes to understand locomotion in complex 3-D terrains.
[25]
arXiv:2002.09711
(replaced)
[pdf, other]
Title:
Robotic modeling of snake traversing large, smooth obstacles reveals stability benefits of body compliance
Qiyuan Fu, Chen Li
Journal-ref:
Royal Society Open Science, 7, 191192 (2020)
Subjects:
Biological Physics (physics.bio-ph); Systems and Control (eess.SY); Quantitative Methods (q-bio.QM)
Snakes can move through almost any terrain. Although their locomotion on flat surfaces using planar gaits is inherently stable, when snakes deform their body out of plane to traverse complex terrain, maintaining stability becomes a challenge. On trees and desert dunes, snakes grip branches or brace against depressed sand for stability. However, how they stably surmount obstacles like boulders too large and smooth to gain such anchor points is less understood. Similarly, snake robots are challenged to stably traverse large, smooth obstacles for search and rescue and building inspection. Our recent study discovered that snakes combine body lateral undulation and cantilevering to stably traverse large steps. Here, we developed a snake robot with this gait and snake-like anisotropic friction and used it as a physical model to understand stability principles. The robot traversed steps as high as a third of its body length rapidly and stably. However, on higher steps, it was more likely to fail due to more frequent rolling and flipping over, which was absent in the snake with a compliant body. Adding body compliance reduced the robot roll instability by statistically improving surface contact, without reducing speed. Besides advancing understanding of snake locomotion, our robot achieved high traversal speed surpassing most previous snake robots and approaching snakes, while maintaining high traversal probability.
[26]
arXiv:2402.17131
(replaced)
[pdf, html, other]
Title:
Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function
Pedro Seber
Subjects:
Machine Learning (cs.LG); Molecular Networks (q-bio.MN)
O-GlcNAcylation, a subtype of glycosylation, has the potential to be an important target for therapeutics, but methods to reliably predict O-GlcNAcylation sites had not been available until 2023; a 2021 review correctly noted that published models were insufficient and failed to generalize. Moreover, many are no longer usable. In 2023, a considerably better recurrent neural network (RNN) model was published. This article creates improved models by using a new loss function, which we call the weighted focal differentiable MCC. RNN models trained with this new loss display superior performance to models trained using the weighted cross-entropy loss; this new function can also be used to fine-tune trained models. An RNN trained with this loss achieves state-of-the-art performance in O-GlcNAcylation site prediction with an F$_1$ score of 38.88% and an MCC of 38.20% on an independent test set from the largest dataset available.
[27]
arXiv:2403.08592
(replaced)
[pdf, html, other]
Title:
Data-Efficient Sleep Staging with Synthetic Time Series Pretraining
Niklas Grieger, Siamak Mehrkanoon, Stephan Bialonski
Comments:
15 pages, 4 figures, 1 table
Journal-ref:
Grieger, N., Mehrkanoon, S., Bialonski, S. (2025). Data-Efficient Sleep Staging with Synthetic Time Series Pretraining. Algorithms, 18(9), 580
Subjects:
Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)
Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets. To address these challenges, various strategies, such as self-supervised learning, have been suggested, but they typically rely on extensive empirical datasets. Inspired by recent advances in computer vision, we propose a pretraining task termed "frequency pretraining" to pretrain a neural network for sleep staging by predicting the frequency content of randomly generated synthetic time series. Our experiments demonstrate that our method surpasses fully supervised learning in scenarios with limited data and few subjects, and matches its performance in regimes with many subjects. Furthermore, our results underline the relevance of frequency information for sleep stage scoring, while also demonstrating that deep neural networks utilize information beyond frequencies to enhance sleep staging performance, which is consistent with previous research. We anticipate that our approach will be advantageous across a broad spectrum of applications where EEG data is limited or derived from a small number of subjects, including the domain of brain-computer interfaces.
[28]
arXiv:2508.08548
(replaced)
[pdf, other]
Title:
Emergence: from physics to biology, sociology, and computer science
Ross H. McKenzie
Comments:
160 pages, 414 references. Revised version has minor corrections and additions
Subjects:
History and Philosophy of Physics (physics.hist-ph); Statistical Mechanics (cond-mat.stat-mech); Neurons and Cognition (q-bio.NC); Quantum Physics (quant-ph)
Many systems involve numerous interacting parts and the whole system can have properties that the individual parts do not. I take this novelty as the defining characteristic of an emergent property. Other characteristics associated with emergence discussed include universality, order, complexity, unpredictability, irreducibility, diversity, self-organisation, discontinuities, and singularities. Emergent phenomena are widespread across physics, biology, social sciences, and computing, and are central to major scientific and societal challenges. Understanding emergence involves considering the stratification of reality across different scales (energy, time, length, complexity), each with its distinct ontology and epistemology, leading to semi-autonomous scientific disciplines. A central challenge is bridging the gap between macroscopic emergent properties and microscopic component interactions. Identifying an intermediate mesoscopic scale where new, weakly interacting entities or modular structures emerge is key. Theoretical approaches, such as effective theories (describing phenomena at a specific scale) and toy models (simplified systems for analysis), are vital. The Ising model exemplifies how toy models can elucidate emergence characteristics. Emergence is central to condensed matter physics, chaotic systems, fluid dynamics, nuclear physics, quantum gravity, neural networks, protein folding, and social segregation. An emergent perspective should influence scientific strategy by shaping research questions, methodologies, priorities, and resource allocation. An elusive goal is the design and control of emergent properties.
[29]
arXiv:2508.14233
(replaced)
[pdf, html, other]
Title:
Excitonic Coupling and Photon Antibunching in Venus Yellow Fluorescent Protein Dimers: A Lindblad Master Equation Approach
Ian T. Abrahams
Comments:
16 pages (excluding references), 4 figures, includes discussions of cryogenic exciton dynamics, quantum biophotonics, quantum technology, evolutionary adaptations in fluorescent proteins, and the potential application of Venus dimers as quantum bits (qubits) for quantum information processing
Subjects:
Quantum Physics (quant-ph); Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Biological Physics (physics.bio-ph); Optics (physics.optics); Biomolecules (q-bio.BM)
Strong excitonic coupling and photon antibunching (AB) have been observed together in Venus yellow fluorescent protein dimers and currently lack a cohesive theoretical explanation. In 2019, Kim et al. demonstrated Davydov splitting in circular dichroism spectra, revealing strong J-like coupling, while antibunched fluorescence emission was confirmed by combined antibunching--fluorescence correlation spectroscopy (AB/FCS fingerprinting). To investigate the implications of this coexistence, Venus dimer population dynamics are modeled within a Lindblad master equation framework, justified by the separation of characteristic coupling, dephasing, and thermal relaxation rates. Simulations predict rapid decoherence, yielding bright/dark state mixtures consistent with antibunched fluorescence emission at room temperature. Thus, excitonic coupling and photon AB are reconciled without invoking long-lived quantum coherence. More broadly, fluorescent proteins emerge as tractable model systems for probing evolutionary pressures on chromoprotein photophysics and quantum dynamics. Cryogenic cooling may extend coherence time into the regime required for ultrafast gate operations, suggesting fluorescent protein dimers as a viable platform for bio-inspired qubits.
Total of 29 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack