Statistics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
stat
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Statistics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Monday, 15 September 2025
Total of 68 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 26 of 26 entries)
[1]
arXiv:2509.09756
[pdf, html, other]
Title:
Computational Study of New Record-Based Transmuted Chen Distribution and Its Applications to the Failure Time and Iron Sheet Data
Caner Tanış
Subjects:
Methodology (stat.ME); Computation (stat.CO)
This study is considered to introduce a novel distribution as an alternative to Chen distribution via the record-based transmutation method. This technique is based on the distributions of first two upper record values. Thus, we suggest a new special case based on Chen distribution in the family of record based transmuted distributions. We explore various distributional properties of the proposed model namely, quantile function, hazard function, median, moments, and stochastic ordering. Our distribution has three parameters and to estimate these parameters, we utilize nine different and well-known estimators. Then, we compare the performances of these estimators via a comprehensive simulation study. Also, we provide two real-world data examples to assess the fits the data sets the suggested model and its some competitors.
[2]
arXiv:2509.09757
[pdf, html, other]
Title:
Record-based transmuted log-logistic distribution: Properties, simulation, and applications to petroleum rock and reactor pump data
Caner Tanış
Subjects:
Methodology (stat.ME); Computation (stat.CO)
This study aims to introduce a new lifetime distribution, called the record-based transformed log-logistic distribution, to the literature. We obtain this distribution using a record-based transformation map based on the distributions of upper record values. We explore some mathematical properties of the suggested distribution, namely the quantile function, hazard function, moments, order statistics, and stochastic ordering. We discuss the point estimation via seven different methods such as maximum likelihood, least squares, weighted least squares, Anderson-Darling, Cramer-von Mises, maximum product spacings, and right tail Anderson Darling. Then, we perform a Monte Carlo simulation study to evaluate the performances of these estimators. Also, we present two practical data examples, reactor pump failure and petroleum rock data to compare the fits of the proposed distribution with its rivals. As a result of data analysis, we conclude that the best-fitted distribution is the record-based transmuted log-logistic distribution for reactor pump failure and petroleum rock data sets.
[3]
arXiv:2509.09758
[pdf, html, other]
Title:
A Path Signature Framework for Detecting Creative Fatigue in Digital Advertising
Charles Shaw
Subjects:
Applications (stat.AP)
The finite lifespan of advertising effectiveness, commonly known as "creative fatigue", presents a significant challenge in computational marketing. Identifying the onset of performance degradation is critical for optimising media spend and maximising campaign returns, yet traditional methods often lack the sensitivity and scalability required in modern advertising ecosystems. This paper introduces a novel methodological framework for detecting creative fatigue by applying path signature analysis, a sophisticated technique from stochastic analysis, to performance time-series data. We treat an ad's performance trajectory as a path in two-dimensional space and use its signature as a rich feature descriptor. By calculating the distance between the signatures of consecutive time windows, our method identifies statistically significant change points in performance dynamics. We further translate these statistical events into a direct financial metric by quantifying the opportunity cost of continued investment in a fatigued creative. Through synthetic experiments and case studies, we demonstrate that this signature-based approach provides a mathematically principled and complementary framework for analysing creative fatigue patterns. To the best of our knowledge, this represents the first application of path signature methods to the advertising fatigue detection problem, opening new avenues for geometric approaches in marketing analytics.
[4]
arXiv:2509.09773
[pdf, html, other]
Title:
Optimal Inference of the Mean Outcome under Optimal Treatment Regime
Shuoxun Xu (1 and 2), Xinzhou Guo (1) ((1) Department of Mathematics, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, China, (2) Division of Biostatistics, School of Public Health, University of California, Berkeley, Berkeley, CA, USA)
Comments:
17 pages, 5 figures
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
When an optimal treatment regime (OTR) is considered, we need to evaluate the OTR in a valid and efficient way. The classical inference applied to the mean outcome under OTR, assuming the OTR is the same as the estimated OTR, might be biased when the regularity assumption that OTR is unique is violated. Although several methods have been proposed to allow nonregularity in such inference, its optimality is unclear due to challenges in deriving semiparametric efficiency bounds under potential nonregularity. In this paper, we address the bias issue via adaptive smoothing over the estimated OTR and develop a valid inference procedure on the mean outcome under OTR regardless of whether regularity is satisfied. We establish the optimality of the proposed method by deriving a lower bound of the asymptotic variance for the robust asymptotically linear unbiased estimator to the mean outcome under OTR and showing that our proposed estimator achieves the variance lower bound. The considered estimator class is general and the derived variance lower bound paves a novel way to establish efficiency optimality theories for OTR in a more general scenario allowing nonregularity. The merit of the proposed method is demonstrated by re-analyzing the ACTG 175 trial.
[5]
arXiv:2509.09845
[pdf, html, other]
Title:
Meta-Analysis with JASP, Part I: Classical Approaches
František Bartoš, Eric-Jan Wagenmakers, Wolfgang Viechtbauer
Subjects:
Methodology (stat.ME); Computation (stat.CO)
Meta-analyses play a crucial part in empirical science, enabling researchers to synthesize evidence across studies and draw more precise and generalizable conclusions. Despite their importance, access to advanced meta-analytic methodology is often limited to scientists and students with considerable expertise in computer programming. To lower the barrier for adoption, we have developed the Meta-Analysis module in JASP (this https URL), a free and open-source software for statistical analyses. The module offers standard and advanced meta-analytic techniques through an easy-to-use graphical user interface (GUI), allowing researchers with diverse technical backgrounds to conduct state-of-the-art analyses. This manuscript presents an overview of the meta-analytic tools implemented in the module and showcases how JASP supports a meta-analytic practice that is rigorous, relevant, and reproducible.
[6]
arXiv:2509.09850
[pdf, html, other]
Title:
Meta-Analysis with JASP, Part II: Bayesian Approaches
František Bartoš, Eric-Jan Wagenmakers
Subjects:
Methodology (stat.ME); Computation (stat.CO)
Bayesian inference is on the rise, partly because it allows researchers to quantify parameter uncertainty, evaluate evidence for competing hypotheses, incorporate model ambiguity, and seamlessly update knowledge as information accumulates. All of these advantages apply to the meta-analytic settings; however, advanced Bayesian meta-analytic methodology is often restricted to researchers with programming experience. In order to make these tools available to a wider audience, we implemented state-of-the-art Bayesian meta-analysis methods in the Meta-Analysis module of JASP, a free and open-source statistical software package (this https URL). The module allows researchers to conduct Bayesian estimation, hypothesis testing, and model averaging with models such as meta-regression, multilevel meta-analysis, and publication bias adjusted meta-analysis. Results can be interpreted using forest plots, bubble plots, and estimated marginal means. This manuscript provides an overview of the Bayesian meta-analysis tools available in JASP and demonstrates how the software enables researchers of all technical backgrounds to perform advanced Bayesian meta-analysis.
[7]
arXiv:2509.09855
[pdf, html, other]
Title:
An Information-Theoretic Framework for Credit Risk Modeling: Unifying Industry Practice with Statistical Theory for Fair and Interpretable Scorecards
Agus Sudjianto, Denis Burakov
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Credit risk modeling relies extensively on Weight of Evidence (WoE) and Information Value (IV) for feature engineering, and Population Stability Index (PSI) for drift monitoring, yet their theoretical foundations remain disconnected. We establish a unified information-theoretic framework revealing these industry-standard metrics as instances of classical information divergences. Specifically, we prove that IV exactly equals PSI (Jeffreys divergence) computed between good and bad credit outcomes over identical bins. Through the delta method applied to WoE transformations, we derive standard errors for IV and PSI, enabling formal hypothesis testing and probabilistic fairness constraints for the first time. We formalize credit modeling's inherent performance-fairness trade-off as maximizing IV for predictive power while minimizing IV for protected attributes. Using automated binning with depth-1 XGBoost stumps, we compare three encoding strategies: logistic regression with one-hot encoding, WoE transformation, and constrained XGBoost. All methods achieve comparable predictive performance (AUC 0.82-0.84), demonstrating that principled, information-theoretic binning outweighs encoding choice. Mixed-integer programming traces Pareto-efficient solutions along the performance-fairness frontier with uncertainty quantification. This framework bridges theory and practice, providing the first rigorous statistical foundation for widely-used credit risk metrics while offering principled tools for balancing accuracy and fairness in regulated environments.
[8]
arXiv:2509.09904
[pdf, html, other]
Title:
A Smooth Computational Transition in Tensor PCA
Zhangsong Li
Comments:
49 pages, 2 figures
Subjects:
Statistics Theory (math.ST); Data Structures and Algorithms (cs.DS); Probability (math.PR); Machine Learning (stat.ML)
We propose an efficient algorithm for tensor PCA based on counting a specific family of weighted hypergraphs. For the order-$p$ tensor PCA problem where $p \geq 3$ is a fixed integer, we show that when the signal-to-noise ratio is $\lambda n^{-\frac{p}{4}}$ where $\lambda=\Omega(1)$, our algorithm succeeds and runs in time $n^{C+o(1)}$ where $C=C(\lambda)$ is a constant depending on $\lambda$. This algorithm improves a poly-logarithmic factor compared to previous algorithms based on the Sum-of-Squares hierarchy \cite{HSS15} or based on the Kikuchi hierarchy in statistical physics \cite{WEM19}. Furthermore, our result shows a smooth tradeoff between the signal-to-noise ratio and the computational cost in this problem, thereby confirming a conjecture posed in \cite{KWB22}.
[9]
arXiv:2509.09965
[pdf, other]
Title:
Confidence Intervals for Extinction Risk: Validating Population Viability Analysis with Limited Data
Hiroshi Hakoyama
Comments:
63 pages total (including appendix), 17 figures, 4 tables
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
Quantitative assessment of extinction risk requires not only point estimates but also confidence intervals (CIs) that remain informative with limited data. Their reliability has been debated, as short observation spans can inflate uncertainty and reduce usefulness. I derive new CIs for extinction probability $G$ under the Wiener process with drift, a canonical model of population viability analysis. The method uses correlated noncentral-$t$ distributions for the transformed statistics $\widehat{w}$ and $\widehat{z}$, derived from drift and variance estimators, and constructs CIs of the extinction probability by exploiting the geometric properties of $G(\widehat{w},\widehat{z})$ in parameter space. Monte Carlo experiments show that the proposed intervals attain nominal coverage with narrower widths than common approximate methods, including the delta method, moment-based approaches, and bootstrap. A key result is that even with short time series, extinction probabilities that are very small or very large can be estimated reliably. This resolves a long-standing concern that population viability analysis fails under data scarcity. Applied to three 64-year catch series for Japanese eel (Anguilla japonica), the analysis indicates extinction risk well below the IUCN Criterion E thresholds for Critically Endangered and Endangered, with narrow CIs. These findings demonstrate that extinction-risk CIs can be both statistically rigorous and practical for Red List evaluations, even when data are limited.
[10]
arXiv:2509.10045
[pdf, html, other]
Title:
A Bayesian Framework for Regularized Estimation in Multivariate Models Integrating Approximate Computing Concepts
Jan Kalina
Subjects:
Methodology (stat.ME)
This paper discusses regularized estimators in the multivariate statistical model as tools naturally arising within a Bayesian framework. First, a link is established between Bayesian estimation and inference under parameter rounding (quantization), thereby connecting two distinct paradigms: Bayesian inference and approximate computing. Next, Bayesian estimation of the means from two independent multivariate normal samples is employed to justify shrinkage estimators, i.e., means shrunk toward the pooled mean. Finally, regularized linear discriminant analysis (LDA) is considered. Various shrinkage strategies for the mean are justified from a Bayesian perspective, and novel algorithms for their computation are proposed. The proposed methods are illustrated by numerical experiments on real and simulated data.
[11]
arXiv:2509.10067
[pdf, html, other]
Title:
Weakening assumptions in the evaluation of treatment effects in longitudinal randomized trials with truncation by death or other intercurrent events
Georgi Baklicharov, Kelly Van Lancker, Stijn Vansteelandt
Subjects:
Methodology (stat.ME)
Intercurrent events, such as treatment switching, rescue medication, or truncation by death, can complicate the interpretation of intention-to-treat (ITT) analyses in randomized clinical trials. Recent advances in causal inference address these challenges by targeting alternative estimands, such as hypothetical estimands or principal stratum estimands (e.g., survivor average causal effects). However, such approaches often require strong, unverifiable assumptions, partly due to limited data on time-varying confounders and the difficulty of adjusting for them. Additionally, strict trial protocols frequently lead to (near) violations of the positivity assumption, resulting in limited information for identifying these estimands.
In this paper, we propose a novel approach that sidesteps these difficulties by focusing on testing the null hypothesis of no treatment effect in the presence of arbitrary intercurrent events, including truncation by death, using longitudinal trial data. Our key idea is to compare treated and untreated individuals, matched on baseline covariates, at the most recent time point before either experiences an intercurrent event. We refer to such contrasts as Pairwise Last Observation Time (PLOT) estimands. These estimands can be identified in randomized clinical trials without requiring additional structural assumptions, and even in the presence of the aforementioned positivity violations. However, they may still be susceptible to a form of residual selection bias. We show that this bias vanishes under the conditions typically required by alternative methods, and find it to be more generally small in extensive simulation studies. Building on this, we develop asymptotically efficient, model-free tests using data-adaptive estimation of nuisance parameters. We evaluate the method's performance via simulation studies.
[12]
arXiv:2509.10073
[pdf, html, other]
Title:
Benchmarking Classical, Machine Learning, and Bayesian Survival Models for Clinical Prediction
Irving Gómez-Méndez, Sivakorn Phromsiri, Ittiphat Kijpaisansak, Settawut Chaithurdthum
Subjects:
Applications (stat.AP)
Survival analysis is a statistical framework for modeling time-to-event data, particularly valuable in healthcare for predicting outcomes like patient discharge or recurrence. This study implements and compares several survival models - including Weibull, Weibull AFT, Weibull AFT with Gamma Frailty, Cox Proportional Hazards (CoxPH), Random Survival Forest (RSF), and DeepSurv - using a publicly available breast cancer dataset. This study aims to benchmark classical, machine learning, and Bayesian survival models in terms of their predictive performance, interpretability, and suitability for clinical deployment. The models are evaluated using performance metrics such as the Concordance Index (C-index) and the Root Mean Squared Error (RMSE). DeepSurv showed the highest predictive performance, while interpretable models like RSF and Weibull AFT with Gamma Frailty offered competitive results. We also explored the implementation of statistical models from a Bayesian perspective, including frailty models, due to their ability to properly quantify uncertainty. Notably, frailty models are not readily available in standard survival analysis libraries, necessitating custom implementation. Our results demonstrate that interpretable statistical models, when correctly implemented using parameters that are effectively estimated using a Bayesian approach, can perform competitively with modern black-box models. These findings illustrate the trade-offs between model complexity, interpretability, and predictive power, highlighting the potential of Bayesian survival models in clinical decision-making settings.
[13]
arXiv:2509.10149
[pdf, html, other]
Title:
A sampling method based on highest density regions: Applications to surrogate models for rare events estimation
Jocelyn Minini, Micha Wasem
Comments:
21 pages, 14 figures
Subjects:
Methodology (stat.ME); Applications (stat.AP)
This paper introduces a practical sampling method for training surrogate models in the context of uncertainty propagation. We propose a heuristic method to uniformly draw samples within highest density regions of the density given by the random vector describing the uncertainty of the model parameters. The resulting experimental design aims to provide a better approximation of the underlying true model compared to the cases where experimental designs have been drawn according to the distribution of the random vector itself. To assess the quality of our approach, three error metrics are considered: The first is the leave-one-out error, the second the relative mean square error and the third is the error generated by the surrogate model when estimating the probability of failure of the system compared to its reference value. The highest density region-based designs are shown to globally outperform the random vector-based designs both in terms of relative mean square error as well as in estimating the probability of failure. The proposed method is applicable within a black-box context and is compatible with existing uncertainty quantification frameworks for low dimensional and moderately correlated inputs. It may thus be useful in case of reliability problems, Bayesian inverse analysis, or whenever the surrogate model is used in a predictor mode.
[14]
arXiv:2509.10166
[pdf, html, other]
Title:
Repulsive Monte Carlo on the sphere for the sliced Wasserstein distance
Vladimir Petrovic, Rémi Bardenet, Agnès Desolneux
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
In this paper, we consider the problem of computing the integral of a function on the unit sphere, in any dimension, using Monte Carlo methods. Although the methods we present are general, our guiding thread is the sliced Wasserstein distance between two measures on $\mathbb{R}^d$, which is precisely an integral on the $d$-dimensional sphere. The sliced Wasserstein distance (SW) has gained momentum in machine learning either as a proxy to the less computationally tractable Wasserstein distance, or as a distance in its own right, due in particular to its built-in alleviation of the curse of dimensionality. There has been recent numerical benchmarks of quadratures for the sliced Wasserstein, and our viewpoint differs in that we concentrate on quadratures where the nodes are repulsive, i.e. negatively dependent. Indeed, negative dependence can bring variance reduction when the quadrature is adapted to the integration task. Our first contribution is to extract and motivate quadratures from the recent literature on determinantal point processes (DPPs) and repelled point processes, as well as repulsive quadratures from the literature specific to the sliced Wasserstein distance. We then numerically benchmark these quadratures. Moreover, we analyze the variance of the UnifOrtho estimator, an orthogonal Monte Carlo estimator. Our analysis sheds light on UnifOrtho's success for the estimation of the sliced Wasserstein in large dimensions, as well as counterexamples from the literature. Our final recommendation for the computation of the sliced Wasserstein distance is to use randomized quasi-Monte Carlo in low dimensions and \emph{UnifOrtho} in large dimensions. DPP-based quadratures only shine when quasi-Monte Carlo also does, while repelled quadratures show moderate variance reduction in general, but more theoretical effort is needed to make them robust.
[15]
arXiv:2509.10197
[pdf, html, other]
Title:
Simultaneous testing of hypotheses and alternatives
P.A. Koldanov, A.P. Koldanov
Comments:
9 pages
Subjects:
Methodology (stat.ME)
To identify statistically significant conclusions, it is proposed to simultaneously test hypotheses and alternatives. It is shown that, under the condition of free combination of hypotheses and alternatives, the closure method leads to single-step procedures for the simultaneous testing of hypotheses and alternatives. Using an example it is shown the result is lost when hypotheses and alternatives do not satisfy the condition of free combination of hypotheses and alternatives. It is shown that the average number of insignificant conclusions is an uncontrolled part of the risk function of a single-step procedure with an appropriate choice of the loss function.
[16]
arXiv:2509.10238
[pdf, html, other]
Title:
Using joint models in phase I dose-finding designs in oncology: considerations for frequentist approaches
Xijin Chen, Pavel Mozgunov, Richard D. Baird, Thomas Jaki
Comments:
30 pages, 14 figures, 11 tables
Subjects:
Methodology (stat.ME)
Dose-finding trials for oncology studies are traditionally designed to assess safety in the early stages of drug development. With the rise of molecularly targeted therapies and immuno-oncology compounds, biomarker-driven approaches have gained significant importance. In this paper, we propose a novel approach that incorporates multiple values of a predictive biomarker to assist in evaluating binary toxicity outcomes using the factorization of a joint model in phase I dose-finding oncology trials. The proposed joint model framework, which utilizes additional repeated biomarker values as an early predictive marker for potential toxicity, is compared to the likelihood-based continual reassessment method (CRM) using only binary toxicity data, across various dose-toxicity relationship scenarios. Our findings highlight a critical limitation of likelihood-based approaches in early-phase dose-finding studies with small sample sizes: estimation challenges that have been previously overlooked in the phase I dose-escalation setting. We explore potential remedies to address these challenges and emphasize the appropriate use of likelihood-based methods. Simulation results demonstrate that the proposed joint model framework, by integrating biomarker information, can alleviate estimation problems in the the likelihood-based continual reassessment method (CRM) and improve the proportion of correct selection. However, we highlight that the inherent data limitations in early-phase dose-finding studies remain a significant challenge that cannot fully be overcomed in the frequentist framework.
[17]
arXiv:2509.10268
[pdf, other]
Title:
Quantifying and testing dependence to categorical variables
Siegfried Hörmann, Daniel Strenger-Galvis
Subjects:
Statistics Theory (math.ST)
We suggest a dependence coefficient between a categorical variable and some general variable taking values in a metric space. We derive important theoretical properties and study the large sample behaviour of our suggested estimator. Moreover, we develop an independence test which has an asymptotic $\chi^2$-distribution if the variables are independent and prove that this test is consistent against any violation of independence. We discuss some extensions, including a variant of the coefficient for measuring conditional dependence.
[18]
arXiv:2509.10321
[pdf, html, other]
Title:
Conformal prediction without knowledge of labeled calibration data
Jonas Flechsig, Maximilian Pilz
Comments:
10 pages, 1 number
Subjects:
Methodology (stat.ME)
We extend the method of conformal prediction beyond the case relying on labeled calibration data. Replacing the calibration scores by suitable estimates, we identify conformity sets $C$ for classification and regression models that rely on unlabeled calibration data. Given a classification model with accuracy $1-\beta$, we prove that the conformity sets guarantee a coverage of $P(Y \in C) \geq 1-\alpha-\beta$ for an arbitrary parameter $\alpha \in (0,1)$. The same coverage guarantee also holds for regression models, if we replace the accuracy by a similar exactness measure. Finally, we describe how to use the theoretical results in practice.
[19]
arXiv:2509.10325
[pdf, html, other]
Title:
Using the rejection sampling for finding tests
Markku Kuismin
Subjects:
Methodology (stat.ME)
A new method based on the rejection sampling for finding statistical tests is proposed. This method is conceptually intuitive, easy to implement, and applicable for arbitrary dimension. To illustrate its potential applicability, three distinct empirical examples are presented: (1) examine the differences between group means of correlated (repeated) or independent samples, (2) examine if a mean vector equals to a specific fixed vector, and (3) investigate if samples come from a specific population distribution. The simulation examples indicate that the new test has similar statistical power as uniformly the most powerful (unbiased) tests. Moreover, these examples demonstrate that the new test is a powerful goodness-of-fit test.
[20]
arXiv:2509.10337
[pdf, html, other]
Title:
Why does your graph neural network fail on some graphs? Insights from exact generalisation error
Nil Ayday, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Graph Neural Networks (GNNs) are widely used in learning on graph-structured data, yet a principled understanding of why they succeed or fail remains elusive. While prior works have examined architectural limitations such as over-smoothing and over-squashing, these do not explain what enables GNNs to extract meaningful representations or why performance varies drastically between similar architectures. These questions are related to the role of generalisation: the ability of a model to make accurate predictions on unlabelled data. Although several works have derived generalisation error bounds for GNNs, these are typically loose, restricted to a single architecture, and offer limited insight into what governs generalisation in practice. In this work, we take a different approach by deriving the exact generalisation error for GNNs in a transductive fixed-design setting through the lens of signal processing. From this viewpoint, GNNs can be interpreted as graph filter operators that act on node features via the graph structure. By focusing on linear GNNs while allowing non-linearity in the graph filters, we derive the first exact generalisation error for a broad range of GNNs, including convolutional, PageRank-based, and attention-based models. The exact characterisation of the generalisation error reveals that only the aligned information between node features and graph structure contributes to generalisation. Furthermore, we quantify the effect of homophily on generalisation. Our work provides a framework that explains when and why GNNs can effectively leverage structural and feature information, offering practical guidance for model selection.
[21]
arXiv:2509.10354
[pdf, html, other]
Title:
Bayesian Semiparametric Joint Modeling of Gap-Time Distribution for Multitype Recurrent Events and a Terminal Event
Mithun Kumar Acharjee, AKM Fazlur Rahman
Subjects:
Methodology (stat.ME)
In biomedical settings, multitype recurrent events such as stroke and heart failure occur frequently, often concluding with a terminal event such as death. Understanding the links between these recurring and terminal events is fundamental to developing interventions that delay detrimental outcomes. Joint modeling is needed to quantify the dependence between event types and between recurrent events and mortality. We propose a Bayesian semiparametric joint model on the gap-time scale for multitype recurrent events and a terminal event. The model includes a shared frailty that links all recurrent types and the terminal event. Each baseline hazard is assigned a gamma-process prior, while regression and frailty parameters receive standard parametric priors. This ensures flexible baselines and familiar effect measures. The construction gives closed-form expressions for the cumulative hazard and frailty component and connects to Breslow-Aalen type estimators as a special case of our estimator, linking the Bayesian procedure to the classical approach. Computationally, we develop a simple MCMC sampler that avoids large matrix factorizations and scales nearly linearly in sample size. A comprehensive simulation evaluates four criteria: accuracy, prediction, robustness, and computation. There is no exact frequentist version of our specification; for comparison, we fit the same model with an EM algorithm in a frequentist framework. Our model and MCMC algorithm demonstrate superior performance on each criterion. We illustrate the approach with data from the Antihypertensive and Lipid-Lowering Treatment to Prevent Heart Attack Trial (ALLHAT), jointly analyzing acute and chronic cardiovascular recurrences and death.
[22]
arXiv:2509.10362
[pdf, html, other]
Title:
Spatial Modeling and Risk Zoning of Global Extreme Precipitation via Graph Neural Networks and r-Pareto Processes
Zimu Wang, Yifan Wu, Daning Bi
Comments:
18 pages
Subjects:
Applications (stat.AP)
Extreme precipitation events occurring over large spatial domains pose substantial threats to societies because they can trigger compound flooding, landslides, and infrastructure failures across wide areas. A hybrid framework for spatial extreme precipitation modeling and risk zoning is proposed that integrates graph neural networks with r-Pareto processes (GNN-rP). Unlike traditional statistical spatial extremes models, this approach learns nonlinear, nonstationary dependence structures from precipitation-derived spatial graphs and applies a data-driven tail functional to model joint exceedances in a low-dimensional embedding space. Using NASA's IMERG observations (2000-2021) and CMIP6 SSP5-8.5 projections, the framework delineates coherent high-risk zones, quantifies their temporal persistence, and detects emerging hotspots under climate change. Compared with two baseline approaches, the GNN-rP pipeline substantially improves pointwise detection of high-risk grid cells while yielding comparable clustering stability. Results highlight persistent high-risk regions in the tropical belt, especially monsoon and convective zones, and reveal decadal-scale persistence that is punctuated by episodic reconfigurations under high-emission scenarios. By coupling machine learning with extreme value theory, GNN-rP offers a scalable, interpretable tool for adaptive climate risk zoning, with direct applications in infrastructure planning, disaster preparedness, and climate-resilient policy design.
[23]
arXiv:2509.10383
[pdf, html, other]
Title:
Network Meta-Analysis of survival outcomes with non-proportional hazards using flexible M-splines
David M. Phillippo (1), Ayman Sadek (1), Hugo Pedder (1), Nicky J. Welton (1) ((1) University of Bristol, Bristol, UK)
Comments:
38 pages, 14 figures
Subjects:
Methodology (stat.ME)
Network meta-analysis (NMA) is widely used in healthcare decision-making, where estimates of the effect of multiple treatments on outcomes are required. For time-to-event outcomes such as survival or disease progression the most common approach is to model log hazard ratios; however, this relies on the proportional hazards assumption. Novel treatments such as immunotherapies are expected to display complex hazard functions that cannot be captured by standard parametric models, which results in non-proportional hazards when comparing treatments from different classes. As a result, alternative models such as fractional polynomials or restricted cubic splines are often used. These allow substantial flexibility on the shape of the baseline hazard, but require time-consuming model selection or are intractable for Bayesian analysis. We propose a flexible NMA model using M-splines on the baseline hazard, with a novel weighted random walk prior distribution that provides shrinkage to avoid overfitting and is invariant to the choice of knots and timescale. Non-proportional hazards are modelled either by stratifying by treatment or by introducing treatment effects on the spline coefficients, and covariates may be included on the log hazard rate and spline coefficients. Treatment and covariate effects on the spline coefficients are given random walk prior distributions to smoothly model departures from proportionality over time. The methods are implemented in the user-friendly R package multinma, which supports analyses with aggregate data, individual participant data, or mixtures of both. We apply the methods to a NMA of progression-free survival with treatments for non-small cell lung cancer.
[24]
arXiv:2509.10385
[pdf, html, other]
Title:
Differentially Private Decentralized Dataset Synthesis Through Randomized Mixing with Correlated Noise
Utsab Saha, Tanvir Muntakim Tonoy, Hafiz Imtiaz
Comments:
This work has been submitted to the IEEE for possible publication
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
In this work, we explore differentially private synthetic data generation in a decentralized-data setting by building on the recently proposed Differentially Private Class-Centric Data Aggregation (DP-CDA). DP-CDA synthesizes data in a centralized setting by mixing multiple randomly-selected samples from the same class and injecting carefully calibrated Gaussian noise, ensuring ({\epsilon}, {\delta})-differential privacy. When deployed in a decentralized or federated setting, where each client holds only a small partition of the data, DP-CDA faces new challenges. The limited sample size per client increases the sensitivity of local computations, requiring higher noise injection to maintain the differential privacy guarantee. This, in turn, leads to a noticeable degradation in the utility compared to the centralized setting. To mitigate this issue, we integrate the Correlation-Assisted Private Estimation (CAPE) protocol into the federated DP-CDA framework and propose CAPE Assisted Federated DP-CDA algorithm. CAPE enables limited collaboration among the clients by allowing them to generate jointly distributed (anti-correlated) noise that cancels out in aggregate, while preserving privacy at the individual level. This technique significantly improves the privacy-utility trade-off in the federated setting. Extensive experiments on MNIST and FashionMNIST datasets demonstrate that the proposed CAPE Assisted Federated DP-CDA approach can achieve utility comparable to its centralized counterpart under some parameter regime, while maintaining rigorous differential privacy guarantees.
[25]
arXiv:2509.10393
[pdf, html, other]
Title:
A Computable Measure of Suboptimality for Entropy-Regularised Variational Objectives
Clémentine Chazal, Heishiro Kanagawa, Zheyang Shen, Anna Korba, Chris. J. Oates
Subjects:
Computation (stat.CO); Machine Learning (stat.ML)
Several emerging post-Bayesian methods target a probability distribution for which an entropy-regularised variational objective is minimised. This increased flexibility introduces a computational challenge, as one loses access to an explicit unnormalised density for the target. To mitigate this difficulty, we introduce a novel measure of suboptimality called 'gradient discrepancy', and in particular a 'kernel gradient discrepancy' (KGD) that can be explicitly computed. In the standard Bayesian context, KGD coincides with the kernel Stein discrepancy (KSD), and we obtain a novel charasterisation of KSD as measuring the size of a variational gradient. Outside this familiar setting, KGD enables novel sampling algorithms to be developed and compared, even when unnormalised densities cannot be obtained. To illustrate this point several novel algorithms are proposed, including a natural generalisation of Stein variational gradient descent, with applications to mean-field neural networks and prediction-centric uncertainty quantification presented. On the theoretical side, our principal contribution is to establish sufficient conditions for desirable properties of KGD, such as continuity and convergence control.
[26]
arXiv:2509.10421
[pdf, html, other]
Title:
Bayesian Optimum Warranty Region for Right Censored Two Dimensional Dependent Data
Tanmay Sen, Rathin Das, Ritwik Bhattacharya
Subjects:
Applications (stat.AP)
Warranty policies play a crucial role in balancing customer satisfaction and cost of the manufacturer. Traditional one-dimensional warranty frameworks, based solely on either age or usage, often fail to capture the joint effect of product life factors. This article investigates two-dimensional warranty policies by combining Free Replacement Warranty, Pro-Rata Warranty, and Combination FRW-PRW Warranty schemes across both age and usage scales. A dissatisfaction cost function is proposed alongside the economic benefit and warranty cost functions, and the expected utility framework is employed to derive optimal warranty parameters. The expectation is taken with respect to the posterior predictive distribution of product lifetime and usage data, ensuring a data-driven approach. Finally, the methodology is validated using an open-source dataset, and a new two-dimensional starter motor dataset is introduced to demonstrate the practical advantages of adopting two-dimensional warranty policies.
Cross submissions (showing 8 of 8 entries)
[27]
arXiv:2509.09723
(cross-list from cs.CL)
[pdf, other]
Title:
ALIGNS: Unlocking nomological networks in psychological measurement through a large language model
Kai R. Larsen, Sen Yan, Roland Müller, Lan Sang, Mikko Rönkkö, Ravi Starzl, Donald Edmondson
Subjects:
Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Methodology (stat.ME)
Psychological measurement is critical to many disciplines. Despite advances in measurement, building nomological networks, theoretical maps of how concepts and measures relate to establish validity, remains a challenge 70 years after Cronbach and Meehl proposed them as fundamental to validation. This limitation has practical consequences: clinical trials may fail to detect treatment effects, and public policy may target the wrong outcomes. We introduce Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS), a large language model-based system trained with validated questionnaire measures. ALIGNS provides three comprehensive nomological networks containing over 550,000 indicators across psychology, medicine, social policy, and other fields. This represents the first application of large language models to solve a foundational problem in measurement validation. We report classification accuracy tests used to develop the model, as well as three evaluations. In the first evaluation, the widely used NIH PROMIS anxiety and depression instruments are shown to converge into a single dimension of emotional distress. The second evaluation examines child temperament measures and identifies four potential dimensions not captured by current frameworks, and questions one existing dimension. The third evaluation, an applicability check, engages expert psychometricians who assess the system's importance, accessibility, and suitability. ALIGNS is freely available at this http URL, complementing traditional validation methods with large-scale nomological analysis.
[28]
arXiv:2509.09728
(cross-list from cs.CL)
[pdf, html, other]
Title:
A meta-analysis on the performance of machine-learning based language models for sentiment analysis
Elena Rohde, Jonas Klingwort, Christian Borgs
Subjects:
Computation and Language (cs.CL); Machine Learning (cs.LG); Applications (stat.AP)
This paper presents a meta-analysis evaluating ML performance in sentiment analysis for Twitter data. The study aims to estimate the average performance, assess heterogeneity between and within studies, and analyze how study characteristics influence model performance. Using PRISMA guidelines, we searched academic databases and selected 195 trials from 20 studies with 12 study features. Overall accuracy, the most reported performance metric, was analyzed using double arcsine transformation and a three-level random effects model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76, 0.84]. This paper provides two key insights: 1) Overall accuracy is widely used but often misleading due to its sensitivity to class imbalance and the number of sentiment classes, highlighting the need for normalization. 2) Standardized reporting of model performance, including reporting confusion matrices for independent test sets, is essential for reliable comparisons of ML classifiers across studies, which seems far from common practice.
[29]
arXiv:2509.09772
(cross-list from cs.LG)
[pdf, html, other]
Title:
Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management
Sanjay Basu, Sadiq Y. Patel, Parth Sheth, Bhairavi Muralidharan, Namrata Elamaran, Aakriti Kinra, Rajaie Batniji
Comments:
10 pages, 5 figures, 4 tables
Subjects:
Machine Learning (cs.LG); Applications (stat.AP)
Population health management programs for Medicaid populations coordinate longitudinal outreach and services (e.g., benefits navigation, behavioral health, social needs support, and clinical scheduling) and must be safe, fair, and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement Learning (HACO) framework that separates risk calibration from preference optimization to generate conservative action recommendations at scale. In our setting, each step involves choosing among common coordination actions (e.g., which member to contact, by which modality, and whether to route to a specialized service) while controlling the near-term risk of adverse utilization events (e.g., unplanned emergency department visits or hospitalizations). Using a de-identified operational dataset from Waymark comprising 2.77 million sequential decisions across 168,126 patients, HACO (i) trains a lightweight risk model for adverse events, (ii) derives a conformal threshold to mask unsafe actions at a target risk level, and (iii) learns a preference policy on the resulting safe subset. We evaluate policies with a version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit subgroup performance across age, sex, and race. HACO achieves strong risk discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at {\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses reveal systematic differences in estimated value across demographics, underscoring the importance of fairness auditing. Our results show that conformal risk gating integrates cleanly with offline RL to deliver conservative, auditable decision support for population health management teams.
[30]
arXiv:2509.09802
(cross-list from math.OC)
[pdf, html, other]
Title:
Sparse Polyak: an adaptive step size rule for high-dimensional M-estimation
Tianqi Qiao, Marie Maros
Subjects:
Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML)
We propose and study Sparse Polyak, a variant of Polyak's adaptive step size, designed to solve high-dimensional statistical estimation problems where the problem dimension is allowed to grow much faster than the sample size. In such settings, the standard Polyak step size performs poorly, requiring an increasing number of iterations to achieve optimal statistical precision-even when, the problem remains well conditioned and/or the achievable precision itself does not degrade with problem size. We trace this limitation to a mismatch in how smoothness is measured: in high dimensions, it is no longer effective to estimate the Lipschitz smoothness constant. Instead, it is more appropriate to estimate the smoothness restricted to specific directions relevant to the problem (restricted Lipschitz smoothness constant). Sparse Polyak overcomes this issue by modifying the step size to estimate the restricted Lipschitz smoothness constant. We support our approach with both theoretical analysis and numerical experiments, demonstrating its improved performance.
[31]
arXiv:2509.09891
(cross-list from math.DS)
[pdf, html, other]
Title:
Data-driven approximation of transfer operators for mean-field stochastic differential equations
Eirini Ioannou, Stefan Klus, Gonçalo dos Reis
Subjects:
Dynamical Systems (math.DS); Numerical Analysis (math.NA); Probability (math.PR); Machine Learning (stat.ML)
Mean-field stochastic differential equations, also called McKean--Vlasov equations, are the limiting equations of interacting particle systems with fully symmetric interaction potential. Such systems play an important role in a variety of fields ranging from biology and physics to sociology and economics. Global information about the behavior of complex dynamical systems can be obtained by analyzing the eigenvalues and eigenfunctions of associated transfer operators such as the Perron--Frobenius operator and the Koopman operator. In this paper, we extend transfer operator theory to McKean--Vlasov equations and show how extended dynamic mode decomposition and the Galerkin projection methodology can be used to compute finite-dimensional approximations of these operators, which allows us to compute spectral properties and thus to identify slowly evolving spatiotemporal patterns or to detect metastable sets. The results will be illustrated with the aid of several guiding examples and benchmark problems including the Cormier model, the Kuramoto model, and a three-dimensional generalization of the Kuramoto model.
[32]
arXiv:2509.10104
(cross-list from cs.AI)
[pdf, html, other]
Title:
AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework
Sofia Vei, Paolo Giudici, Pavlos Sermpezis, Athena Vakali, Adelaide Emma Bernardelli
Subjects:
Artificial Intelligence (cs.AI); Methodology (stat.ME)
The absolute dominance of Artificial Intelligence (AI) introduces unprecedented societal harms and risks. Existing AI risk assessment models focus on internal compliance, often neglecting diverse stakeholder perspectives and real-world consequences. We propose a paradigm shift to a human-centric, harm-severity adaptive approach grounded in empirical incident data. We present AI Harmonics, which includes a novel AI harm assessment metric (AIH) that leverages ordinal severity data to capture relative impact without requiring precise numerical estimates. AI Harmonics combines a robust, generalized methodology with a data-driven, stakeholder-aware framework for exploring and prioritizing AI harms. Experiments on annotated incident data confirm that political and physical harms exhibit the highest concentration and thus warrant urgent mitigation: political harms erode public trust, while physical harms pose serious, even life-threatening risks, underscoring the real-world relevance of our approach. Finally, we demonstrate that AI Harmonics consistently identifies uneven harm distributions, enabling policymakers and organizations to target their mitigation efforts effectively.
[33]
arXiv:2509.10384
(cross-list from cs.LG)
[pdf, html, other]
Title:
Flow Straight and Fast in Hilbert Space: Functional Rectified Flow
Jianxin Zhang, Clayton Scott
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Many generative models originally developed in finite-dimensional Euclidean space have functional generalizations in infinite-dimensional settings. However, the extension of rectified flow to infinite-dimensional spaces remains unexplored. In this work, we establish a rigorous functional formulation of rectified flow in an infinite-dimensional Hilbert space. Our approach builds upon the superposition principle for continuity equations in an infinite-dimensional space. We further show that this framework extends naturally to functional flow matching and functional probability flow ODEs, interpreting them as nonlinear generalizations of rectified flow. Notably, our extension to functional flow matching removes the restrictive measure-theoretic assumptions in the existing theory of \citet{kerrigan2024functional}. Furthermore, we demonstrate experimentally that our method achieves superior performance compared to existing functional generative models.
[34]
arXiv:2509.10439
(cross-list from cs.LG)
[pdf, html, other]
Title:
Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration
Ahmed Khaled, Satyen Kale, Arthur Douillard, Chi Jin, Rob Fergus, Manzil Zaheer
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
Modern machine learning often requires training with large batch size, distributed data, and massively parallel compute hardware (like mobile and other edge devices or distributed data centers). Communication becomes a major bottleneck in such settings but methods like Local Stochastic Gradient Descent (Local SGD) show great promise in reducing this additional communication overhead. Local SGD consists of three parts: a local optimization process, an aggregation mechanism, and an outer optimizer that uses the aggregated updates from the nodes to produce a new model. While there exists an extensive literature on understanding the impact of hyperparameters in the local optimization process, the choice of outer optimizer and its hyperparameters is less clear. We study the role of the outer optimizer in Local SGD, and prove new convergence guarantees for the algorithm. In particular, we show that tuning the outer learning rate allows us to (a) trade off between optimization error and stochastic gradient noise variance, and (b) make up for ill-tuning of the inner learning rate. Our theory suggests that the outer learning rate should sometimes be set to values greater than $1$. We extend our results to settings where we use momentum in the outer optimizer, and we show a similar role for the momentum-adjusted outer learning rate. We also study acceleration in the outer optimizer and show that it improves the convergence rate as a function of the number of communication rounds, improving upon the convergence rate of prior algorithms that apply acceleration locally. Finally, we also introduce a novel data-dependent analysis of Local SGD that yields further insights on outer learning rate tuning. We conduct comprehensive experiments with standard language models and various outer optimizers to validate our theory.
Replacement submissions (showing 34 of 34 entries)
[35]
arXiv:2210.06737
(replaced)
[pdf, html, other]
Title:
Joint Optimization and Statistical Inference for Zero-th Order Simulation Optimization
Yuhang Wu, Zeyu Zheng, Yingfei Wang, Guangyu Zhang, Zuohua Zhang, Chu Wang
Subjects:
Methodology (stat.ME)
We consider stochastic optimization problems with the dual tasks of (i) effectively finding the optimizer and (ii) reliably conducting statistical inference for the optimal objective function value. We find that classical simulation optimization and stochastic optimization algorithms, despite of their fast convergence rates to the optimizer under strong convexity assumptions, may not come with a valid central limit theorem (CLT) with a vanishing bias. This non-vanishing bias can harm statistical inference and the construction of asymptotically valid confidence intervals. We fix this issue by providing a new stochastic optimization algorithm that on one hand maintains the same fast convergence rate and on the other hand permits the establishment of a valid CLT with vanishing bias. We discuss practical implementations of the proposed algorithm and conduct numerical experiments to illustrate the theoretical findings.
[36]
arXiv:2303.03084
(replaced)
[pdf, html, other]
Title:
On Regression in Extreme Regions
Stephan Clémençon, Nathan Huet, Anne Sabourin
Comments:
30 pages (main paper), 12 pages (appendix), 3 figures, 2 tables. Accepted for publication in EJS
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)
We establish a statistical learning theoretical framework aimed at extrapolation, or out-of-domain generalization, on the unobserved tails of covariates in continuous regression problems. Our strategy involves performing statistical regression on a subsample of observations with continuous labels that are the furthest away from the origin, focusing specifically on their angular components. The underlying assumptions of our approach are grounded in the theory of multivariate regular variation, a cornerstone of extreme value theory. We address the stylized problem of nonparametric least squares regression with predictors chosen from a Vapnik-Chervonenkis class.
This work contributes to a broader initiative to develop statistical learning theoretical foundations for supervised learning strategies that enhance performance on the supposedly heavy tails of covariates. Previous efforts in this area have focused exclusively on binary classification on extreme covariates. Although the continuous target setting necessitates different techniques and regularity assumptions, our main results echo findings from earlier studies. We quantify the predictive performance on tail regions in terms of excess risk, presenting it as a finite sample risk bound with a clear bias-variance decomposition. Numerical experiments with simulated and real data illustrate our theoretical findings.
[37]
arXiv:2306.10635
(replaced)
[pdf, html, other]
Title:
Finite Population Survey Sampling: An Unapologetic Bayesian Perspective
Sudipto Banerjee
Comments:
This version corrects a few errors (mostly typographic) in the version published in Sankhya Series A 86, 95-124 (2024; URL/DOI: this https URL)
Journal-ref:
Sankhya A 86 (Suppl 1), 95--124 (2024)
Subjects:
Methodology (stat.ME)
This article attempts to offer some perspectives on Bayesian inference for finite population quantities when the units in the population are assumed to exhibit complex dependencies. Beginning with an overview of Bayesian hierarchical models, including some that yield design-based Horvitz-Thompson estimators, the article proceeds to introduce dependence in finite populations and sets out inferential frameworks for ignorable and nonignorable responses. Multivariate dependencies using graphical models and spatial processes are discussed and some salient features of two recent analyses for spatial finite populations are presented.
[38]
arXiv:2405.14711
(replaced)
[pdf, html, other]
Title:
Zero-inflation in the Multivariate Poisson Lognormal Family
Bastien Batardière, Julien Chiquet, François Gindraud, Mahendra Mariadassou
Comments:
26 pages including appendices. 7 figures, 2 tables
Subjects:
Methodology (stat.ME); Applications (stat.AP); Machine Learning (stat.ML)
Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies. However PLN doesn't account for zero-inflation, a feature frequently observed in real-world datasets. Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable. The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates. We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one. The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to 90% of the observed counts. We then apply both ZIPLN and PLN to a cow microbiome dataset, containing 90.6% of zeroes. Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination.
[39]
arXiv:2408.16039
(replaced)
[pdf, html, other]
Title:
Generalizing Difference-in-Differences to Non-Canonical Settings: Identifying an Array of Estimands
Zach Shahn, Laura Hatfield
Subjects:
Methodology (stat.ME)
Consider a general setting in which data on an outcome is collected in two `groups' at two time periods, with certain group-periods deemed `treated' and others `untreated'. A special case is the canonical Difference-in-Differences (DiD) setting in which one group is treated only in the second period while the other is treated in neither period. Then it is well known that under a parallel trends assumption across the two groups the classic DiD formula (subtracting the average change in outcome across periods in the treated group by the average change in the outcome across periods in the untreated group) identifies the average treatment effect on the treated in the second period. But other relations between group, period, and treatment are possible. For example, the groups might be demographic (or other baseline covariate) categories with all units in both groups treated in the second period and none treated in the first, i.e. a pre-post design. Or one group might be treated in both periods while the other is treated in neither. Furthermore, other parallel trends assumptions under other treatment regimes are possible. For example, we could assume the two groups' potential outcomes would evolve in parallel under a regime of `do not switch treatment in the second period'. In fact, there is a literal array of data structures and parallel trends assumptions. The difference between the changes in outcomes of the two groups, which we dub the `group DiD' (gDiD) formula, identifies different causal estimands depending on the data structure and parallel trends assumption adopted. Here, we determine under which combinations of data structure and assumptions the gDiD formula identifies meaningful causal estimands. We also explore when parallel trends assumptions are amenable to empirical check or structural justification via Single World Intervention Graphs.
[40]
arXiv:2409.05271
(replaced)
[pdf, html, other]
Title:
Priors from Envisioned Posterior Judgments: A Novel Elicitation Approach With Application to Bayesian Clinical Trials
Yongdong Ouyang, Janice J Eng, Denghuang Zhan, Hubert Wong, The WnW Research Team
Comments:
minor update
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Background: The uptake of formalized prior elicitation from experts in Bayesian clinical trials has been limited due to challenges such as complex statistical modeling, lack of practical tools, and the cognitive burden placed on experts arising from needing to quantify their uncertainty probabilistically. Existing methods also fail to address prior-posterior coherence, i.e., how do we ensure that the posterior distribution, obtained mathematically from combining the estimated prior with the trial data, reflects the expert's actual posterior beliefs?
Method: In this study, we propose a new elicitation approach that effectuates prior-posterior coherence and reduces cognitive burden. This is achieved by eliciting expert responses, comprising point estimates only, about envisioned posterior judgments under various data outcomes and inferring the prior distribution by minimizing discrepancies between these responses and expected responses derived from the posterior distribution. Via an iterative process, experts receive feedback on the degree of coherency of their responses, and are invited to revise their responses to achieve greater coherency. The feasibility and potential value of this new approach are illustrated through an application to an ongoing trial.
Results: We involved 10 experts from Walk 'n watch trial research team. Experts were presented with 16 hypothetical outcome scenarios to experts and elicit the priors followed by the developed elicitation framework. Following two rounds of elicitation, experts' judgments showed substantial improvement in coherency, demonstrating the practical applicability of the proposed elicitation approach.
Conclusion: The proposed method provides a practical solution to the challenges of formalized prior elicitation in Bayesian clinical trials by addressing prior-posterior coherence and reducing cognitive demands on experts.
[41]
arXiv:2409.16613
(replaced)
[pdf, html, other]
Title:
Oral exams in introductory statistics class with non-native English speakers
Eric Yanchenko
Subjects:
Other Statistics (stat.OT)
Oral exams are a powerful tool to assess student's learning. This is particularly important in introductory statistics classes where students struggle to grasp various topics like the interpretation of probability, $p$-values and more. The challenge of acquiring conceptual understanding is only heightened when students are learning in a second language. In this paper, I share my experience administering oral exams to an introductory statistics class of non-native English speakers at a Japanese university. I explain the context of the university and course, before detailing the exam. Of particular interest is the relationship between exam performance and English proficiency. The results showed little relationship between the two, meaning the exam seemed to truly test student's statistical knowledge rather than their English ability. I close with encouragements and recommendations for practitioners hoping to implement similar oral exams, focusing on the unique difficulties faced by students not learning in their mother tongue.
[42]
arXiv:2410.18486
(replaced)
[pdf, html, other]
Title:
Evolving Voices Based on Temporal Poisson Factorisation
Jan Vávra (1 and 2), Bettina Grün (1), Paul Hofmarcher (2) ((1) Vienna University of Economics and Business, (2) Paris-Lodron University of Salzburg)
Comments:
main paper: 20 pages (2 single figures, 3 double figures, 3 tables), appendix: 2 pages, supplementary materials: 18 pages (2 plots, 4 quadruple plots, 2 tables), references: 3 pages
Journal-ref:
Statistical Modelling. 2025;0(0)
Subjects:
Methodology (stat.ME); Machine Learning (cs.LG)
The world is evolving and so is the vocabulary used to discuss topics in speech. Analysing political speech data from more than 30 years requires the use of flexible topic models to uncover the latent topics and their change in prevalence over time as well as the change in the vocabulary of the topics. We propose the temporal Poisson factorisation (TPF) model as an extension to the Poisson factorisation model to model sparse count data matrices obtained based on the bag-of-words assumption from text documents with time stamps. We discuss and empirically compare different model specifications for the time-varying latent variables consisting either of a flexible auto-regressive structure of order one or a random walk. Estimation is based on variational inference where we consider a combination of coordinate ascent updates with automatic differentiation using batching of documents. Suitable variational families are proposed to ease inference. We compare results obtained using independent univariate variational distributions for the time-varying latent variables to those obtained with a multivariate variant. We discuss in detail the results of the TPF model when analysing speeches from 18 sessions in the U.S. Senate (1981-2016).
[43]
arXiv:2411.04229
(replaced)
[pdf, html, other]
Title:
Detecting State Changes in Functional Neuronal Connectivity using Factorial Switching Linear Dynamical Systems
Yiwei Gong, Susanna B. Mierau, Sinead A. Williamson
Subjects:
Methodology (stat.ME)
A key question in brain sciences is how to identify time-evolving functional connectivity, such as that obtained from recordings of neuronal activity over time. We wish to explain the observed phenomena in terms of latent states which, in the case of neuronal activity, might correspond to subnetworks of neurons within a brain or organoid. Many existing approaches assume that only one latent state can be active at a time, in contrast to our domain knowledge. We propose a switching dynamical system based on the factorial hidden Markov model. Unlike existing approaches, our model acknowledges that neuronal activity can be caused by multiple subnetworks, which may be activated either jointly or independently. A change in one part of the network does not mean that the entire connectivity pattern will change. We pair our model with scalable variational inference algorithm, using a concrete relaxation of the underlying factorial hidden Markov model, to effectively infer the latent states and model parameters. We show that our algorithm can recover ground-truth structure and yield insights about the maturation of neuronal activity in microelectrode array recordings from in vitro neuronal cultures.
[44]
arXiv:2412.20724
(replaced)
[pdf, other]
Title:
Soft Diamond Regularizers for Deep Learning
Olaoluwa Adigun, Bart Kosko
Comments:
25 pages, 15 figures. This version extends the earlier version titled "Training Deep Neural Classifiers with Soft Diamond Regularizers"
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
This chapter presents the new family of soft diamond synaptic regularizers based on thick-tailed symmetric alpha stable $S{\alpha}S$ probability bell curves. These new parametrized weight priors improved deep-learning performance on image and language-translation test sets and increased the sparsity of the trained weights. They outperformed the state-of-the-art hard-diamond Laplacian regularizer of sparse lasso regression and classification. The $S{\alpha}S$ synaptic weight priors have power-law bell-curve tails that are thicker than the thin exponential tails of Gaussian bell curves that underly ridge regularizers. Their tails get thicker as the $\alpha$ parameter decreases. These thicker tails model more impulsive behavior and allow for occasional distant search in synaptic weight spaces of extremely high dimension. The geometry of their constraint sets has a diamond shape. The shape varies from a circle to a star or diamond that depends on the $\alpha$ tail thickness and dispersion of the $S{\alpha}S$ weight prior. These $S{\alpha}S$ bell curves lack a closed form in general and this makes direct training computationally intensive. We removed this computational bottleneck by using a precomputed look-up table. We tested the soft diamond regularizers with deep neural classifiers on both image test sets and German-to-English language translation. The image simulations used the three datasets CIFAR-10, CIFAR-100, and Caltech-256. The regularizers improved the accuracy and sparsity of the classifiers. We also tested with deep neural machine-translation models on the IWSLT-2016 Evaluation dataset for German-to-English text translation. They also outperformed ridge regularizers and lasso regularizers. These findings recommend the sub-Cauchy $\alpha = 0.5$ soft diamond regularizer as a competitive and sparse regularizer for large-scale machine learning.
[45]
arXiv:2501.06920
(replaced)
[pdf, html, other]
Title:
The ladder of abstraction in statistical graphics
Andrew Gelman
Comments:
11 figures
Subjects:
Methodology (stat.ME)
Graphical forms such as scatterplots, line plots, and histograms are so familiar that it can be easy to forget how abstract they are. As a result, we often produce graphs that are difficult to follow. We propose a strategy for graphical communication by climbing a ladder of abstraction (a term from linguistics that we borrow from Hayakawa, 1939), starting with simple plots of special cases and then at each step embedding a graph into a more general framework. We demonstrate with two examples, first graphing a set of equations related to a modeled trajectory and then graphing data from an analysis of income and voting.
[46]
arXiv:2502.21194
(replaced)
[pdf, html, other]
Title:
Prior shift estimation for positive unlabeled data through the lens of kernel embedding
Jan Mielniczuk, Wojciech Rejchel, Paweł Teisseyre
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We study estimation of a class prior for unlabeled target samples which possibly differs from that of source population. Moreover, it is assumed that the source data is partially observable: only samples from the positive class and from the whole population are available (PU learning scenario). We introduce a novel direct estimator of a class prior which avoids estimation of posterior probabilities in both populations and has a simple geometric interpretation. It is based on a distribution matching technique together with kernel embedding in a Reproducing Kernel Hilbert Space and is obtained as an explicit solution to an optimisation task. We establish its asymptotic consistency as well as an explicit non-asymptotic bound on its deviation from the unknown prior, which is calculable in practice. We study finite sample behaviour for synthetic and real data and show that the proposal works consistently on par or better than its competitors.
[47]
arXiv:2503.04100
(replaced)
[pdf, html, other]
Title:
Improving discrepancy by moving a few points
Gleb Smirnov, Roman Vershynin
Comments:
9 pages; accepted version
Subjects:
Statistics Theory (math.ST); Probability (math.PR)
We show how to improve the discrepancy of an iid sample by moving only a few points. Specifically, modifying \( O(m) \) sample points on average reduces the Kolmogorov-Smirnov distance to the population distribution to \(1/m\).
[48]
arXiv:2504.15946
(replaced)
[pdf, html, other]
Title:
The e-Partitioning Principle of False Discovery Rate Control
Jelle Goeman, Rianne de Heide, Aldo Solari
Comments:
This paper has been subsumed into the merged work arXiv:2509.02517 . Please read and cite that paper instead of this one
Subjects:
Statistics Theory (math.ST)
We present a novel necessary and sufficient principle for False Discovery Rate (FDR) control. This e-Partitioning Principle says that a procedure controls FDR if and only if it is a special case of a general e-Partitioning procedure. By writing existing methods as special cases of this procedure, we can achieve uniform improvements of these methods, and we show this in particular for the eBH, BY and Su methods. We also show that methods developed using the $e$-Partitioning Principle have several valuable properties. They generally control FDR not just for one rejected set, but simultaneously over many, allowing post hoc flexibility for the researcher in the final choice of the rejected hypotheses. Under some conditions, they also allow for post hoc adjustment of the error rate, choosing the FDR level $\alpha$ post hoc, or switching to familywise error control after seeing the data. In addition, e-Partitioning allows FDR control methods to exploit logical relationships between hypotheses to gain power.
[49]
arXiv:2505.23592
(replaced)
[pdf, html, other]
Title:
A Modern Theory of Cross-Validation through the Lens of Stability
Jing Lei
Comments:
130 pages, 1 figure
Subjects:
Statistics Theory (math.ST)
Modern data analysis and statistical learning are marked by complex data structures and black-box algorithms. Data complexity stems from technologies like imaging, remote sensing, wearables, and genomic sequencing. Simultaneously, black-box models -- especially deep neural networks -- have achieved impressive results. This combination raises new challenges for uncertainty quantification and statistical inference, which we term "black-box inference."
Black-box inference is difficult due to the lack of traditional modeling assumptions and the opaque behavior of modern estimators. These make it hard to characterize the distribution of estimation errors. A popular solution is post-hoc randomization, which, under mild assumptions like exchangeability, can yield valid uncertainty quantification. Such methods range from classical techniques like permutation tests, jackknife, and bootstrap, to recent innovations like conformal inference. These approaches typically need little knowledge of data distributions or the internal working of estimators. Many rely on the idea that estimators behave similarly under small data changes -- a concept formalized as stability. Over time, stability has become a key principle in data science, influencing generalization error, privacy, and adaptive inference.
This article investigates cross-validation (CV) -- a widely used resampling method -- through the lens of stability. We first review recent theoretical results on CV for estimating generalization error and model selection under stability. We then examine uncertainty quantification for CV-based risk estimates. Together, these insights yield new theory and tools, which we apply to topics like model selection, selective inference, and conformal prediction.
[50]
arXiv:2506.05776
(replaced)
[pdf, html, other]
Title:
The effects of retraining on the stability of global models in retail demand forecasting
Marco Zanotti
Subjects:
Applications (stat.AP); Other Statistics (stat.OT)
Forecast stability, that is, the consistency of predictions over time, is essential in business settings where sudden shifts in forecasts can disrupt planning and erode trust in predictive systems. Despite its importance, stability is often overlooked in favor of accuracy. In this study, we evaluate the stability of point and probabilistic forecasts across several retraining scenarios using two large retail demand datasets (M5 and VN1) and ten different global forecasting models. To analyze stability in the probabilistic setting, we propose a new model-agnostic, distribution-free, and scale-free metric that measures probabilistic instability: the Scaled Multi-Quantile Change (SMQC). Furthermore, we also evaluate the effects of retraining on various ensemble configurations based on forecast pooling. The results show that, compared to continuous retraining, less frequent retraining not only preserves but often improves forecast stability, challenging the need for continuous retraining. The study promotes a shift toward stability-aware forecasting practices, proposing a new tool to effectively evaluate forecast stability in probabilistic settings, and offering practical guidelines for building more robust prediction systems.
[51]
arXiv:2507.04779
(replaced)
[pdf, html, other]
Title:
Constructive Universal Approximation and Sure Convergence for Multi-Layer Neural Networks
Chien-Ming Chi
Comments:
34 pages, 3 figures, 7 tables
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
We propose o1Neuro, a new neural network model built on sparse indicator activation neurons, with two key statistical properties. (1) Constructive universal approximation: At the population level, a deep o1Neuro can approximate any measurable function of $\boldsymbol{X}$, while a shallow o1Neuro suffices for additive models with two-way interaction components, including XOR and univariate terms, assuming $\boldsymbol{X} \in [0,1]^p$ has bounded density. Combined with prior work showing that a single-hidden-layer non-sparse network is a universal approximator, this highlights a trade-off between activation sparsity and network depth in approximation capability. (2) Sure convergence: At the sample level, the optimization of o1Neuro reaches an optimal model with probability approaching one after sufficiently many update rounds, and we provide an example showing that the required number of updates is well bounded under linear data-generating models. Empirically, o1Neuro is compared with XGBoost, Random Forests, and TabNet for learning complex regression functions with interactions, demonstrating superior predictive performance on several benchmark datasets from OpenML and the UCI Machine Learning Repository with $n = 10000$, as well as on synthetic datasets with $100 \le n \le 20000$.
[52]
arXiv:2507.13024
(replaced)
[pdf, other]
Title:
When Pattern-by-Pattern Works: Theoretical and Empirical Insights for Logistic Models with Missing Values
Christophe Muller (PREMEDICAL), Erwan Scornet (LPSM), Julie Josse (PREMEDICAL)
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Predicting a response with partially missing inputs remains a challenging task even in parametric models, since parameter estimation in itself is not sufficient to predict on partially observed inputs. Several works study prediction in linear models. In this paper, we focus on logistic models, which present their own difficulties. From a theoretical perspective, we prove that a Pattern-by-Pattern strategy (PbP), which learns one logistic model per missingness pattern, accurately approximates Bayes probabilities in various missing data scenarios (MCAR, MAR and MNAR). Empirically, we thoroughly compare various methods (constant and iterative imputations, complete case analysis, PbP, and an EM algorithm) across classification, probability estimation, calibration, and parameter inference. Our analysis provides a comprehensive view on the logistic regression with missing values. It reveals that mean imputation can be used as baseline for low sample sizes, and improved performance is obtained via nonlinear multiple iterative imputation techniques with the labels (MICE.RF.Y). For large sample sizes, PbP is the best method for Gaussian mixtures, and we recommend MICE.RF.Y in presence of nonlinear features.
[53]
arXiv:2507.15529
(replaced)
[pdf, html, other]
Title:
Algorithms for Approximating Conditionally Optimal Bounds
George Bissias
Subjects:
Computation (stat.CO)
This work develops algorithms for non-parametric confidence regions for samples from a univariate distribution whose support is a discrete mesh bounded on the left. We generalize the theory of Learned-Miller to preorders over the sample space. In this context, we show that the lexicographic low and lexicographic high orders are in some way extremal in the class of monotone preorders. From this theory we derive several approximation algorithms: 1) Closed form approximations for the lexicographic low and high orders with error tending to zero in the mesh size; 2) A polynomial-time approximation scheme for quantile orders with error tending to zero in the mesh size; 3) Monte Carlo methods for calculating quantile and lexicographic low orders applicable to any mesh size.
[54]
arXiv:2508.10357
(replaced)
[pdf, html, other]
Title:
Efficient Inference for Time-to-Event Outcomes by Integrating Right-Censored and Current Status Data
Xiudi Li, Sijia Li
Subjects:
Methodology (stat.ME)
We propose a semiparametric data fusion framework for efficient inference on survival probabilities by integrating right-censored and current status data. Existing data fusion methods focus largely on fusing right-censored data only, while standard meta-analysis approaches are inadequate for combining right-censored and current status data, as estimators based on current status data alone typically converge at slower rates and have non-normal limiting distributions. In this work, we consider a semiparametric model under exchangeable event time distribution across data sources. We derive the canonical gradient of the survival probability at a given time, and develop one-step estimators along with the corresponding inference procedure. Specifically, we propose a doubly robust estimator and an efficient estimator that attains the semiparametric efficiency bound under mild conditions. Importantly, we show that incorporating current status data can lead to meaningful efficiency gains despite the slower convergence rate of current status-only estimators. We demonstrate the performance of our proposed method in simulations and discuss extensions to settings with covariate shift. We believe that this work has the potential to open new directions in data fusion methodology, particularly for settings involving mixed censoring types.
[55]
arXiv:2509.03853
(replaced)
[pdf, html, other]
Title:
Simulation-based Inference via Langevin Dynamics with Score Matching
Haoyu Jiang, Yuexi Wang, Yun Yang
Subjects:
Methodology (stat.ME); Computation (stat.CO); Machine Learning (stat.ML)
Simulation-based inference (SBI) enables Bayesian analysis when the likelihood is intractable but model simulations are available. Recent advances in statistics and machine learning, including Approximate Bayesian Computation and deep generative models, have expanded the applicability of SBI, yet these methods often face challenges in moderate to high-dimensional parameter spaces. Motivated by the success of gradient-based Monte Carlo methods in Bayesian sampling, we propose a novel SBI method that integrates score matching with Langevin dynamics to explore complex posterior landscapes more efficiently in such settings. Our approach introduces tailored score-matching procedures for SBI, including a localization scheme that reduces simulation costs and an architectural regularization that embeds the statistical structure of log-likelihood scores to improve score-matching accuracy. We provide theoretical analysis of the method and illustrate its practical benefits on benchmark tasks and on more challenging problems in moderate to high dimensions, where it performs favorably compared to existing approaches.
[56]
arXiv:2509.04603
(replaced)
[pdf, html, other]
Title:
DRtool: An Interactive Tool for Analyzing High-Dimensional Clusterings
Justin Lin, Julia Fukuyama
Comments:
34 pages, 12 figures
Subjects:
Applications (stat.AP); Machine Learning (cs.LG)
Technological advances have spurred an increase in data complexity and dimensionality. We are now in an era in which data sets containing thousands of features are commonplace. To digest and analyze such high-dimensional data, dimension reduction techniques have been developed and advanced along with computational power. Of these techniques, nonlinear methods are most commonly employed because of their ability to construct visually interpretable embeddings. Unlike linear methods, these methods non-uniformly stretch and shrink space to create a visual impression of the high-dimensional data. Since capturing high-dimensional structures in a significantly lower number of dimensions requires drastic manipulation of space, nonlinear dimension reduction methods are known to occasionally produce false structures, especially in noisy settings. In an effort to deal with this phenomenon, we developed an interactive tool that enables analysts to better understand and diagnose their dimension reduction results. It uses various analytical plots to provide a multi-faceted perspective on results to determine legitimacy. The tool is available via an R package named DRtool.
[57]
arXiv:2509.07147
(replaced)
[pdf, other]
Title:
On the Ambiguities of Incompatibility in Frequentist Inference
Alessandro Rovetta
Subjects:
Other Statistics (stat.OT)
The interpretation of the P-value and its monotone transform s=-log2(p), or S-value, remains debated despite decades of dedicated literature. Within the neo-Fisherian framework, these values are often described as indices of (in)compatibility between the observed data and a set of ideal assumptions (i.e., the statistical model). In this regard, this paper proposes the distinction between two domains: the model domain, where assumptions are taken as perfectly true and every admissible outcome is, by construction, fully compatible with the model; and the real domain, where assumptions may fail and face empirical scrutiny. I argue that, although interpreted through an objective numerical index, any level of incompatibility can arise only in the latter domain, where the epistemic status of the model under examination is uncertain and a genuine conflict between data and hypotheses can therefore occur. The extent to which P- and S-values are taken as indicating incompatibility is a matter of contextual judgment. Within this framework, descriptive approaches serve to quantify the numerical values of P and S; these can be interpreted as indicative of a certain degree (or amount) of incompatibility between data and hypotheses once causal knowledge of the data-generating process and information about the costs and benefits of related decisions become clearer. Although the distinction between the model domain and the real domain may appear merely theoretical or even philosophical, I argue that this perspective is useful for developing a clear mental representation of how statistical estimates should be evaluated in practical settings and applications.
[58]
arXiv:2203.11820
(replaced)
[pdf, html, other]
Title:
Dealing with Logs and Zeros in Regression Models
Christophe Bellégo, David Benatia, Louis Pape
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
The log transformation is widely used in linear regression, mainly because coefficients are interpretable as proportional effects. Yet this practice has fundamental limitations, most notably that the log is undefined at zero, creating an identification problem. We propose a new estimator, iterated OLS (iOLS), which targets the normalized average treatment effect, preserving the percentage-change interpretation while addressing these limitations. Our procedure is the theoretically justified analogue of the ad-hoc log(1+Y) transformation and delivers a consistent and asymptotically normal estimator of the parameters of the exponential conditional mean model. iOLS is computationally efficient, globally convergent, and free of the incidental-parameter bias, while extending naturally to endogenous regressors through iterated 2SLS. We illustrate the methods with simulations and revisit three influential publications.
[59]
arXiv:2210.13533
(replaced)
[pdf, html, other]
Title:
Sufficient Invariant Learning for Distribution Shift
Taero Kim, Subeen Park, Sungjun Lim, Yonghan Jung, Krikamol Muandet, Kyungwoo Song
Comments:
Accepted by CVPR 2025. Corresponding author: Kyungwoo Song
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Learning robust models under distribution shifts between training and test datasets is a fundamental challenge in machine learning. While learning invariant features across environments is a popular approach, it often assumes that these features are fully observed in both training and test sets, a condition frequently violated in practice. When models rely on invariant features absent in the test set, their robustness in new environments can deteriorate. To tackle this problem, we introduce a novel learning principle called the Sufficient Invariant Learning (SIL) framework, which focuses on learning a sufficient subset of invariant features rather than relying on a single feature. After demonstrating the limitation of existing invariant learning methods, we propose a new algorithm, Adaptive Sharpness-aware Group Distributionally Robust Optimization (ASGDRO), to learn diverse invariant features by seeking common flat minima across the environments. We theoretically demonstrate that finding a common flat minima enables robust predictions based on diverse invariant features. Empirical evaluations on multiple datasets, including our new benchmark, confirm ASGDRO's robustness against distribution shifts, highlighting the limitations of existing methods.
[60]
arXiv:2303.13865
(replaced)
[pdf, html, other]
Title:
Compositionality in algorithms for smoothing
Moritz Schauer, Frank van der Meulen, Andi Q. Wang
Subjects:
Category Theory (math.CT); Methodology (stat.ME)
Backward Filtering Forward Guiding (BFFG) is a bidirectional algorithm proposed in Mider et al. [2021] and studied more in depth in a general setting in Van der Meulen and Schauer [2022]. In category theory, optics have been proposed for modelling systems with bidirectional data flow. We connect BFFG with optics by demonstrating that the forward and backwards map together define a functor from a category of Markov kernels into a category of optics, which is furthermore lax monoidal in the case when the guiding kernels coincide with the generative dynamics
[61]
arXiv:2402.02734
(replaced)
[pdf, html, other]
Title:
Integrative Variational Autoencoders for Generative Modeling of an Image Outcome with Multiple Input Images
Bowen Lei, Yeseul Jeon, Rajarshi Guhaniyogi, Aaron Scheffler, Bani Mallick, Alzheimer's Disease Neuroimaging Initiatives
Subjects:
Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE); Applications (stat.AP); Machine Learning (stat.ML)
Understanding relationships across multiple imaging modalities is central to neuroimaging research. We introduce the Integrative Variational Autoencoder (InVA), the first hierarchical VAE framework for image-on-image regression in multimodal neuroimaging. Unlike standard VAEs, which are not designed for predictive integration across modalities, InVA models outcome images as functions of both shared and modality-specific features. This flexible, data-driven approach avoids rigid assumptions of classical tensor regression and outperforms conventional VAEs and nonlinear models such as BART. As a key application, InVA accurately predicts costly PET scans from structural MRI, offering an efficient and powerful tool for multimodal neuroimaging.
[62]
arXiv:2406.17002
(replaced)
[pdf, other]
Title:
Deep Survival Analysis from Adult and Pediatric Electrocardiograms: A Multi-center Benchmark Study
Platon Lukyanenko, Joshua Mayourian, Mingxuan Liu, John K. Triedman, Sunil J. Ghelani, William G. La Cava
Comments:
16 pages plus appendix
Subjects:
Signal Processing (eess.SP); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Applications (stat.AP)
Artificial intelligence applied to electrocardiography (AI-ECG) shows potential for mortality prediction, but heterogeneous approaches and private datasets have limited generalizable insights. To address this, we systematically evaluated model design choices across three large cohorts: Beth Israel Deaconess (MIMIC-IV: n = 795,546 ECGs, United States), Telehealth Network of Minas Gerais (Code-15: n = 345,779, Brazil), and Boston Children's Hospital (BCH: n = 255,379, United States). We evaluated models predicting all-cause mortality, comparing horizon-based classification and deep survival methods with neural architectures including convolutional networks and transformers, benchmarking against demographic-only and gradient boosting baselines. Top models performed well (median concordance: Code-15, 0.83; MIMIC-IV, 0.78; BCH, 0.81). Incorporating age and sex improved performance across all datasets. Classifier-Cox models showed site-dependent sensitivity to horizon choice (median Pearson's R: Code-15, 0.35; MIMIC-IV, -0.71; BCH, 0.37). External validation reduced concordance, and in some cases demographic-only models outperformed externally trained AI-ECG models on Code-15. However, models trained on multi-site data outperformed site-specific models by 5-22%. Findings highlight factors for robust AI-ECG deployment: deep survival methods outperformed horizon-based classifiers, demographic covariates improved predictive performance, classifier-based models required site-specific calibration, and cross-cohort training, even between adult and pediatric cohorts, substantially improved performance. These results emphasize the importance of model type, demographics, and training diversity in developing AI-ECG models reliably applicable across populations.
[63]
arXiv:2408.16115
(replaced)
[pdf, html, other]
Title:
Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations
Richard Bergna, Sergio Calvo-Ordoñez, Felix L. Opolka, Pietro Liò, Jose Miguel Hernandez-Lobato
Comments:
Accepted at ICLR 2025 as Spotlight. 18 pages including appendix
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
We propose a novel Stochastic Differential Equation (SDE) framework to address the problem of learning uncertainty-aware representations for graph-structured data. While Graph Neural Ordinary Differential Equations (GNODEs) have shown promise in learning node representations, they lack the ability to quantify uncertainty. To address this, we introduce Latent Graph Neural Stochastic Differential Equations (LGNSDE), which enhance GNODE by embedding randomness through a Bayesian prior-posterior mechanism for epistemic uncertainty and Brownian motion for aleatoric uncertainty. By leveraging the existence and uniqueness of solutions to graph-based SDEs, we prove that the variance of the latent space bounds the variance of model outputs, thereby providing theoretically sensible guarantees for the uncertainty estimates. Furthermore, we show mathematically that LGNSDEs are robust to small perturbations in the input, maintaining stability over time. Empirical results across several benchmarks demonstrate that our framework is competitive in out-of-distribution detection, robustness to noise, and active learning, underscoring the ability of LGNSDEs to quantify uncertainty reliably. Code is available at \href{this https URL}{\texttt{this http URL}}.
[64]
arXiv:2504.20586
(replaced)
[pdf, html, other]
Title:
Faster Random Walk-based Capacitance Extraction with Generalized Antithetic Sampling
Periklis Liaskovitis, Marios Visvardis, Efthymios Efstathiou
Subjects:
Computational Physics (physics.comp-ph); Computational Engineering, Finance, and Science (cs.CE); Applications (stat.AP)
Floating random walk-based capacitance extraction has emerged in recent years as a tried and true approach for extracting parasitic capacitance in very large scale integrated circuits. Being a Monte Carlo method, its performance is dependent on the variance of sampled quantities and variance reduction methods are crucial for the challenges posed by ever denser process technologies and layout-dependent effects. In this work, we present a novel, universal variance reduction method for floating random walk-based capacitance extraction, which is conceptually simple, highly efficient and provably reduces variance in all extractions, especially when layout-dependent effects are present. It is complementary to existing mathematical formulations for variance reduction and its performance gains are experienced on top of theirs. Numerical experiments demonstrate substantial such gains of up to 50% in number of walks necessary as well as in actual extraction times compared to the best previously proposed variance reduction approaches for the floating random-walk.
[65]
arXiv:2508.13355
(replaced)
[pdf, html, other]
Title:
Counterfactual Probabilistic Diffusion with Expert Models
Wenhao Mu, Zhi Cao, Mehmed Uludag, Alexander Rodríguez
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Methodology (stat.ME)
Predicting counterfactual distributions in complex dynamical systems is essential for scientific modeling and decision-making in domains such as public health and medicine. However, existing methods often rely on point estimates or purely data-driven models, which tend to falter under data scarcity. We propose a time series diffusion-based framework that incorporates guidance from imperfect expert models by extracting high-level signals to serve as structured priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and data-driven approaches, enabling more reliable and interpretable causal inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations, synthetic pharmacological dynamics, and real-world case studies, demonstrating that it consistently outperforms strong baselines in both point prediction and distributional accuracy.
[66]
arXiv:2508.19468
(replaced)
[pdf, html, other]
Title:
Weighted Levenberg-Marquardt methods for fitting multichannel nuclear cross section data
M. Imbrišak, A. E. Lovell, M. R. Mumpower
Comments:
14 pages, 10 figures, changed email address
Subjects:
Nuclear Theory (nucl-th); Machine Learning (stat.ML)
We present an extension of the Levenberg-Marquardt algorithm for fitting multichannel nuclear cross section data. Our approach offers a practical and robust alternative to conventional trust-region methods for analyzing experimental data. The CoH$_3$ code, based on the Hauser-Feshbach statistical model, involves a large number of interdependent parameters, making optimization challenging due to the presence of "sloppy" directions in parameter space. To address the uneven distribution of experimental data across reaction channels, we construct a weighted Fisher Information Metric by integrating prior distributions over dataset weights. This framework enables a more balanced treatment of heterogeneous data, improving both parameter estimation and convergence robustness. We show that the resulting weighted Levenberg-Marquardt method yields more physically consistent fits for both raw and smoothed datasets, using experimental data for ${}^{148}$Sm as a representative example. Additionally, we introduce a geometric scaling strategy to accelerate convergence -- a method based on the local geometry of the manifold.
[67]
arXiv:2509.08765
(replaced)
[pdf, html, other]
Title:
PCGBandit: One-shot acceleration of transient PDE solvers via online-learned preconditioners
Mikhail Khodak, Min Ki Jung, Brian Wynne, Edmond Chow, Egemen Kolemen
Comments:
code available at this https URL
Subjects:
Computational Physics (physics.comp-ph); Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML)
Data-driven acceleration of scientific computing workflows has been a high-profile aim of machine learning (ML) for science, with numerical simulation of transient partial differential equations (PDEs) being one of the main applications. The focus thus far has been on methods that require classical simulations to train, which when combined with the data-hungriness and optimization challenges of neural networks has caused difficulties in demonstrating a convincing advantage against strong classical baselines. We consider an alternative paradigm in which the learner uses a classical solver's own data to accelerate it, enabling a one-shot speedup of the simulation. Concretely, since transient PDEs often require solving a sequence of related linear systems, the feedback from repeated calls to a linear solver such as preconditioned conjugate gradient (PCG) can be used by a bandit algorithm to online-learn an adaptive sequence of solver configurations (e.g. preconditioners). The method we develop, PCGBandit, is implemented directly on top of the popular open source software OpenFOAM, which we use to show its effectiveness on a set of fluid and magnetohydrodynamics (MHD) problems.
[68]
arXiv:2509.08926
(replaced)
[pdf, html, other]
Title:
Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures
Waqar Ahmad, Evan Murphy, Vladimir A. Krylov
Subjects:
Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
Object re-identification (Re-ID) methods are highly sensitive to label noise, which typically leads to significant performance degradation. We address this challenge by reframing Re-ID as a supervised image similarity task and adopting a Siamese network architecture trained to capture discriminative pairwise relationships. Central to our approach is a novel statistical outlier detection (OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier Detection), which models the distribution of cosine similarities between embedding pairs using a two-component Beta distribution mixture model. We establish a novel identifiability result for mixtures of two Beta distributions, ensuring that our learning task is well-posed. The proposed OD step complements the Re-ID architecture combining binary cross-entropy, contrastive, and cosine embedding losses that jointly optimize feature-level similarity this http URL demonstrate the effectiveness of Beta-SOD in de-noising and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance compared to the state-of-the-art methods across various noise levels (10-30\%), demonstrating both robustness and broad applicability in noisy Re-ID scenarios. The implementation of Beta-SOD is available at: this http URL
Total of 68 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack