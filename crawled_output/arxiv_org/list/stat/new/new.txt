Statistics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
stat
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Statistics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Friday, 19 September 2025
Total of 63 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 18 of 18 entries)
[1]
arXiv:2509.14245
[pdf, html, other]
Title:
A Bayesian thinning algorithm for the point source identification of heat equation
Zhiliang Deng, Chen Li, Xiaomei Yang
Comments:
6 pages
Subjects:
Computation (stat.CO); Numerical Analysis (math.NA)
In this work, we propose a Bayesian thinning algorithm for recovering weighted point source functions in the heat equation from boundary flux observations. The major challenge in the classical Bayesian framework lies in constructing suitable priors for such highly structured unknowns. To address this, we introduce a level set representation on a discretized mesh for the unknown, which enables the infinite-dimensional Bayesian framework to the reconstruction. From another perspective, the point source configuration can be modeled as a marked Poisson point process (PPP), then a thinning mechanism is employed to selectively retain points. These two proposals are complementary with the Bayesian level set sampling generating candidate point sources and the thinning process acting as a filter to refine them. This combined framework is validated through numerical experiments, which demonstrate its accuracy in reconstructing point sources.
[2]
arXiv:2509.14387
[pdf, html, other]
Title:
Environmental Risk Assessment via Nonhomogeneous Hidden Semi-Markov Models with Penalized Vector Auto-Regression
Marco Mingione, Pierfrancesco Alaimo Di Loro, Francesco Lagona, Antonello Maruotti
Subjects:
Methodology (stat.ME)
Motivated by the study of pollution trends in the city of Bergen, we introduce a flexible statistical framework for modeling multivariate air pollution data via a nonhomogeneous Hidden Semi-Markov Vector Auto-Regression. The hidden process captures unobserved environmental conditions, while the vector autoregressive structure accounts for temporal autocorrelation and cross-pollutant dependencies. The model further allows time-varying environmental conditions to influence both the average levels of pollutant concentrations and the duration of different transient states. Parameters are estimated via maximum likelihood using a tailored Expectation-Maximization (EM) algorithm, integrated with state-specific $\ell_1$ regularization to control overfitting and automatically select relevant temporal lags. The proposal is tested on simulated data under different scenarios and then applied to daily concentrations of nitrogens and particulate matter recorded in a urban area. Environmental risk is assessed by a Shapley value-based decomposition that attribute marginal risk contributions. This approach offers a comprehensive framework for multivariate environmental risk modeling, enabling better identification of high-pollution episodes and informing policy interventions.
[3]
arXiv:2509.14428
[pdf, html, other]
Title:
A Scalable Formula for the Moments of a Family of Self-Normalized Statistics
Haolin Zou, Heyuan Yao, Victor de la Pe√±a
Subjects:
Statistics Theory (math.ST); Computation (stat.CO)
Following the student t-statistic, normalization has been a widely used method in statistic and other disciplines including economics, ecology and machine learning. We focus on statistics taking the form of a ratio over (some power of) the sample mean, the probabilistic features of which remain unknown. We develop a unified formula for the moments of these self-normalized statistics with non-negative observations, yielding closed-form expressions for several important cases. Moreover, the complexity of our formula doesn't scale with the sample size $n$. Our theoretical findings, supported by extensive numerical experiments, reveal novel insights into their bias and variance, and we propose a debiasing method illustrated with applications such as the odds ratio, Gini coefficient and squared coefficient of variation.
[4]
arXiv:2509.14502
[pdf, html, other]
Title:
Rate doubly robust estimation for weighted average treatment effects
Yiming Wang, Yi Liu, Shu Yang
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST); Machine Learning (stat.ML)
The weighted average treatment effect (WATE) defines a versatile class of causal estimands for populations characterized by propensity score weights, including the average treatment effect (ATE), treatment effect on the treated (ATT), on controls (ATC), and for the overlap population (ATO). WATE has broad applicability in social and medical research, as many datasets from these fields align with its framework. However, the literature lacks a systematic investigation into the robustness and efficiency conditions for WATE estimation. Although doubly robust (DR) estimators are well-studied for ATE, their applicability to other WATEs remains uncertain. This paper investigates whether widely used WATEs admit DR or rate doubly robust (RDR) estimators and assesses the role of nuisance function accuracy, particularly with machine learning. Using semiparametric efficient influence function (EIF) theory and double/debiased machine learning (DML), we propose three RDR estimators under specific rate and regularity conditions and evaluate their performance via Monte Carlo simulations. Applications to NHANES data on smoking and blood lead levels, and SIPP data on 401(k) eligibility, demonstrate the methods' practical relevance in medical and social sciences.
[5]
arXiv:2509.14520
[pdf, html, other]
Title:
A Review of Statistical Methods for Handling Nonignorable Missing Data using Instrument Approach
Yujie Zhao
Subjects:
Methodology (stat.ME)
Nonignorable missing data, where the probability of missingness depends on unobserved values, presents a significant challenge in statistical analysis. Traditional methods often rely on strong parametric assumptions that are difficult to verify and may lead to biased estimates if misspecified. Recent advances have introduced the concept of a nonresponse instrument or shadow variable as a powerful tool to enhance model identifiability without requiring full parametric specification. This paper provides a comprehensive review of statistical methods that leverage instrumental variables to address nonignorable missingness, focusing on two predominant semiparametric frameworks: one with a parametric data model and a nonparametric propensity model, and the other with a parametric propensity model and a nonparametric data model. We discuss key developments, methodological insights, and remaining challenges in this rapidly evolving field.
[6]
arXiv:2509.14522
[pdf, html, other]
Title:
Semiparametric Learning from Open-Set Label Shift Data
Siyan Liu, Yukun Liu, Qinglong Tian, Pengfei Li, Jing Qin
Subjects:
Methodology (stat.ME); Machine Learning (stat.ML)
We study the open-set label shift problem, where the test data may include a novel class absent from training. This setting is challenging because both the class proportions and the distribution of the novel class are not identifiable without extra assumptions. Existing approaches often rely on restrictive separability conditions, prior knowledge, or computationally infeasible procedures, and some may lack theoretical guarantees. We propose a semiparametric density ratio model framework that ensures identifiability while allowing overlap between novel and known classes. Within this framework, we develop maximum empirical likelihood estimators and confidence intervals for class proportions, establish their asymptotic validity, and design a stable Expectation-Maximization algorithm for computation. We further construct an approximately optimal classifier based on posterior probabilities with theoretical guarantees. Simulations and a real data application confirm that our methods improve both estimation accuracy and classification performance compared with existing approaches.
[7]
arXiv:2509.14598
[pdf, html, other]
Title:
Randomization inference for stepped-wedge designs with noncompliance with application to a palliative care pragmatic trial
Jeffrey Zhang, Zhe Chen, Katherine R. Courtright, Scott D. Halpern, Michael O. Harhay, Dylan S. Small, Fan Li
Subjects:
Methodology (stat.ME); Applications (stat.AP)
While palliative care is increasingly commonly delivered to hospitalized patients with serious illnesses, few studies have estimated its causal effects. Courtright et al. (2016) adopted a cluster-randomized stepped-wedge design to assess the effect of palliative care on a patient-centered outcome. The randomized intervention was a nudge to administer palliative care but did not guarantee receipt of palliative care, resulting in noncompliance (compliance rate ~30%). A subsequent analysis using methods suited for standard trial designs produced statistically anomalous results, as an intention-to-treat analysis found no effect while an instrumental variable analysis did (Courtright et al., 2024). This highlights the need for a more principled approach to address noncompliance in stepped-wedge designs. We provide a formal causal inference framework for the stepped-wedge design with noncompliance by introducing a relevant causal estimand and corresponding estimators and inferential procedures. Through simulation, we compare an array of estimators across a range of stepped-wedge designs and provide practical guidance in choosing an analysis method. Finally, we apply our recommended methods to reanalyze the trial of Courtright et al. (2016), producing point estimates suggesting a larger effect than the original analysis of (Courtright et al., 2024), but intervals that did not reach statistical significance.
[8]
arXiv:2509.14805
[pdf, html, other]
Title:
Forecasting in small open emerging economies Evidence from Thailand
Paponpat Taveeapiradeecharoen, Nattapol Aunsri
Subjects:
Applications (stat.AP); Econometrics (econ.EM)
Forecasting inflation in small open economies is difficult because limited time series and strong external exposures create an imbalance between few observations and many potential predictors. We study this challenge using Thailand as a representative case, combining more than 450 domestic and international indicators. We evaluate modern Bayesian shrinkage and factor models, including Horseshoe regressions, factor-augmented autoregressions, factor-augmented VARs, dynamic factor models, and Bayesian additive regression trees.
Our results show that factor models dominate at short horizons, when global shocks and exchange rate movements drive inflation, while shrinkage-based regressions perform best at longer horizons. These models not only improve point and density forecasts but also enhance tail-risk performance at the one-year horizon.
Shrinkage diagnostics, on the other hand, additionally reveal that Google Trends variables, especially those related to food essential goods and housing costs, progressively rotate into predictive importance as the horizon lengthens. This underscores their role as forward-looking indicators of household inflation expectations in small open economies.
[9]
arXiv:2509.14961
[pdf, html, other]
Title:
Towards universal property prediction in Cartesian space: TACE is all you need
Zemin Xu, Wenbo Xie, Daiqian Xie, P. Hu
Subjects:
Machine Learning (stat.ML); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)
Machine learning has revolutionized atomistic simulations and materials science, yet current approaches often depend on spherical-harmonic representations. Here we introduce the Tensor Atomic Cluster Expansion and Tensor Moment Potential, the first unified framework formulated entirely in Cartesian space for the systematic prediction of arbitrary structure-determined tensorial properties. TACE achieves this by decomposing atomic environments into a complete hierarchy of (irreducible) Cartesian tensors, ensuring symmetry-consistent representations that naturally encode invariance and equivariance constraints. Beyond geometry, TACE incorporates universal embeddings that flexibly integrate diverse attributes including basis sets, charges, magnetic moments and field perturbations. This allows explicit control over external invariants and equivariants in the prediction process. Long-range interactions are also accurately described through the Latent Ewald Summation module within the short-range approximation, providing a rigorous yet computationally efficient treatment of electrostatic interactions. We demonstrate that TACE attains accuracy, stability, and efficiency on par with or surpassing leading equivariant frameworks across finite molecules and extended materials, including in-domain and out-of-domain benchmarks, spectra, hessians, external-field response, charged systems, magnetic systems, multi-fidelity training, and heterogeneous catalytic systems. Crucially, TACE bridges scalar and tensorial modeling and establishes a Cartesian-space paradigm that unifies and extends beyond the design space of spherical-harmonic-based methods. This work lays the foundation for a new generation of universal atomistic machine learning models capable of systematically capturing the rich interplay of geometry, fields and material properties within a single coherent framework.
[10]
arXiv:2509.14971
[pdf, html, other]
Title:
Alternative Likelihood Approximations for High-Dimensional Intervals for Lasso
Logan Harris, Patrick Breheny
Comments:
17 pages, 5 figures, 2 tables
Subjects:
Methodology (stat.ME)
Classical frequentist approaches to inference for the lasso emphasize exact coverage for each feature, which requires debiasing and severs the connection between confidence intervals and the original lasso estimates. To address this, in earlier work we introduced the idea of average coverage, allowing for biased intervals that align with the lasso point estimates, and proposed the Relaxed Lasso Posterior (RL-P) intervals, which leverage the Bayesian interpretation of the lasso penalty as a Laplace prior together with a Normal likelihood conditional on the selected features. While RL-P achieves approximate average coverage, its intervals need not contain the lasso estimates. In this work, we propose alternative constructions based on different likelihood approximations to the full high-dimensional likelihood, yielding intervals that remain centered on the lasso estimates while still achieving average coverage. Our results continue to demonstrate that intentionally biased intervals provide a principled and practically useful framework for inference in high-dimensional regression.
[11]
arXiv:2509.15023
[pdf, html, other]
Title:
Modelling peaks over thresholds in panel data: a two-level grouped panel generalized Pareto regression
Zefan Liu, Natalia Nolde
Subjects:
Methodology (stat.ME)
Panel data arise in a wide range of application areas, and developing modelling methods for extreme values under such a setup is essential for reliable risk assessment and management. When choosing to model the marginal distributions of univariate extremes, one may wish to balance the flexibility in capturing the heterogeneity among margins and the efficiency of estimation. This can be achieved through a combination of regression techniques and assuming a latent group structure based on parameter values, which needs to be estimated from data. Building on an existing method, we propose a two-level grouped panel generalized Pareto regression framework, which models peaks over high thresholds in panel data. While retaining the wide applicability of the original modelling strategy, which is largely domain-knowledge-free, our new methodology uses the information of extreme events more exhaustively and allows the exploration of a broader model space, where parsimony and good model fit can be achieved simultaneously. We also address several estimation challenges associated with high-dimensional optimization and group structure identification. The finite-sample performance of our methodology is carefully evaluated through simulation studies. With an application to the summer river flow data from 31 stations in the upper Danube basin, we show that our methodology can effectively improve estimation efficiency while discovering patterns in the tail behavior that can be omitted by domain-knowledge-based regionalization and the existing method.
[12]
arXiv:2509.15127
[pdf, html, other]
Title:
Learning Rate Should Scale Inversely with High-Order Data Moments in High-Dimensional Online Independent Component Analysis
M. Oguzhan Gultekin, Samet Demir, Zafer Dogan
Comments:
MLSP 2025, 6 pages, 3 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We investigate the impact of high-order moments on the learning dynamics of an online Independent Component Analysis (ICA) algorithm under a high-dimensional data model composed of a weighted sum of two non-Gaussian random variables. This model allows precise control of the input moment structure via a weighting parameter. Building on an existing ordinary differential equation (ODE)-based analysis in the high-dimensional limit, we demonstrate that as the high-order moments increase, the algorithm exhibits slower convergence and demands both a lower learning rate and greater initial alignment to achieve informative solutions. Our findings highlight the algorithm's sensitivity to the statistical structure of the input data, particularly its moment characteristics. Furthermore, the ODE framework reveals a critical learning rate threshold necessary for learning when moments approach their maximum. These insights motivate future directions in moment-aware initialization and adaptive learning rate strategies to counteract the degradation in learning speed caused by high non-Gaussianity, thereby enhancing the robustness and efficiency of ICA in complex, high-dimensional settings.
[13]
arXiv:2509.15134
[pdf, other]
Title:
Sequential sample size calculations and learning curves safeguard the robust development of a clinical prediction model for individuals
Amardeep Legha, Joie Ensor, Rebecca Whittle, Lucinda Archer, Ben Van Calster, Evangelia Christodoulou, Kym I.E. Snell, Mohsen Sadatsafavi, Gary S. Collins, Richard D. Riley
Comments:
36 pages, 5 figures
Subjects:
Methodology (stat.ME)
When prospectively developing a new clinical prediction model (CPM), fixed sample size calculations are typically conducted before data collection based on sensible assumptions. But if the assumptions are inaccurate the actual sample size required to develop a reliable model may be very different. To safeguard against this, adaptive sample size approaches have been proposed, based on sequential evaluation of a models predictive performance. Aim: illustrate and extend sequential sample size calculations for CPM development by (i) proposing stopping rules based on minimising uncertainty (instability) and misclassification of individual-level predictions, and (ii) showcasing how it safeguards against inaccurate fixed sample size calculations. Using the sequential approach repeats the pre-defined model development strategy every time a chosen number (e.g., 100) of participants are recruited and adequately followed up. At each stage, CPM performance is evaluated using bootstrapping, leading to prediction and classification stability statistics and plots, alongside optimism-adjusted measures of calibration and discrimination. Our approach is illustrated for development of acute kidney injury using logistic regression CPMs. The fixed sample size calculation, based on perceived sensible assumptions suggests recruiting 342 patients to minimise overfitting; however, the sequential approach reveals that a much larger sample size of 1100 is required to minimise overfitting (targeting population-level stability). If the stopping rule criteria also target small uncertainty and misclassification probability of individual predictions, the sequential approach suggests an even larger sample size (n=1800). Our sequential sample size approach allows users to dynamically monitor individual-level prediction and classification instability and safeguard against using inaccurate assumptions.
[14]
arXiv:2509.15141
[pdf, html, other]
Title:
Benefits of Online Tilted Empirical Risk Minimization: A Case Study of Outlier Detection and Robust Regression
Yigit E. Yildirim, Samet Demir, Zafer Dogan
Comments:
MLSP 2025, 6 pages, 3 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Empirical Risk Minimization (ERM) is a foundational framework for supervised learning but primarily optimizes average-case performance, often neglecting fairness and robustness considerations. Tilted Empirical Risk Minimization (TERM) extends ERM by introducing an exponential tilt hyperparameter $t$ to balance average-case accuracy with worst-case fairness and robustness. However, in online or streaming settings where data arrive one sample at a time, the classical TERM objective degenerates to standard ERM, losing tilt sensitivity. We address this limitation by proposing an online TERM formulation that removes the logarithm from the classical objective, preserving tilt effects without additional computational or memory overhead. This formulation enables a continuous trade-off controlled by $t$, smoothly interpolating between ERM ($t \to 0$), fairness emphasis ($t > 0$), and robustness to outliers ($t < 0$). We empirically validate online TERM on two representative streaming tasks: robust linear regression with adversarial outliers and minority-class detection in binary classification. Our results demonstrate that negative tilting effectively suppresses outlier influence, while positive tilting improves recall with minimal impact on precision, all at per-sample computational cost equivalent to ERM. Online TERM thus recovers the full robustness-fairness spectrum of classical TERM in an efficient single-sample learning regime.
[15]
arXiv:2509.15143
[pdf, html, other]
Title:
Next-Depth Lookahead Tree
Jaeho Lee, Kangjin Kim, Gyeong Taek Lee
Comments:
25 pages, 2 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
This paper proposes the Next-Depth Lookahead Tree (NDLT), a single-tree model designed to improve performance by evaluating node splits not only at the node being optimized but also by evaluating the quality of the next depth level.
[16]
arXiv:2509.15152
[pdf, html, other]
Title:
Asymptotic Study of In-context Learning with Random Transformers through Equivalent Models
Samet Demir, Zafer Dogan
Comments:
MLSP 2025, 6 pages 2 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We study the in-context learning (ICL) capabilities of pretrained Transformers in the setting of nonlinear regression. Specifically, we focus on a random Transformer with a nonlinear MLP head where the first layer is randomly initialized and fixed while the second layer is trained. Furthermore, we consider an asymptotic regime where the context length, input dimension, hidden dimension, number of training tasks, and number of training samples jointly grow. In this setting, we show that the random Transformer behaves equivalent to a finite-degree Hermite polynomial model in terms of ICL error. This equivalence is validated through simulations across varying activation functions, context lengths, hidden layer widths (revealing a double-descent phenomenon), and regularization settings. Our results offer theoretical and empirical insights into when and how MLP layers enhance ICL, and how nonlinearity and over-parameterization influence model performance.
[17]
arXiv:2509.15164
[pdf, html, other]
Title:
Bayesian inference for spatio-temporal hidden Markov models using the exchange algorithm
Daniele Tancini, Riccardo Rastelli, Francesco Bartolucci
Subjects:
Methodology (stat.ME); Computation (stat.CO)
Spatio-temporal hidden Markov models are extremely difficult to estimate because their latent joint distributions are available only in trivial cases. In the estimation phase, these latent distributions are usually substituted with pseudo-distributions, which could affect the estimation results, in particular in the presence of strong dependence between the latent variables. In this work, we propose a spatio-temporal hidden Markov model where the latent process is an extension of the autologistic model. We show how inference can be carried out in a Bayesian framework using an approximate exchange algorithm, which circumvents the impractical calculations of the normalizing constants that arise in the model. Our proposed method leads to a Markov chain Monte Carlo sampler that targets the correct posterior distribution of the model and not a pseudo-posterior. In addition, we develop a new initialization approach for the approximate exchange method, reducing the computational time of the algorithm. An extensive simulation study shows that the approximate exchange algorithm generally outperforms the pseudo-distribution approach, yielding more accurate parameter estimates. Finally, the proposed methodology is applied to a real-world case study analyzing rainfall levels across Italian regions over time.
[18]
arXiv:2509.15197
[pdf, html, other]
Title:
Consistent causal discovery with equal error variances: a least-squares perspective
Anamitra Chaudhuri, Yang Ni, Anirban Bhattacharya
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME); Machine Learning (stat.ML)
We consider the problem of recovering the true causal structure among a set of variables, generated by a linear acyclic structural equation model (SEM) with the error terms being independent and having equal variances. It is well-known that the true underlying directed acyclic graph (DAG) encoding the causal structure is uniquely identifiable under this assumption. In this work, we establish that the sum of minimum expected squared errors for every variable, while predicted by the best linear combination of its parent variables, is minimised if and only if the causal structure is represented by any supergraph of the true DAG. This property is further utilised to design a Bayesian DAG selection method that recovers the true graph consistently.
Cross submissions (showing 10 of 10 entries)
[19]
arXiv:2509.11780
(cross-list from cond-mat.dis-nn)
[pdf, html, other]
Title:
Variational Gaussian Approximation in Replica Analysis of Parametric Models
Takashi Takahashi
Comments:
24 pages, 2 figures
Subjects:
Disordered Systems and Neural Networks (cond-mat.dis-nn); Statistics Theory (math.ST); Machine Learning (stat.ML)
We revisit the replica method for analyzing inference and learning in parametric models, considering situations where the data-generating distribution is unknown or analytically intractable. Instead of assuming idealized distributions to carry out quenched averages analytically, we use a variational Gaussian approximation for the replicated system in grand canonical formalism in which the data average can be deferred and replaced by empirical averages, leading to stationarity conditions that adaptively determine the parameters of the trial Hamiltonian for each dataset. This approach clarifies how fluctuations affect information extraction and connects directly with the results of mathematical statistics or learning theory such as information criteria. As a concrete application, we analyze linear regression and derive learning curves. This includes cases with real-world datasets, where exact replica calculations are not feasible.
[20]
arXiv:2509.14258
(cross-list from physics.soc-ph)
[pdf, other]
Title:
Comprehensive indicators and fine granularity refine density scaling laws in rural-urban systems
Jack Sutton, Quentin S. Hanley, Gerri Mortimore, Ovidiu Bagdasar, Haroldo V. Ribeiro, Thomas Peron, Golnaz Shahtahmassebi, Peter Scriven
Comments:
17 pages, 5 figures, 2 tables
Subjects:
Physics and Society (physics.soc-ph); Applications (stat.AP)
Density scaling laws complement traditional population scaling laws by enabling the analysis of the full range of human settlements and revealing rural-to-urban transitions with breakpoints at consistent population densities. However, previous studies have been constrained by the granularity of rural and urban units, as well as limitations in the quantity and diversity of indicators. This study addresses these gaps by examining Middle Layer Super Output Areas (MSOAs) in England and Wales, incorporating an extensive set of 117 indicators for the year 2021, spanning age, ethnicity, educational attainment, religion, disability, economic activity, mortality, crime, property transactions, and road accidents. Results indicate that the relationship between indicator density and population density is best described by a segmented power-law model with a consistent breakpoint (33 +- 5 persons per hectare) for 92 of the 117 indicators. Additionally, increasing granularity reveals further rural-to-urban transitions not observed at coarser spatial resolutions. Our findings also highlight the influence of population characteristics on scaling exponents, where stratifying dementia and ischaemic heart disease by older age groups (aged 70 and above) significantly affects these exponents, illustrating a protective urban effect.
[21]
arXiv:2509.14444
(cross-list from cs.LG)
[pdf, html, other]
Title:
FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport
Herlock (SeyedAbolfazl)Rahimi, Dionysis Kalogerias
Comments:
5 pages, 1 figure, ICASSP
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Federated Learning (FL) allows distributed model training without sharing raw data, but suffers when client participation is partial. In practice, the distribution of available users (\emph{availability distribution} $q$) rarely aligns with the distribution defining the optimization objective (\emph{importance distribution} $p$), leading to biased and unstable updates under classical FedAvg. We propose \textbf{Fereated AVerage with Optimal Transport (\textbf{FedAVOT})}, which formulates aggregation as a masked optimal transport problem aligning $q$ and $p$. Using Sinkhorn scaling, \textbf{FedAVOT} computes transport-based aggregation weights with provable convergence guarantees. \textbf{FedAVOT} achieves a standard $\mathcal{O}(1/\sqrt{T})$ rate under a nonsmooth convex FL setting, independent of the number of participating users per round. Our experiments confirm drastically improved performance compared to FedAvg across heterogeneous, fairness-sensitive, and low-availability regimes, even when only two clients participate per round.
[22]
arXiv:2509.14498
(cross-list from cond-mat.stat-mech)
[pdf, html, other]
Title:
Data coarse graining can improve model performance
Alex Nguyen, David J. Schwab, Vudtiwat Ngampruetikorn
Comments:
7 pages, 4 figures
Subjects:
Statistical Mechanics (cond-mat.stat-mech); Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)
Lossy data transformations by definition lose information. Yet, in modern machine learning, methods like data pruning and lossy data augmentation can help improve generalization performance. We study this paradox using a solvable model of high-dimensional, ridge-regularized linear regression under 'data coarse graining.' Inspired by the renormalization group in statistical physics, we analyze coarse-graining schemes that systematically discard features based on their relevance to the learning task. Our results reveal a nonmonotonic dependence of the prediction risk on the degree of coarse graining. A 'high-pass' scheme--which filters out less relevant, lower-signal features--can help models generalize better. By contrast, a 'low-pass' scheme that integrates out more relevant, higher-signal features is purely detrimental. Crucially, using optimal regularization, we demonstrate that this nonmonotonicity is a distinct effect of data coarse graining and not an artifact of double descent. Our framework offers a clear, analytical explanation for why careful data augmentation works: it strips away less relevant degrees of freedom and isolates more predictive signals. Our results highlight a complex, nonmonotonic risk landscape shaped by the structure of the data, and illustrate how ideas from statistical physics provide a principled lens for understanding modern machine learning phenomena.
[23]
arXiv:2509.14596
(cross-list from math.PR)
[pdf, html, other]
Title:
Efficient Importance Sampling for Wrong Exit Probabilities over Combinatorially Many Rare Regions
Yanglei Song, Georgios Fellouris
Subjects:
Probability (math.PR); Computation (stat.CO)
We consider importance sampling for estimating the probability that a light-tailed $d$-dimensional random walk exits through one of many disjoint rare-event regions before reaching an anticipated target. This problem arises in sequential multiple hypothesis testing, where the number of such regions may grow combinatorially and in some cases exponentially with the dimension. While mixtures over all associated exponential tilts are asymptotically efficient, they become computationally infeasible even for moderate values of $d$. We develop a method for constructing asymptotically efficient mixtures with substantially fewer components by combining optimal tilts for a small number of regions with additional proposals that control variance across a large collection of regions. The approach is applied to the estimation of three probabilities that arise in sequential multiple testing, including a multidimensional extension of Siegmund's classical exit problem, and is supported by both theoretical analysis and numerical experiments.
[24]
arXiv:2509.14827
(cross-list from cs.CV)
[pdf, html, other]
Title:
Template-Based Cortical Surface Reconstruction with Minimal Energy Deformation
Patrick Madlindl, Fabian Bongratz, Christian Wachinger
Subjects:
Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)
Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI) is fundamental to neuroimage analysis, enabling morphological studies of the cerebral cortex and functional brain mapping. Recent advances in learning-based CSR have dramatically accelerated processing, allowing for reconstructions through the deformation of anatomical templates within seconds. However, ensuring the learned deformations are optimal in terms of deformation energy and consistent across training runs remains a particular challenge. In this work, we design a Minimal Energy Deformation (MED) loss, acting as a regularizer on the deformation trajectories and complementing the widely used Chamfer distance in CSR. We incorporate it into the recent V2C-Flow model and demonstrate considerable improvements in previously neglected training consistency and reproducibility without harming reconstruction accuracy and topological correctness.
[25]
arXiv:2509.14969
(cross-list from cs.LG)
[pdf, html, other]
Title:
Stochastic Adaptive Gradient Descent Without Descent
Jean-Fran√ßois Aujol, J√©r√©mie Bigot, Camille Castera
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
We introduce a new adaptive step-size strategy for convex optimization with stochastic gradient that exploits the local geometry of the objective function only by means of a first-order stochastic oracle and without any hyper-parameter tuning. The method comes from a theoretically-grounded adaptation of the Adaptive Gradient Descent Without Descent method to the stochastic setting. We prove the convergence of stochastic gradient descent with our step-size under various assumptions, and we show that it empirically competes against tuned baselines.
[26]
arXiv:2509.15058
(cross-list from cs.LG)
[pdf, html, other]
Title:
Communication Efficient Split Learning of ViTs with Attention-based Double Compression
Federico Alvetreti, Jary Pomponi, Paolo Di Lorenzo, Simone Scardapane
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)
This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during the SL training process. ADC incorporates two parallel compression strategies. The first one merges samples' activations that are similar, based on the average attention score calculated in the last client layer; this strategy is class-agnostic, meaning that it can also merge samples having different classes, without losing generalization ability nor decreasing final results. The second strategy follows the first and discards the least meaningful tokens, further reducing the communication cost. Combining these strategies not only allows for sending less during the forward pass, but also the gradients are naturally compressed, allowing the whole model to be trained without additional tuning or approximations of the gradients. Simulation results demonstrate that Attention-based Double Compression outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.
[27]
arXiv:2509.15060
(cross-list from cs.LG)
[pdf, html, other]
Title:
Probabilistic and nonlinear compressive sensing
Lukas Silvester Barth, Paulo von Petersenn
Subjects:
Machine Learning (cs.LG); Information Theory (cs.IT); Statistics Theory (math.ST); Computation (stat.CO); Machine Learning (stat.ML)
We present a smooth probabilistic reformulation of $\ell_0$ regularized regression that does not require Monte Carlo sampling and allows for the computation of exact gradients, facilitating rapid convergence to local optima of the best subset selection problem. The method drastically improves convergence speed compared to similar Monte Carlo based approaches. Furthermore, we empirically demonstrate that it outperforms compressive sensing algorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and signal-to-noise ratios. The implementation runs efficiently on both CPUs and GPUs and is freely available at this https URL.
We also contribute to research on nonlinear generalizations of compressive sensing by investigating when parameter recovery of a nonlinear teacher network is possible through compression of a student network. Building upon theorems of Fefferman and Markel, we show theoretically that the global optimum in the infinite-data limit enforces recovery up to certain symmetries. For empirical validation, we implement a normal-form algorithm that selects a canonical representative within each symmetry class. However, while compression can help to improve test loss, we find that exact parameter recovery is not even possible up to symmetries. In particular, we observe a surprising rebound effect where teacher and student configurations initially converge but subsequently diverge despite continuous decrease in test loss. These findings indicate fundamental differences between linear and nonlinear compressive sensing.
[28]
arXiv:2509.15198
(cross-list from cs.LG)
[pdf, html, other]
Title:
Explaining deep learning for ECG using time-localized clusters
Ahc√®ne Boubekki, Konstantinos Patlatzoglou, Joseph Barker, Fu Siong Ng, Ant√¥nio H. Ribeiro
Subjects:
Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)
Deep learning has significantly advanced electrocardiogram (ECG) analysis, enabling automatic annotation, disease screening, and prognosis beyond traditional clinical capabilities. However, understanding these models remains a challenge, limiting interpretation and gaining knowledge from these developments. In this work, we propose a novel interpretability method for convolutional neural networks applied to ECG analysis. Our approach extracts time-localized clusters from the model's internal representations, segmenting the ECG according to the learned characteristics while quantifying the uncertainty of these representations. This allows us to visualize how different waveform regions contribute to the model's predictions and assess the certainty of its decisions. By providing a structured and interpretable view of deep learning models for ECG, our method enhances trust in AI-driven diagnostics and facilitates the discovery of clinically relevant electrophysiological patterns.
Replacement submissions (showing 35 of 35 entries)
[29]
arXiv:2210.06459
(replaced)
[pdf, html, other]
Title:
Differentially private multivariate medians
Kelly Ramsay, Aukosh Jagannath, Shoja'eddin Chenouri
Comments:
50 pages, 4 figures, 2 tables
Subjects:
Statistics Theory (math.ST); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML)
Statistical tools which satisfy rigorous privacy guarantees are necessary for modern data analysis. It is well-known that robustness against contamination is linked to differential privacy. Despite this fact, using multivariate medians for differentially private and robust multivariate location estimation has not been systematically studied. We develop novel finite-sample performance guarantees for differentially private multivariate depth-based medians, which are essentially sharp. Our results cover commonly used depth functions, such as the halfspace (or Tukey) depth, spatial depth, and the integrated dual depth. We show that under Cauchy marginals, the cost of heavy-tailed location estimation outweighs the cost of privacy. We demonstrate our results numerically using a Gaussian contamination model in dimensions up to $d = 100$, and compare them to a state-of-the-art private mean estimation algorithm. As a by-product of our investigation, we prove concentration inequalities for the output of the exponential mechanism about the maximizer of the population objective function. This bound applies to objective functions that satisfy a mild regularity condition.
[30]
arXiv:2306.07939
(replaced)
[pdf, other]
Title:
Media Bias and Polarization through the Lens of a Markov Switching Latent Space Network Model
Roberto Casarin, Antonio Peruzzi, Mark F.J. Steel
Subjects:
Applications (stat.AP); Social and Information Networks (cs.SI)
News outlets are now more than ever incentivized to provide their audience with slanted news, while the intrinsic homophilic nature of online social media may exacerbate polarized opinions. Here, we propose a new dynamic latent space model for time-varying online audience-duplication networks, which exploits social media content to conduct inference on media bias and polarization of news outlets. We contribute to the literature in several directions: 1) Our model provides a novel measure of media bias that combines information from both network data and text-based indicators; 2) we endow our model with Markov-Switching dynamics to capture polarization regimes while maintaining a parsimonious specification; 3) we contribute to the literature on the statistical properties of latent space network models. The proposed model is applied to a set of data on the online activity of national and local news outlets from four European countries in the years 2015 and 2016. We find evidence of a strong positive correlation between our media slant measure and a well-grounded external source of media bias. In addition, we provide insight into the polarization regimes across the four countries considered.
[31]
arXiv:2306.16043
(replaced)
[pdf, html, other]
Title:
Application of Multivariate Selective Bandwidth Kernel Density Estimation for Data Correction
Hai Bui, Mostafa Bakhoday-Paskyabi
Comments:
16 pages, 6 figures
Subjects:
Applications (stat.AP)
This paper presents an intuitive application of multivariate kernel density estimation (KDE) for data correction. The method utilizes the expected value of the conditional probability density function (PDF) and a credible interval to quantify correction uncertainty. A selective KDE factor is proposed to adjust both kernel size and shape, determined through least-squares cross-validation (LSCV) or mean conditional squared error (MCSE) criteria. The selective bandwidth method can be used in combination with the adaptive method to potentially improve accuracy. Two examples, involving a hypothetical dataset and a realistic dataset, demonstrate the efficacy of the method. The selective bandwidth methods consistently outperform non-selective methods, while the adaptive bandwidth methods improve results for the hypothetical dataset but not for the realistic dataset. The MCSE criterion minimizes root mean square error but may yield under-smoothed distributions, whereas the LSCV criterion strikes a balance between PDF fitness and low RMSE.
[32]
arXiv:2405.06763
(replaced)
[pdf, html, other]
Title:
Post-selection inference for causal effects after causal discovery
Ting-Hsuan Chang, Zijian Guo, Daniel Malinsky
Subjects:
Methodology (stat.ME)
Algorithms for constraint-based causal discovery select graphical causal models among a space of possible candidates (e.g., all directed acyclic graphs) by executing a sequence of conditional independence tests. These may be used to inform the estimation of causal effects (e.g., average treatment effects) when there is uncertainty about which covariates ought to be adjusted for, or which variables act as confounders versus mediators. However, naively using the data twice, for model selection and estimation, would lead to invalid confidence intervals. Moreover, if the selected graph is incorrect, the inferential claims may apply to a selected functional that is distinct from the actual causal effect. We propose an approach to post-selection inference that is based on a resampling and screening procedure, which essentially performs causal discovery multiple times with randomly varying intermediate test statistics. Then, an estimate of the target causal effect and corresponding confidence sets are constructed from a union of individual graph-based estimates and intervals. We show that this construction has asymptotically correct coverage for the true causal effect parameter. Importantly, the guarantee holds for a fixed population-level effect, not a data-dependent or selection-dependent quantity. Most of our exposition focuses on the PC-algorithm for learning directed acyclic graphs and the multivariate Gaussian case for simplicity, but the approach is general and modular, so it may be used with other conditional independence based discovery algorithms and distributional families.
[33]
arXiv:2408.07463
(replaced)
[pdf, html, other]
Title:
A novel framework for quantifying nominal outlyingness
Efthymios Costa, Ioanna Papatsouma
Comments:
22 pages
Subjects:
Methodology (stat.ME)
Outlier detection is an important data mining tool that becomes particularly challenging when dealing with nominal data. First and foremost, flagging observations as outlying requires a well-defined notion of nominal outlyingness. This paper presents a definition of nominal outlyingness and introduces a general framework for quantifying outlyingness of nominal data. The proposed framework makes use of ideas from the association rule mining literature and can be used for calculating scores that indicate how outlying a nominal observation is. Methods for determining the involved hyperparameter values are presented and the concepts of variable contributions and outlyingness depth are introduced, in an attempt to enhance interpretability of the results. The proposed framework is evaluated on both synthetic and publicly available data sets, demonstrating comparable performance to state-of-the-art frequent pattern mining algorithms and even outperforming them in certain cases. The ideas presented can serve as a tool for assessing the degree to which an observation differs from the rest of the data, under the assumption of sequences of nominal levels having been generated from a Multinomial distribution with varying event probabilities.
[34]
arXiv:2408.13235
(replaced)
[pdf, html, other]
Title:
Double Descent: Understanding Linear Model Estimation of Nonidentifiable Parameters and a Model for Overfitting
Ronald Christensen
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)
We consider ordinary least squares estimation and variations on least squares estimation such as penalized (regularized) least squares and spectral shrinkage estimates for problems with p > n and associated problems with prediction of new observations. After the introduction of Section 1, Section 2 examines a number of commonly used estimators for p > n. Section 3 introduces prediction with p > n. Section 4 introduces notational changes to facilitate discussion of overfitting and Section 5 illustrates the phenomenon of double descent. We conclude with some final comments.
[35]
arXiv:2409.10374
(replaced)
[pdf, other]
Title:
Nonlinear Causality in Time Series Networks: With Application to Motor Imagery vs Execution
Sipan Aslan, Hernando Ombao
Subjects:
Applications (stat.AP); Computation (stat.CO); Methodology (stat.ME)
Causal interactions in time series networks can be dynamic and nonlinear, making it difficult to identify them using conventional linear causality estimations. We propose a novel approach, called Threshold Autoregressive Modeling for Causality (TAR4C), a causality detection approach built on threshold autoregressive (TAR) models, where a potential driver (cause variable) acts both as a predictor and as a trigger (switching threshold) that governs which autoregressive process the target (effect variable) follows. Threshold nonlinearity is conceptualized here to determine causality. The flow of the target is forced to transition between regimes with distinct dynamics when the driver exceeds a data-driven threshold in the past. We propose a two-stage inference procedure: Stage 1 tests for threshold connectivity (TC); Stage 2, conditional on a detected threshold effect, estimates threshold Granger causality (TGC). TAR4C is applied to a multichannel EEG dataset collected from a motor imagery and execution experiment. Delay-dependent directional interactions are observed among channels across different sites of the EEG map. The real-world application demonstrates the usefulness of the proposed approach for determining nonlinear causal connectivity in complex time-series networks, such as brain circuitry. The proposed model-based methodology extends to other complex networks of time series.
[36]
arXiv:2410.08080
(replaced)
[pdf, html, other]
Title:
Bayesian Nonparametric Sensitivity Analysis of Multiple Test Procedures Under Dependence
George Karabatsos
Subjects:
Methodology (stat.ME)
This article introduces a sensitivity analysis method for Multiple Testing Procedures (MTPs) using marginal $p$-values. The method is based on the Dirichlet process (DP) prior distribution, specified to support the entire space of MTPs, where each MTP controls either the family-wise error rate (FWER) or the false discovery rate (FDR) under arbitrary dependence between $p$-values. This DP MTP sensitivity analysis method provides uncertainty quantification for MTPs, by accounting for uncertainty in the selection of such MTPs and their respective threshold decisions regarding which number of smallest $p$-values are significant discoveries, from a given set of null hypothesis tested, while measuring each $p$-value's probability of significance over the DP prior predictive distribution of this space of all MTPs, and reducing the possible conservativeness of using only one such MTP for multiple testing. The DP MTP sensitivity analysis method is illustrated through the analysis of over twenty-eight thousand $p$-values arising from hypothesis tests performed on a 2022 dataset of a representative sample of three million U.S. high school students observed on 239 variables. They include tests which, respectively, relate variables about the disruption caused by school closures during the COVID-19 pandemic, with various mathematical cognition, academic achievement, and student background variables. R software code for the DP MTP sensitivity analysis method is provided in the Code and Data Supplement of this article (available upon request).
[37]
arXiv:2410.19031
(replaced)
[pdf, html, other]
Title:
High-dimensional Statistical Inference and Variable Selection Using Sufficient Dimension Association
Shangyuan Ye, Shauna Rakshe, Ye Liang
Subjects:
Methodology (stat.ME)
Simultaneous variable selection and statistical inference is challenging in high-dimensional data analysis. Most existing post-selection inference methods require explicitly specified regression models, which are often linear, as well as sparsity in the regression model. The performance of such procedures can be poor under either misspecified nonlinear models or a violation of the sparsity assumption. In this paper, we propose a sufficient dimension association (SDA) technique that measures the association between each predictor and the response variable conditioning on other predictors in the high-dimensional setting. Our proposed SDA method requires neither a specific form of regression model nor sparsity in the regression. Alternatively, our method assumes normalized or Gaussian-distributed predictors with a Markov blanket property. We propose an estimator for the SDA and prove asymptotic properties for the estimator. We construct three types of test statistics for the SDA and propose a multiple testing procedure to control the false discovery rate. Extensive simulation studies have been conducted to show the validity and superiority of our SDA method. Gene expression data from the Alzheimer Disease Neuroimaging Initiative are used to demonstrate a real application.
[38]
arXiv:2411.05869
(replaced)
[pdf, html, other]
Title:
Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data
Mark D. Risser, Marcus M. Noack, Hengrui Luo, Ronald Pandolfi
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP); Computation (stat.CO); Methodology (stat.ME)
The Gaussian process (GP) is a widely used probabilistic machine learning method with implicit uncertainty characterization for stochastic function approximation, stochastic modeling, and analyzing real-world measurements of nonlinear processes. Traditional implementations of GPs involve stationary kernels (also termed covariance functions) that limit their flexibility, and exact methods for inference that prevent application to data sets with more than about ten thousand points. Modern approaches to address stationarity assumptions generally fail to accommodate large data sets, while all attempts to address scalability focus on approximating the Gaussian likelihood, which can involve subjectivity and lead to inaccuracies. In this work, we explicitly derive an alternative kernel that can discover and encode both sparsity and nonstationarity. We embed the kernel within a fully Bayesian GP model and leverage high-performance computing resources to enable the analysis of massive data sets. We demonstrate the favorable performance of our novel kernel relative to existing exact and approximate GP methods across a variety of synthetic data examples. Furthermore, we conduct space-time prediction based on more than one million measurements of daily maximum temperature and verify that our results outperform state-of-the-art methods in the Earth sciences. More broadly, having access to exact GPs that use ultra-scalable, sparsity-discovering, nonstationary kernels allows GP methods to truly compete with a wide variety of machine learning methods.
[39]
arXiv:2412.03486
(replaced)
[pdf, html, other]
Title:
Tight PAC-Bayesian Risk Certificates for Contrastive Learning
Anna Van Elst, Debarghya Ghoshdastidar
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Contrastive representation learning is a modern paradigm for learning representations of unlabeled data via augmentations -- precisely, contrastive models learn to embed semantically similar pairs of samples (positive pairs) closer than independently drawn samples (negative samples). In spite of its empirical success and widespread use in foundation models, statistical theory for contrastive learning remains less explored. Recent works have developed generalization error bounds for contrastive losses, but the resulting risk certificates are either vacuous (certificates based on Rademacher complexity or $f$-divergence) or require strong assumptions about samples that are unreasonable in practice. The present paper develops non-vacuous PAC-Bayesian risk certificates for contrastive representation learning, considering the practical considerations of the popular SimCLR framework. Notably, we take into account that SimCLR reuses positive pairs of augmented data as negative samples for other data, thereby inducing strong dependence and making classical PAC or PAC-Bayesian bounds inapplicable. We further refine existing bounds on the downstream classification loss by incorporating SimCLR-specific factors, including data augmentation and temperature scaling, and derive risk certificates for the contrastive zero-one risk. The resulting bounds for contrastive loss and downstream prediction are much tighter than those of previous risk certificates, as demonstrated by experiments on CIFAR-10.
[40]
arXiv:2502.02859
(replaced)
[pdf, html, other]
Title:
Gap-Dependent Bounds for Federated $Q$-learning
Haochen Zhang, Zhong Zheng, Lingzhou Xue
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We present the first gap-dependent analysis of regret and communication cost for on-policy federated $Q$-Learning in tabular episodic finite-horizon Markov decision processes (MDPs). Existing FRL methods focus on worst-case scenarios, leading to $\sqrt{T}$-type regret bounds and communication cost bounds with a $\log T$ term scaling with the number of agents $M$, states $S$, and actions $A$, where $T$ is the average total number of steps per agent. In contrast, our novel framework leverages the benign structures of MDPs, such as a strictly positive suboptimality gap, to achieve a $\log T$-type regret bound and a refined communication cost bound that disentangles exploration and exploitation. Our gap-dependent regret bound reveals a distinct multi-agent speedup pattern, and our gap-dependent communication cost bound removes the dependence on $MSA$ from the $\log T$ term. Notably, our gap-dependent communication cost bound also yields a better global switching cost when $M=1$, removing $SA$ from the $\log T$ term.
[41]
arXiv:2503.12044
(replaced)
[pdf, html, other]
Title:
A Simple and Explainable Model for Park-and-Ride Car Park Occupancy Prediction
Andreas Kaltenbrunner, Josep Ferrer, David Moreno, Vicen√ß G√≥mez
Comments:
25 pages, 16 figures, 1 table
Subjects:
Applications (stat.AP); Emerging Technologies (cs.ET); Physics and Society (physics.soc-ph)
In a scenario of growing usage of park-and-ride facilities, understanding and predicting car park occupancy is becoming increasingly important. This study presents a model that effectively captures the occupancy patterns of park-and-ride car parks for commuters using truncated normal distributions for vehicle arrival and departure times. The objective is to develop a predictive model with minimal parameters corresponding to commuter behaviour, enabling the estimation of parking saturation and unfulfilled demand. The proposed model successfully identifies the regular, periodic nature of commuter parking behaviour, where vehicles arrive in the morning and depart in the afternoon. It operates using aggregate data, eliminating the need for individual tracking of arrivals and departures. The model's predictive and now-casting capabilities are demonstrated through real-world data from car parks in the Barcelona Metropolitan Area. A simple model extension furthermore enables the prediction of when a car park will reach its occupancy limit and estimates the additional spaces required to accommodate such excess demand. Thus, beyond forecasting, the model serves as a valuable tool for evaluating interventions, such as expanding parking capacity, to optimize park-and-ride facilities.
[42]
arXiv:2503.21719
(replaced)
[pdf, html, other]
Title:
The Principle of Redundant Reflection
Martin Metodiev, Maarten Marsman, Lourens Waldorp, Quentin F. Gronau, Eric-Jan Wagenmakers
Comments:
11 pages, 0 figures
Subjects:
Methodology (stat.ME); Other Statistics (stat.OT)
The fact that redundant information does not update a rational belief implies that rational beliefs are updated using Bayes rule. In the framework of Hild (1998a), this is true under mild conditions for discrete, continuous, and arbitrary measure spaces. We prove this result and illustrate it with two examples.
[43]
arXiv:2504.19172
(replaced)
[pdf, html, other]
Title:
A new look at fiducial inference
Pier Giovanni Bissiri, Chris Holmes, Stephen Walker
Comments:
24 pages, 6 figures
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
Since the idea of fiducial inference was put forward by Fisher, researchers have been attempting to place it within a rigorous and well motivated framework. It is fair to say that a general definition has remained elusive. In this paper we start with a representation of Bayesian posterior distributions provided by Doob that relies on martingales. This is explicit in defining how a true parameter value should depend on a random sample and hence an approach to "inverse probability" as originally conceived by Fisher. Taking this as our cue, we introduce a definition of fiducial inference that can be regarded as general.
[44]
arXiv:2504.20360
(replaced)
[pdf, html, other]
Title:
Identification and estimation of vaccine effectiveness in the test-negative design under equi-confounding
Christopher B. Boyer, Kendrick Qijun Li, Xu Shi, Eric J. Tchetgen Tchetgen
Subjects:
Methodology (stat.ME)
The test-negative design (TND) is widely used to evaluate vaccine effectiveness in real-world settings. In a TND study, individuals with similar symptoms who seek care are tested, and effectiveness is estimated by comparing vaccination histories of test-positive cases and test-negative controls. The TND is often justified on the grounds that it reduces confounding due to unmeasured health-seeking behavior, although this has not been formally described using potential outcomes. At the same time, concerns persist that conditioning on test receipt can introduce selection bias. We provide a formal justification of the TND under an assumption of odds ratio equi-confounding, where unmeasured confounders affect test-positive and test-negative individuals equivalently on the odds ratio scale. Health-seeking behavior is one plausible example. We also show that these results hold under the outcome-dependent sampling used in TNDs. We discuss the design implications of the equi-confounding assumption and provide alternative estimators for the marginal risk ratio among the vaccinated under equi-confounding, including outcome modeling and inverse probability weighting estimators as well as a semiparametric estimator that is doubly robust. When equi-confounding does not hold, we suggest a straightforward sensitivity analysis that parameterizes the magnitude of the deviation on the odds ratio scale. A simulation study evaluates the empirical performance of our proposed estimators under a wide range of scenarios. Finally, we discuss broader uses of test-negative outcomes to de-bias cohort studies in which testing is triggered by symptoms.
[45]
arXiv:2505.16428
(replaced)
[pdf, html, other]
Title:
Sharp Asymptotic Minimaxity for One-Group Priors in Sparse Normal Means Problem
Sayantan Paul, Prasenjit Ghosh, Arijit Chakrabarti
Subjects:
Statistics Theory (math.ST)
In this paper, we consider the asymptotic properties of the Bayesian multiple testing rules when the mean parameter of the sparse normal means problem is modeled by a broad class of global-local priors, expressed as a scale mixture of normals. We are interested in studying the least possible risk, i.e., the minimax risk for two frequentist losses, one being the usual misclassification (or Hamming) loss, and the other one, measured as the sum of FDR and FNR. Under the betamin separation condition, at first, assuming the level of sparsity to be known, we propose a condition on the global parameter of our chosen class of priors, such that the resultant decision rule attains the minimax risk for both of the losses mentioned above. When the level of sparsity is unknown, we either use an estimate of the global parameter obtained from the data, or propose an absolutely continuous prior on it. For both of the procedures, under some assumption on the unknown level of sparsity, we show that the decision rules also attain the minimax risk, again for both of the losses. Our results also provide a guideline regarding the selection of priors, in the sense that beyond a subclass(horseshoe type priors) of our chosen class of priors, the minimax risk is not achievable with respect to any one of the two loss functions considered in this article. However, the subclass, horseshoe-type priors, is such a large subclass that it contains Horseshoe, Strawderman Berger, standard double Pareto, inverse gamma priors, just to name a few. In this way, along with the most popular BH procedure and approach using spike and slab prior, a multiple testing rule based on one group priors also achieves the optimal boundary. To the best of our knowledge, these are the first results in the literature of global local priors which ensure the optimal minimax risk can be achieved exactly.
[46]
arXiv:2505.19336
(replaced)
[pdf, html, other]
Title:
Model-robust standardization in cluster-randomized trials
Fan Li, Jiaqi Tong, Xi Fang, Chao Cheng, Brennan C. Kahan, Bingkai Wang
Subjects:
Methodology (stat.ME)
In cluster-randomized trials, generalized linear mixed models and generalized estimating equations have conventionally been the default analytic methods for estimating the average treatment effect as routine practice. However, recent studies have demonstrated that their treatment effect coefficient estimators may correspond to ambiguous estimands when the models are misspecified or when there exists informative cluster sizes. In this article, we present a unified approach that standardizes output from a given regression model to ensure estimand-aligned inference for the treatment effect parameters in cluster-randomized trials. We introduce estimators for both the cluster-average and the individual-average treatment effects (marginal estimands) that are always consistent regardless of whether the specified working regression models align with the unknown data generating process. We further explore the use of a deletion-based jackknife variance estimator for inference. The development of our approach also motivates a natural test for informative cluster size. Extensive simulation experiments are designed to demonstrate the advantage of the proposed estimators under a variety of scenarios. The proposed model-robust standardization methods are implemented in the MRStdCRT R package.
[47]
arXiv:2506.22236
(replaced)
[pdf, html, other]
Title:
A Plea for History and Philosophy of Statistics and Machine Learning
Hanti Lin
Subjects:
Other Statistics (stat.OT); Machine Learning (cs.LG)
The integration of the history and philosophy of statistics was initiated at least by Hacking (1975) and advanced by Hacking (1990), Mayo (1996), and Zabell (2005), but it has not received sustained follow-up. Yet such integration is more urgent than ever, as the recent success of artificial intelligence has been driven largely by machine learning -- a field historically developed alongside statistics. Today, the boundary between statistics and machine learning is increasingly blurred. What we now need is integration, twice over: of history and philosophy, and of two fields they engage -- statistics and machine learning. I present a case study of a philosophical idea in machine learning (and in formal epistemology) whose root can be traced back to an often under-appreciated insight in Neyman and Pearson's 1936 work (a follow-up to their 1933 classic). This leads to the articulation of an epistemological principle -- largely implicit in, but shared by, the practices of frequentist statistics and machine learning -- which I call achievabilism: the thesis that the correct standard for assessing non-deductive inference methods should not be fixed, but should instead be sensitive to what is achievable in specific problem contexts. Another integration also emerges at the level of methodology, combining two ends of the philosophy of science spectrum: history and philosophy of science on the one hand, and formal epistemology on the other hand.
[48]
arXiv:2507.09077
(replaced)
[pdf, html, other]
Title:
The Why and How of Convex Clustering
Eric C. Chi, Aaron J. Molstad, Zheming Gao, Jocelyn T. Chi
Subjects:
Methodology (stat.ME); Machine Learning (stat.ML)
This survey reviews a clustering method based on solving a convex optimization problem. Despite the plethora of existing clustering methods, convex clustering has several uncommon features that distinguish it from prior art. The optimization problem is free of spurious local minima, and its unique global minimizer is stable with respect to all its inputs, including the data, a tuning parameter, and weight hyperparameters. Its single tuning parameter controls the number of clusters and can be chosen using standard techniques from penalized regression. We give intuition into the behavior and theory for convex clustering as well as practical guidance. We highlight important algorithms and discuss how their computational costs scale with the problem size. Finally, we highlight the breadth of its uses and flexibility to be combined and integrated with other inferential methods.
[49]
arXiv:2508.04287
(replaced)
[pdf, html, other]
Title:
Parameter Estimation for Weakly Interacting Hypoelliptic Diffusions
Yuga Iguchi, Alexandros Beskos, Grigorios A. Pavliotis
Subjects:
Statistics Theory (math.ST)
We study parameter estimation for interacting particle systems (IPSs) consisting of $N$ weakly interacting multivariate hypoelliptic SDEs. We propose a locally Gaussian approximation of the transition dynamics, carefully designed to address the degenerate structure of the noise (diffusion matrix), thus leading to the formation of a well-defined full likelihood. Our approach permits carrying out statistical inference for a wide class of hypoelliptic IPSs that are not covered by recent works as the latter rely on the Euler-Maruyama scheme. We analyze a contrast estimator based on the developed likelihood with $n$ high-frequency particle observations over a fixed period $[0,T]$ and show its asymptotic normality as $n, N \to \infty$ with a requirement that the step-size $\Delta_n = T/n$ is such that $N\Delta_n\rightarrow 0$, assuming that all particle coordinates (e.g.~position and velocity) are observed. In practical situations where only partial observations (e.g. particle positions but not velocities) are available, the proposed locally Gaussian approximation offers greater flexibility for inference, when combined with established Bayesian techniques. In particular, unlike the Euler-Maruyama-based approaches, we do not have to impose restrictive structures on the hypoelliptic IPSs. We present numerical experiments that illustrate the effectiveness of our approach, both with complete and partial particle observations.
[50]
arXiv:2508.13418
(replaced)
[pdf, html, other]
Title:
Identification and Estimation of Multi-order Tensor Factor Models
Zetai Cen
Comments:
45 pages, 3 figures
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
We propose a novel framework in high-dimensional factor models to simultaneously analyse multiple tensor time series, each with potentially different tensor orders and dimensionality. The connection between different tensor time series is through their global factors that are correlated to each other. A salient feature of our model is that when all tensor time series have the same order, it can be regarded as an extension of multilevel factor models from vectors to general tensors. Under very mild conditions, we separate the global and local components in the proposed model. Parameter estimation is thoroughly discussed, including a consistent factor number estimator. With strong correlation between global factors and noise allowed, we derive the rates of convergence of our estimators, which can be more superior than those of existing methods for multilevel factor models. We also develop estimators that are more computationally efficient, with rates of convergence spelt out. Extensive experiments are performed under various settings, corroborating with the pronounced theoretical results. As a real application example, we analyse a set of taxi data to study the traffic flow between Times Squares and its neighbouring areas.
[51]
arXiv:2509.01774
(replaced)
[pdf, html, other]
Title:
Generalized Correlation Regression for Disentangling Dependence in Clustered Data
Yibo Wang, Chenlei Leng, Cheng Yong Tang
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Clustered and longitudinal data are pervasive in scientific studies, from prenatal health programs to clinical trials and public health surveillance. Such data often involve non-Gaussian responses--including binary, categorical, and count outcomes--that exhibit complex correlation structures driven by multilevel clustering, covariates, over-dispersion, or zero inflation. Conventional approaches such as mixed-effects models and generalized estimating equations (GEEs) can capture some of these dependencies, but they are often too rigid or impose restrictive assumptions that limit interpretability and predictive performance.
We investigate \emph{generalized correlation regression} (GCR), a unified framework that models correlations directly as functions of interpretable covariates while simultaneously estimating marginal means. By applying a generalized $z$-transformation, GCR guarantees valid correlation matrices, accommodates unbalanced cluster sizes, and flexibly incorporates covariates such as time, space, or group membership into the dependence structure. Through applications to modern prenatal care, a longitudinal toenail infection trial, and clustered health count data, we show that GCR not only achieves superior predictive performance over standard methods, but also reveals family-, community-, and individual-level drivers of dependence that are obscured under conventional modeling. These results demonstrate the broad applied value of GCR for analyzing binary, count, and categorical data in clustered and longitudinal settings.
[52]
arXiv:2509.12420
(replaced)
[pdf, html, other]
Title:
System Reliability Estimation via Shrinkage
Beidi Qiang, Edsel Pena
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
In a coherent reliability system composed of multiple components configured according to a specific structure function, the distribution of system time to failure, or system lifetime, is often of primary interest. Accurate estimation of system reliability is critical in a wide range of engineering and industrial applications, forming decisions in system design, maintenance planning, and risk assessment. The system lifetime distribution can be estimated directly using the observed system failure times. However, when component-level lifetime data is available, it can yield improved estimates of system reliability. In this work, we demonstrate that under nonparametric assumptions about the component time-to-failure distributions, traditional estimators such as the Product-Limit Estimator (PLE) can be further improved under specific loss functions. We propose a novel methodology that enhances the nonparametric system reliability estimation through a shrinkage transformation applied to component-level estimators. This shrinkage approach leads to improved efficiency in estimating system reliability.
[53]
arXiv:2409.16311
(replaced)
[pdf, other]
Title:
New Insights into Global Warming: End-to-End Visual Analysis and Prediction of Temperature Variations
Meihua Zhou, Nan Wan, Tianlong Zheng, Hanwen Xu, Li Yang, Tingting Wang
Comments:
28 pages
Subjects:
Atmospheric and Oceanic Physics (physics.ao-ph); Human-Computer Interaction (cs.HC); Applications (stat.AP)
Global warming presents an unprecedented challenge to our planet however comprehensive understanding remains hindered by geographical biases temporal limitations and lack of standardization in existing research. An end to end visual analysis of global warming using three distinct temperature datasets is presented. A baseline adjusted from the Paris Agreements one point five degrees Celsius benchmark based on data analysis is employed. A closed loop design from visualization to prediction and clustering is created using classic models tailored to the characteristics of the data. This approach reduces complexity and eliminates the need for advanced feature engineering. A lightweight convolutional neural network and long short term memory model specifically designed for global temperature change is proposed achieving exceptional accuracy in long term forecasting with a mean squared error of three times ten to the power of negative six and an R squared value of zero point nine nine nine nine. Dynamic time warping and KMeans clustering elucidate national level temperature anomalies and carbon emission patterns. This comprehensive method reveals intricate spatiotemporal characteristics of global temperature variations and provides warming trend attribution. The findings offer new insights into climate change dynamics demonstrating that simplicity and precision can coexist in environmental analysis.
[54]
arXiv:2411.09516
(replaced)
[pdf, html, other]
Title:
Sharp Matrix Empirical Bernstein Inequalities
Hongjian Wang, Aaditya Ramdas
Subjects:
Probability (math.PR); Functional Analysis (math.FA); Statistics Theory (math.ST); Machine Learning (stat.ML)
We present two sharp, closed-form empirical Bernstein inequalities for symmetric random matrices with bounded eigenvalues. By sharp, we mean that both inequalities adapt to the unknown variance in a tight manner: the deviation captured by the first-order $1/\sqrt{n}$ term asymptotically matches the matrix Bernstein inequality exactly, including constants, the latter requiring knowledge of the variance. Our first inequality holds for the sample mean of independent matrices, and our second inequality holds for a mean estimator under martingale dependence at stopping times.
[55]
arXiv:2411.11697
(replaced)
[pdf, html, other]
Title:
Robust Reinforcement Learning under Diffusion Models for Data with Jumps
Chenyang Jiang, Donggyu Kim, Alejandra Quintos, Yazhen Wang
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Reinforcement Learning (RL) has proven effective in solving complex decision-making tasks across various domains, but challenges remain in continuous-time settings, particularly when state dynamics are governed by stochastic differential equations (SDEs) with jump components. In this paper, we address this challenge by introducing the Mean-Square Bipower Variation Error (MSBVE) algorithm, which enhances robustness and convergence in scenarios involving significant stochastic noise and jumps. We first revisit the Mean-Square TD Error (MSTDE) algorithm, commonly used in continuous-time RL, and highlight its limitations in handling jumps in state dynamics. The proposed MSBVE algorithm minimizes the mean-square quadratic variation error, offering improved performance over MSTDE in environments characterized by SDEs with jumps. Simulations and formal proofs demonstrate that the MSBVE algorithm reliably estimates the value function in complex settings, surpassing MSTDE's performance when faced with jump processes. These findings underscore the importance of alternative error metrics to improve the resilience and effectiveness of RL algorithms in continuous-time frameworks.
[56]
arXiv:2502.03048
(replaced)
[pdf, html, other]
Title:
The Ensemble Kalman Update is an Empirical Matheron Update
Dan MacKinlay
Comments:
Under submission
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
The Ensemble Kalman Filter (EnKF) is a widely used method for data assimilation in high-dimensional systems, with an ensemble update step equivalent to an empirical version of the Matheron update popular in Gaussian process regression -- a connection that links half a century of data-assimilation engineering to modern path-wise GP sampling.
This paper provides a compact introduction to this simple but under-exploited connection, with necessary definitions accessible to all fields involved.
Source code is available at this https URL .
[57]
arXiv:2505.10876
(replaced)
[pdf, html, other]
Title:
Preference Isolation Forest for Structure-based Anomaly Detection
Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi
Comments:
Accepted at Pattern Recognition (2025)
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)
We address the problem of detecting anomalies as samples that do not conform to structured patterns represented by low-dimensional manifolds. To this end, we conceive a general anomaly detection framework called Preference Isolation Forest (PIF), that combines the benefits of adaptive isolation-based methods with the flexibility of preference embedding. The key intuition is to embed the data into a high-dimensional preference space by fitting low-dimensional manifolds, and to identify anomalies as isolated points. We propose three isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most general solution, $ii$) RuzHash-iForest, that avoids explicit computation of distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a locality prior to improve efficiency and effectiveness.
[58]
arXiv:2505.12553
(replaced)
[pdf, html, other]
Title:
Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time
Qiang Fu, Andre Wibisono
Subjects:
Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML)
We study the Hamiltonian flow for optimization (HF-opt), which simulates the Hamiltonian dynamics for some integration time and resets the velocity to $0$ to decrease the objective function; this is the optimization analogue of the Hamiltonian Monte Carlo algorithm for sampling. For short integration time, HF-opt has the same convergence rates as gradient descent for minimizing strongly and weakly convex functions. We show that by randomizing the integration time in HF-opt, the resulting randomized Hamiltonian flow (RHF) achieves accelerated convergence rates in continuous time, similar to the rates for the accelerated gradient flow. We study a discrete-time implementation of RHF as the randomized Hamiltonian gradient descent (RHGD) algorithm. We prove that RHGD achieves the same accelerated convergence rates as Nesterov's accelerated gradient descent (AGD) for minimizing smooth strongly and weakly convex functions. We provide numerical experiments to demonstrate that RHGD is competitive with classical accelerated methods such as AGD across all settings and outperforms them in certain regimes.
[59]
arXiv:2505.13241
(replaced)
[pdf, html, other]
Title:
Reconstructing Physics-Informed Machine Learning for Traffic Flow Modeling: a Multi-Gradient Descent and Pareto Learning Approach
Yuan-Zheng Lei, Yaobang Gong, Dianwei Chen, Yao Cheng, Xianfeng Terry Yang
Journal-ref:
Transportation Research Part C: Emerging Technologies, Volume 180, November 2025, 105344
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Physics-informed machine learning (PIML) is crucial in modern traffic flow modeling because it combines the benefits of both physics-based and data-driven approaches. In conventional PIML, physical information is typically incorporated by constructing a hybrid loss function that combines data-driven loss and physics loss through linear scalarization. The goal is to find a trade-off between these two objectives to improve the accuracy of model predictions. However, from a mathematical perspective, linear scalarization is limited to identifying only the convex region of the Pareto front, as it treats data-driven and physics losses as separate objectives. Given that most PIML loss functions are non-convex, linear scalarization restricts the achievable trade-off solutions. Moreover, tuning the weighting coefficients for the two loss components can be both time-consuming and computationally challenging. To address these limitations, this paper introduces a paradigm shift in PIML by reformulating the training process as a multi-objective optimization problem, treating data-driven loss and physics loss independently. We apply several multi-gradient descent algorithms (MGDAs), including traditional multi-gradient descent (TMGD) and dual cone gradient descent (DCGD), to explore the Pareto front in this multi-objective setting. These methods are evaluated on both macroscopic and microscopic traffic flow models. In the macroscopic case, MGDAs achieved comparable performance to traditional linear scalarization methods. Notably, in the microscopic case, MGDAs significantly outperformed their scalarization-based counterparts, demonstrating the advantages of a multi-objective optimization approach in complex PIML scenarios.
[60]
arXiv:2509.07054
(replaced)
[pdf, html, other]
Title:
Statistical Methods in Generative AI
Edgar Dobriban
Comments:
Invited review paper for Annual Review of Statistics and Its Application. Feedback welcome
Subjects:
Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Methodology (stat.ME)
Generative Artificial Intelligence is emerging as an important technology, promising to be transformative in many areas. At the same time, generative AI techniques are based on sampling from probabilistic models, and by default, they come with no guarantees about correctness, safety, fairness, or other properties. Statistical methods offer a promising potential approach to improve the reliability of generative AI techniques. In addition, statistical methods are also promising for improving the quality and efficiency of AI evaluation, as well as for designing interventions and experiments in AI. In this paper, we review some of the existing work on these topics, explaining both the general statistical techniques used, as well as their applications to generative AI. We also discuss limitations and potential future directions.
[61]
arXiv:2509.09723
(replaced)
[pdf, other]
Title:
ALIGNS: Unlocking nomological networks in psychological measurement through a large language model
Kai R. Larsen, Sen Yan, Roland M. Mueller, Lan Sang, Mikko R√∂nkk√∂, Ravi Starzl, Donald Edmondson
Subjects:
Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Methodology (stat.ME)
Psychological measurement is critical to many disciplines. Despite advances in measurement, building nomological networks, theoretical maps of how concepts and measures relate to establish validity, remains a challenge 70 years after Cronbach and Meehl proposed them as fundamental to validation. This limitation has practical consequences: clinical trials may fail to detect treatment effects, and public policy may target the wrong outcomes. We introduce Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS), a large language model-based system trained with validated questionnaire measures. ALIGNS provides three comprehensive nomological networks containing over 550,000 indicators across psychology, medicine, social policy, and other fields. This represents the first application of large language models to solve a foundational problem in measurement validation. We report classification accuracy tests used to develop the model, as well as three evaluations. In the first evaluation, the widely used NIH PROMIS anxiety and depression instruments are shown to converge into a single dimension of emotional distress. The second evaluation examines child temperament measures and identifies four potential dimensions not captured by current frameworks, and questions one existing dimension. The third evaluation, an applicability check, engages expert psychometricians who assess the system's importance, accessibility, and suitability. ALIGNS is freely available at this http URL, complementing traditional validation methods with large-scale nomological analysis.
[62]
arXiv:2509.10987
(replaced)
[pdf, html, other]
Title:
A High-Order Cumulant Extension of Quasi-Linkage Equilibrium
Kai S. Shimagaki, Jorge Fernandez-de-Cossio-Diaz, Mauro Pastore, R√©mi Monasson, Simona Cocco, John P. Barton
Subjects:
Populations and Evolution (q-bio.PE); Computation (stat.CO)
A central question in evolutionary biology is how to quantitatively understand the dynamics of genetically diverse populations. Modeling the genotype distribution is challenging, as it ultimately requires tracking all correlations (or cumulants) among alleles at different loci. The quasi-linkage equilibrium (QLE) approximation simplifies this by assuming that correlations between alleles at different loci are weak -- i.e., low linkage disequilibrium -- allowing their dynamics to be modeled perturbatively. However, QLE breaks down under strong selection, significant epistatic interactions, or weak recombination. We extend the multilocus QLE framework to allow cumulants up to order $K$ to evolve dynamically, while higher-order cumulants ($>K$) are assumed to equilibrate rapidly. This extended QLE (exQLE) framework yields a general equation of motion for cumulants up to order $K$, which parallels the standard QLE dynamics (recovered when $K = 1$). In this formulation, cumulant dynamics are driven by the gradient of average fitness, mediated by a geometrically interpretable matrix that stems from competition among genotypes. Our analysis shows that the exQLE with $K=2$ accurately captures cumulant dynamics even when the fitness function includes higher-order (e.g., third- or fourth-order) epistatic interactions, capabilities that standard QLE lacks. We also applied the exQLE framework to infer fitness parameters from temporal sequence data. Overall, exQLE provides a systematic and interpretable approximation scheme, leveraging analytical cumulant dynamics and reducing complexity by progressively truncating higher-order cumulants.
[63]
arXiv:2509.13548
(replaced)
[pdf, html, other]
Title:
Mixture-of-Experts Framework for Field-of-View Enhanced Signal-Dependent Binauralization of Moving Talkers
Manan Mittal, Thomas Deppisch, Joseph Forrer, Chris Le Sueur, Zamir Ben-Hur, David Lou Along, Daniel D.E. Wong
Comments:
5 pages, 3 figures
Subjects:
Sound (cs.SD); Machine Learning (stat.ML)
We propose a novel mixture of experts framework for field-of-view enhancement in binaural signal matching. Our approach enables dynamic spatial audio rendering that adapts to continuous talker motion, allowing users to emphasize or suppress sounds from selected directions while preserving natural binaural cues. Unlike traditional methods that rely on explicit direction-of-arrival estimation or operate in the Ambisonics domain, our signal-dependent framework combines multiple binaural filters in an online manner using implicit localization. This allows for real-time tracking and enhancement of moving sound sources, supporting applications such as speech focus, noise reduction, and world-locked audio in augmented and virtual reality. The method is agnostic to array geometry offering a flexible solution for spatial audio capture and personalized playback in next-generation consumer audio devices.
Total of 63 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack