Statistics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
stat
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Statistics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Wednesday, 17 September 2025
Total of 76 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 32 of 32 entries)
[1]
arXiv:2509.12206
[pdf, html, other]
Title:
Haussdorff consistency of MLE in folded normal and Gaussian mixtures
Koustav Mallik
Comments:
27 pages. This is a series of works on nonidentifiable models
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME)
We develop a constant-tracking likelihood theory for two nonregular models: the folded normal and finite Gaussian mixtures. For the folded normal, we prove boundary coercivity for the profiled likelihood, show that the profile path of the location parameter exists and is strictly decreasing by an implicit-function argument, and establish a unique profile maximizer in the scale parameter. Deterministic envelopes for the log-likelihood, the score, and the Hessian yield elementary uniform laws of large numbers with finite-sample bounds, avoiding covering numbers. Identification and Kullback-Leibler separation deliver consistency. A sixth-order expansion of the log hyperbolic cosine creates a quadratic-minus-quartic contrast around zero, leading to a nonstandard one-fourth-power rate for the location estimator at the kink and a standard square-root rate for the scale estimator, with a uniform remainder bound. For finite Gaussian mixtures with distinct components and positive weights, we give a short identifiability proof up to label permutations via Fourier and Vandermonde ideas, derive two-sided Gaussian envelopes and responsibility-based gradient bounds on compact sieves, and obtain almost-sure and high-probability uniform laws with explicit constants. Using a minimum-matching distance on permutation orbits, we prove Hausdorff consistency on fixed and growing sieves. We quantify variance-collapse spikes via an explicit spike-bonus bound and show that a quadratic penalty in location and log-scale dominates this bonus, making penalized likelihood coercive; when penalties shrink but sample size times penalty diverges, penalized estimators remain consistent. All proofs are constructive, track constants, verify measurability of maximizers, and provide practical guidance for tuning sieves, penalties, and EM-style optimization.
[2]
arXiv:2509.12217
[pdf, html, other]
Title:
Correcting for partial verification bias in diagnostic accuracy studies: A tutorial using R
Wan Nor Arifin, Umi Kalsom Yusof
Comments:
20 pages, 2 tables
Journal-ref:
Statistics in Medicine 41 (2022) 1709-1727
Subjects:
Applications (stat.AP)
Diagnostic tests play a crucial role in medical care. Thus any new diagnostic tests must undergo a thorough evaluation. New diagnostic tests are evaluated in comparison with the respective gold standard tests. The performance of binary diagnostic tests is quantified by accuracy measures, with sensitivity and specificity being the most important measures. In any diagnostic accuracy study, the estimates of these measures are often biased owing to selective verification of the patients, which is referred to as partial verification bias. Several methods for correcting partial verification bias are available depending on the scale of the index test, target outcome and missing data mechanism. However, these are not easily accessible to the researchers due to the complexity of the methods. This article aims to provide a brief overview of the methods available to correct for partial verification bias involving a binary diagnostic test and provide a practical tutorial on how to implement the methods using the statistical programming language R.
[3]
arXiv:2509.12276
[pdf, other]
Title:
New generalized unit distributions based on order statistics
Iman Mohamed Attia
Subjects:
Methodology (stat.ME); Probability (math.PR); Statistics Theory (math.ST)
In the present paper, the author discusses the derivation of unit distributions and the derivation of the generalized form using the order statistics. The author discusses the Kumaraswamy as the smallest order statistic of the unit power distribution derived from the inverse Weibull distribution. The author discusses the unit Rayleigh distribution and how it can be generalized using the smallest, largest, and kth order statistics. Using the order statistics to generalize a distribution differs from other techniques like the power transformation and T-X family (transformed-transformer) method. For the discussed distribution, the author demonstrates the basic functions and properties with real data analysis.
[4]
arXiv:2509.12292
[pdf, html, other]
Title:
Significant inference and confidence sets for graphical models
P.A. Koldanov, A.P. Koldanov
Comments:
26 pages, 4 figures
Subjects:
Methodology (stat.ME); Applications (stat.AP)
The problem of identifying statistically significant inferences about the structure of the graphical model is considered, along with the related task of constructing a confidence set for a graphical model. It has been proven that the procedure for constructing such set is equivalent to the procedure for simultaneous testing of hypotheses and alternatives regarding the composition of the graphical model. Some variants of the simultaneous testing of hypotheses and alternatives are discussed. It is shown that under the condition of free combination of hypotheses and alternatives, a simple generalization of the closure method leads to singlestep procedures for simultaneous testing of hypotheses and alternatives. The structure of the confidence set for the graphical model is analyzed, demonstrating how the confidence set leads to a separation of inferences about the graphical model into statistically significant and insignificant categories, or into an area of uncertainty. General results are detailed by analyzing confidence sets for undirected Gaussian graphical model selection. Examples are provided that illustrate the separation of inferences about the composition of undirected Gaussian graphical models into significant results and areas of uncertainty, and a comparison is made with known results obtained using the SINful approach to undirected Gaussian graphical model selection.
[5]
arXiv:2509.12356
[pdf, other]
Title:
Jackknife Variance Estimation for Hájek-Dominated Generalized U-Statistics
Jakob R. Juergens
Subjects:
Statistics Theory (math.ST); Machine Learning (stat.ML)
We prove ratio-consistency of the jackknife variance estimator, and certain variants, for a broad class of generalized U-statistics whose variance is asymptotically dominated by their Hájek projection, with the classical fixed-order case recovered as a special instance. This Hájek projection dominance condition unifies and generalizes several criteria in the existing literature, placing the simple nonparametric jackknife on the same footing as the infinitesimal jackknife in the generalized setting. As an illustration, we apply our result to the two-scale distributional nearest-neighbor regression estimator, obtaining consistent variance estimates under substantially weaker conditions than previously required.
[6]
arXiv:2509.12420
[pdf, html, other]
Title:
System Reliability Estimation via Shrinkage
Beidi Qiang, Edsel Pena
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
In a coherent reliability system composed of multiple components configured according to a specific structure function, the distribution of system time to failure, or system lifetime, is often of primary interest. Accurate estimation of system reliability is critical in a wide range of engineering and industrial applications, forming decisions in system design, maintenance planning, and risk assessment. The system lifetime distribution can be estimated directly using the observed system failure times. However, when component-level lifetime data is available, it can yield improved estimates of system reliability. In this work, we demonstrate that under nonparametric assumptions about the component time-to-failure distributions, traditional estimators such as the Product-Limit Estimator (PLE) can be further improved under specific loss functions. We propose a novel methodology that enhances the nonparametric system reliability estimation through a shrinkage transformation applied to component-level estimators. This shrinkage approach leads to improved efficiency in estimating system reliability.
[7]
arXiv:2509.12448
[pdf, html, other]
Title:
A computational method for type I error rate control in power-maximizing response-adaptive randomization
Stef Baas, Lukas Pin, Sofía S. Villar, William F. Rosenberger
Comments:
27 pages, 7 tables, 6 figures
Subjects:
Methodology (stat.ME)
Maximizing statistical power in experimental design often involves imbalanced treatment allocation, but several challenges hinder its practical adoption: (1) the misconception that equal allocation always maximizes power, (2) when only targeting maximum power, more than half the participants may be expected to obtain inferior treatment, and (3) response-adaptive randomization (RAR) targeting maximum statistical power may inflate type I error rates substantially. Recent work identified issue (3) and proposed a novel allocation procedure combined with the asymptotic score test. Instead, the current research focuses on finite-sample guarantees. First, we analyze the power for traditional power-maximizing RAR procedures under exact tests, including a novel generalization of Boschloo's test. Second, we evaluate constrained Markov decision process (CMDP) RAR procedures under exact tests. These procedures target maximum average power under constraints on pointwise and average type I error rates, with averages taken across the parametric space. A combination of the unconditional exact test and the CMDP procedure protecting allocations to the superior arm gives the best performance, providing substantial power gains over equal allocation while allocating more participants in expectation to the superior treatment. Future research could focus on the randomization test, in which CMDP procedures exhibited lower power compared to other examined RAR procedures.
[8]
arXiv:2509.12473
[pdf, html, other]
Title:
Cox Regression on the Plane
Yael Travis-Lumer, Micha Mandel, Rebecca A. Betensky, Malka Gorfine
Comments:
28 pages, 0 figures
Subjects:
Methodology (stat.ME)
The Cox proportional hazards model is the most widely used regression model in univariate survival analysis. Extensions of the Cox model to bivariate survival data, however, remain scarce. We propose two novel extensions based on a Lehmann-type representation of the survival function. The first, the simple Lehmann model, is a direct extension that retains a straightforward structure. The second, the generalized Lehmann model, allows greater flexibility by incorporating three distinct regression parameters and includes the simple Lehmann model as a special case. For both models, we derive the corresponding regression formulations for the three bivariate hazard functions and discuss their interpretation and model validity. To estimate the regression parameters, we adopt a bivariate pseudo-observations approach. For the generalized Lehmann model, we extend this approach to accommodate a trivariate structure: trivariate pseudo-observations and a trivariate link function. We then propose a two-step estimation procedure, where the marginal regression parameters are estimated in the first step, and the remaining parameters are estimated in the second step. Finally, we establish the consistency and asymptotic normality of the resulting estimators.
[9]
arXiv:2509.12533
[pdf, html, other]
Title:
Transporting Predictions via Double Machine Learning: Predicting Partially Unobserved Students' Outcomes
Falco J. Bargagli-Stoffi, Emma Landry, Kevin P. Josey, Kenneth De Beckker, Joana E. Maldonado, Kristof De Witte
Comments:
arXiv admin note: substantial text overlap with arXiv:2102.04382
Subjects:
Applications (stat.AP); Methodology (stat.ME)
Educational policymakers often lack data on student outcomes in regions where standardized tests were not administered. Machine learning techniques can be used to predict unobserved outcomes in target populations by training models on data from a source population. However, differences between the source and target populations, particularly in covariate distributions, can reduce the transportability of these models, potentially reducing predictive accuracy and introducing bias. We propose using double machine learning for a covariate-shift weighted model. First, we estimate the overlap score-namely, the probability that an observation belongs to the source dataset given its covariates. Second, balancing weights, defined as the density ratio of target-to-source membership probabilities, are used to reweight the individual observations' contribution to the loss or likelihood function in the target outcome prediction model. This approach downweights source observations that are less similar to the target population, allowing predictions to rely more heavily on observations with greater overlap. As a result, predictions become more generalizable under covariate shift. We illustrate this framework in the context of uncertain data on students' standardized financial literacy scores (FLS). Using Bayesian Additive Regression Trees (BART), we predict missing FLS. We find minimal differences in predictive performance between the weighted and unweighted models, suggesting limited covariate shift in our empirical setting. Nonetheless, the proposed approach provides a principled framework for addressing covariate shift and is broadly applicable to predictive modeling in the social and health sciences, where differences between source and target populations are common.
[10]
arXiv:2509.12557
[pdf, html, other]
Title:
Instrument, Variable and Model Selection with Nonignorable Nonresponse
Ji Chen, Jun Shao
Subjects:
Methodology (stat.ME)
With nonignorable nonresponse, an effective method to construct valid estimators of population parameters is to use a covariate vector called instrument that can be excluded from the nonresponse propensity but are still useful covariate even when other covariates are conditioned. The existing work in this approach assumes such an instrument is given, which is frequently not the case in applications. In this paper we investigate how to search for an instrument from a given set of covariates. The method for estimation we apply is the pseudo likelihood proposed by Tang et al. (2003) and Zhao and Shao (2015), which assumed that an instrument is given and the distribution of response given covariates is parametric and the propensity is nonparametric. Thus, in addition to the challenge of searching an instrument, we also need to do variable and model selection simultaneously. We propose a method for instrument, variable, and model selection and show that our method produces consistent instrument and model selection as the sample size tends to infinity, under some regularity conditions. Empirical results including two simulation studies and two real examples are present to show that the proposed method works well.
[11]
arXiv:2509.12584
[pdf, html, other]
Title:
Sharp mean-field analysis of permutation mixtures and permutation-invariant decisions
Yiguo Liang, Yanjun Han
Subjects:
Statistics Theory (math.ST); Information Theory (cs.IT)
We develop sharp bounds on the statistical distance between high-dimensional permutation mixtures and their i.i.d. counterparts. Our approach establishes a new geometric link between the spectrum of a complex channel overlap matrix and the information geometry of the channel, yielding tight dimension-independent bounds that close gaps left by previous work. Within this geometric framework, we also derive dimension-dependent bounds that uncover phase transitions in dimensionality for Gaussian and Poisson families. Applied to compound decision problems, this refined control of permutation mixtures enables sharper mean-field analyses of permutation-invariant decision rules, yielding strong non-asymptotic equivalence results between two notions of compound regret in Gaussian and Poisson models.
[12]
arXiv:2509.12587
[pdf, html, other]
Title:
Inverse regression for causal inference with multiple outcomes
Wei Zhang, Qizhai Li, Peng Ding
Comments:
77 pages, 2 figures
Subjects:
Methodology (stat.ME)
With multiple outcomes in empirical research, a common strategy is to define a composite outcome as a weighted average of the original outcomes. However, the choices of weights are often subjective and can be controversial. We propose an inverse regression strategy for causal inference with multiple outcomes. The key idea is to regress the treatment on the outcomes, which is the inverse of the standard regression of the outcomes on the treatment. Although this strategy is simple and even counterintuitive, it has several advantages. First, testing for zero coefficients of the outcomes is equivalent to testing for the null hypothesis of zero effects, even though the inverse regression is deemed misspecified. Second, the coefficients of the outcomes provide a data-driven choice of the weights for defining a composite outcome. We also discuss the associated inference issues. Third, this strategy is applicable to general study designs. We illustrate the theory in both randomized experiments and observational studies.
[13]
arXiv:2509.12666
[pdf, html, other]
Title:
PBPK-iPINNs : Inverse Physics-Informed Neural Networks for Physiologically Based Pharmacokinetic Brain Models
Charuka D. Wickramasinghe, Krishanthi C. Weerasinghe, Pradeep K. Ranaweera
Comments:
24 pages, 11 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Numerical Analysis (math.NA)
Physics-Informed Neural Networks (PINNs) leverage machine learning with differential equations to solve direct and inverse problems, ensuring predictions follow physical laws. Physiologically based pharmacokinetic (PBPK) modeling advances beyond classical compartmental approaches by using a mechanistic, physiology focused framework. A PBPK model is based on a system of ODEs, with each equation representing the mass balance of a drug in a compartment, such as an organ or tissue. These ODEs include parameters that reflect physiological, biochemical, and drug-specific characteristics to simulate how the drug moves through the body. In this paper, we introduce PBPK-iPINN, a method to estimate drug-specific or patient-specific parameters and drug concentration profiles in PBPK brain compartment models using inverse PINNs. We demonstrate that, for the inverse problem to converge to the correct solution, the loss function components (data loss, initial conditions loss, and residual loss) must be appropriately weighted, and parameters (including number of layers, number of neurons, activation functions, learning rate, optimizer, and collocation points) must be carefully tuned. The performance of the PBPK-iPINN approach is then compared with established traditional numerical and statistical methods.
[14]
arXiv:2509.12686
[pdf, html, other]
Title:
A Doubly-Flexible Model Based on Generalized Gamma Frailty for Two-component Load-sharing Systems
Shilpi Biswas, Ayon Ganguly, Debanjan Mitra
Subjects:
Applications (stat.AP); Methodology (stat.ME)
For two-component load-sharing systems, a doubly-flexible model is developed where the generalized Fruend bivariate (GFB) distribution is used for the baseline of the component lifetimes, and the generalized gamma (GG) family of distributions is used to incorporate a shared frailty that captures dependence between the component lifetimes. The proposed model structure results in a very general two-way class of models that enables a researcher to choose an appropriate model for a given two-component load-sharing data within the respective families of distributions. The GFB-GG model structure provides better fit to two-component load-sharing systems compared to existing models. Fitting methods for the proposed model, based on direct optimization and an expectation maximization (EM) type algorithm, are discussed. Through simulations, effectiveness of the fitting methods is demonstrated. Also, through simulations, it is shown that the proposed model serves the intended purpose of model choice for a given two-component load-sharing data. A simulation case, and analysis of a real dataset are presented to illustrate the strength of the proposed model.
[15]
arXiv:2509.12691
[pdf, html, other]
Title:
Power-Dominance in Estimation Theory: A Third Pathological Axis
Sri Satish Krishna Chaitanya Bulusu, Mikko Sillanpää
Comments:
5 pages, 1 figure
Subjects:
Methodology (stat.ME); Signal Processing (eess.SP); Statistics Theory (math.ST); Machine Learning (stat.ML)
This paper introduces a novel framework for estimation theory by introducing a second-order diagnostic for estimator design. While classical analysis focuses on the bias-variance trade-off, we present a more foundational constraint. This result is model-agnostic, domain-agnostic, and is valid for both parametric and non-parametric problems, Bayesian and frequentist frameworks. We propose to classify the estimators into three primary power regimes. We theoretically establish that any estimator operating in the `power-dominant regime' incurs an unavoidable mean-squared error penalty, making it structurally prone to sub-optimal performance. We propose a `safe-zone law' and make this diagnostic intuitive through two safe-zone maps. One map is a geometric visualization analogous to a receiver operating characteristic curve for estimators, and the other map shows that the safe-zone corresponds to a bounded optimization problem, while the forbidden `power-dominant zone' represents an unbounded optimization landscape. This framework reframes estimator design as a path optimization problem, providing new theoretical underpinnings for regularization and inspiring novel design philosophies.
[16]
arXiv:2509.12700
[pdf, html, other]
Title:
Shape-to-Scale InSAR Adaptive Filtering and Phase Linking under Complex Elliptical Models
Shuyi Yao, Alejandro C. Frery, Timo Balz
Subjects:
Applications (stat.AP)
Distributed scatterers in InSAR (DS-InSAR) processing are essential for retrieving surface deformation in areas lacking strong point targets. Conventional workflows typically involve selecting statistically homogeneous pixels based on amplitude similarity, followed by phase estimation under the complex circular Gaussian model. However, amplitude statistics primarily reflect the backscattering strength of surface targets and may not sufficiently capture differences in decorrelation behavior. For example, when distinct scatterers exhibit similar backscatter strength but differ in coherence, amplitude-based selection methods may fail to differentiate them. Moreover, CCG-based phase estimators may lack robustness and suffer performance degradation under non-Rayleigh amplitude fluctuations.
Centered around scale-invariant second-order statistics, we propose ``Shape-to-Scale,'' a novel DS-InSAR framework. We first identify pixels that share a common angular scattering structure (``shape statistically homogeneous pixels'') with an angular consistency adaptive filter: a parametric selection method based on the complex angular central Gaussian distribution. Then, we introduce a complex generalized Gaussian-based phase estimation approach that is robust to potential non-Rayleigh scattering.
Experiments on both simulated and SAR datasets show that the proposed framework improves coherence structure clustering and enhances phase estimation robustness. This work provides a unified and physically interpretable strategy for DS-InSAR processing and offers new insights for high-resolution SAR time series analysis.
[17]
arXiv:2509.12734
[pdf, html, other]
Title:
A Statistical Test for Comparing the Linkage and Admixture Model Based on Central Limit Theorems
Carola Sophia Heinzel
Subjects:
Statistics Theory (math.ST)
In the Admixture Model, the probability that an individual carries a certain allele at a specific marker depends on the allele frequencies in $K$ ancestral populations and the proportion of the individual's genome originating from these populations. The markers are assumed to be independent. The Linkage Model is a Hidden Markov Model (HMM) that extends the Admixture Model by incorporating linkage between neighboring loci.
This study investigates the consistency and central limit behavior of maximum likelihood estimators (MLEs) for individual ancestry in the Linkage Model, complementing earlier results by \citep{pfaff2004information, pfaffelhuber2022central, heinzel2025consistency} for the Admixture Model. These theoretical results are used to prove theoretical properties of a statistical test that allows for model selection between the Admixture Model and the Linkage Model. Finally, we demonstrate the practical relevance of our results by applying the test to real-world data from \cite{10002015global}.
[18]
arXiv:2509.12825
[pdf, html, other]
Title:
Multivariate Low-Rank State-Space Model with SPDE Approach for High-Dimensional Data
Jacopo Rodeschini, Lorenzo Tedesco, Francesco Finazzi, Philipp Otto, Alessandro Fassò
Subjects:
Methodology (stat.ME); Applications (stat.AP)
This paper proposes a novel low-rank approximation to the multivariate State-Space Model. The Stochastic Partial Differential Equation (SPDE) approach is applied component-wise to the independent-in-time Matérn Gaussian innovation term in the latent equation, assuming component independence. This results in a sparse representation of the latent process on a finite element mesh, allowing for scalable inference through sparse matrix operations. Dependencies among observed components are introduced through a matrix of weights applied to the latent process. Model parameters are estimated using the Expectation-Maximisation algorithm, which features closed-form updates for most parameters and efficient numerical routines for the remaining parameters. We prove theoretical results regarding the accuracy and convergence of the SPDE-based approximation under fixed-domain asymptotics. Simulation studies show our theoretical results. We include an empirical application on air quality to demonstrate the practical usefulness of the proposed model, which maintains computational efficiency in high-dimensional settings. In this application, we reduce computation time by about 93%, with only a 15% increase in the validation error.
[19]
arXiv:2509.12884
[pdf, html, other]
Title:
Modeling nonstationary spatial processes with normalizing flows
Pratik Nag, Andrew Zammit-Mangion, Ying Sun
Subjects:
Methodology (stat.ME); Machine Learning (stat.ML)
Nonstationary spatial processes can often be represented as stationary processes on a warped spatial domain. Selecting an appropriate spatial warping function for a given application is often difficult and, as a result of this, warping methods have largely been limited to two-dimensional spatial domains. In this paper, we introduce a novel approach to modeling nonstationary, anisotropic spatial processes using neural autoregressive flows (NAFs), a class of invertible mappings capable of generating complex, high-dimensional warpings. Through simulation studies we demonstrate that a NAF-based model has greater representational capacity than other commonly used spatial process models. We apply our proposed modeling framework to a subset of the 3D Argo Floats dataset, highlighting the utility of our framework in real-world applications.
[20]
arXiv:2509.12889
[pdf, html, other]
Title:
Gaussian Mixture Model with unknown diagonal covariances via continuous sparse regularization
Romane Giard (ECL, ICJ, PSPM), Yohann de Castro (ICJ, ECL, PSPM, IUF), Clément Marteau (PSPM, ICJ, UCBL)
Subjects:
Statistics Theory (math.ST); Machine Learning (stat.ML)
This paper addresses the statistical estimation of Gaussian Mixture Models (GMMs) with unknown diagonal covariances from independent and identically distributed samples. We employ the Beurling-LASSO (BLASSO), a convex optimization framework that promotes sparsity in the space of measures, to simultaneously estimate the number of components and their parameters. Our main contribution extends the BLASSO methodology to multivariate GMMs with component-specific unknown diagonal covariance matrices-a significantly more flexible setting than previous approaches requiring known and identical covariances. We establish non-asymptotic recovery guarantees with nearly parametric convergence rates for component means, diagonal covariances, and weights, as well as for density prediction. A key theoretical contribution is the identification of an explicit separation condition on mixture components that enables the construction of non-degenerate dual certificates-essential tools for establishing statistical guarantees for the BLASSO. Our analysis leverages the Fisher-Rao geometry of the statistical model and introduces a novel semi-distance adapted to our framework, providing new insights into the interplay between component separation, parameter space geometry, and achievable statistical recovery.
[21]
arXiv:2509.12906
[pdf, html, other]
Title:
Least squares estimation of the transition density in bifurcating Markov models
S. Valère Bitseki Penda
Comments:
19 pages, 4 figures
Subjects:
Methodology (stat.ME); Probability (math.PR); Statistics Theory (math.ST)
In this article, we propose a least squares method for the estimation of the transition density in bifurcating Markov models. Unlike the kernel estimation, this method do not use the quotient which can be a source of errors. In order to study the rate of convergence for least squares estimators, we develop exponential inequalities for empirical process of bifurcating Markov chain under bracketing assumption. Unlike the classical processes, we observe that for bifurcating Markov chains, the complexity parameter depends on the ergodicity rate and as consequence, we have that the convergence rate of our estimator is a function of the ergodicity rate. We conclude with a numerical study to validate our theoretical results.
[22]
arXiv:2509.12921
[pdf, html, other]
Title:
Non-parametric estimation of non-linear diffusion coefficient in parabolic SPDEs
Martin Andersson, Benny Avelin, Valentin Garino, Pauliina Ilmonen, Lauri Viitasaari
Subjects:
Statistics Theory (math.ST)
In this article, we introduce a novel non-parametric predictor, based on conditional expectation, for the unknown diffusion coefficient function $\sigma$ in the stochastic partial differential equation $Lu = \sigma(u)\dot{W}$, where $L$ is a parabolic second order differential operator and $\dot{W}$ is a suitable Gaussian noise. We prove consistency and derive an upper bound for the error in the $L^p$ norm, in terms of discretization and smoothening parameters $h$ and $\varepsilon$. We illustrate the applicability of the approach and the role of the parameters with several interesting numerical examples.
[23]
arXiv:2509.13054
[pdf, html, other]
Title:
Efficient estimation for flexible spatial zero-inflated models with environmental applications
Chung-Wei Shen (1), Bu-Ren Hsu (2), Chia-Ming Hsu (2), Chun-Shu Chen (2) ((1) Department of Mathematics, National Chung Cheng University, Chia-Yi, Taiwan, R.O.C., (2) Graduate Institute of Statistics, National Central University, Taoyuan, Taiwan, R.O.C.)
Subjects:
Methodology (stat.ME)
Spatial two-component mixture models offer a robust framework for analyzing spatially correlated data with zero inflation. To circumvent potential biases introduced by assuming a specific distribution for the response variables, we employ a flexible spatial zero-inflated model. Despite its flexibility, this model poses significant computational challenges, particularly with large datasets, due to the high dimensionality of spatially dependent latent variables, the complexity of matrix operations, and the slow convergence of estimation procedures. To overcome these challenges, we propose a projection-based approach that reduces the dimensionality of the problem by projecting spatially dependent latent variables onto a lower-dimensional space defined by a selected set of basis functions. We further develop an efficient iterative algorithm for parameter estimation, incorporating a generalized estimating equation (GEE) framework. The optimal number of basis functions is determined using Akaike's information criterion (AIC), and the stability of the parameter estimates is assessed using the block jackknife method. The proposed method is validated through a comprehensive simulation study and applied to the analysis of Taiwan's daily rainfall data for 2016, demonstrating its practical utility and effectiveness.
[24]
arXiv:2509.13130
[pdf, html, other]
Title:
Optimal Conformal Prediction, E-values, Fuzzy Prediction Sets and Subsequent Decisions
Nick W. Koning, Sam van Meer
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME); Machine Learning (stat.ML)
We make three contributions to conformal prediction. First, we propose fuzzy conformal confidence sets that offer a degree of exclusion, generalizing beyond the binary inclusion/exclusion offered by classical confidence sets. We connect fuzzy confidence sets to e-values to show this degree of exclusion is equivalent to an exclusion at different confidence levels, capturing precisely what e-values bring to conformal prediction. We show that a fuzzy confidence set is a predictive distribution with a more appropriate error guarantee. Second, we derive optimal conformal confidence sets by interpreting the minimization of the expected measure of the confidence set as an optimal testing problem against a particular alternative. We use this to characterize exactly in what sense traditional conformal prediction is optimal. Third, we generalize the inheritance of guarantees by subsequent minimax decisions from confidence sets to fuzzy confidence sets. All our results generalize beyond the exchangeable conformal setting to prediction sets for arbitrary models. In particular, we find that any valid test (e-value) for a hypothesis automatically defines a (fuzzy) prediction confidence set.
[25]
arXiv:2509.13167
[pdf, html, other]
Title:
Scale-Location-Truncated Beta Regression: Expanding Beta Regression to Accommodate 0 and 1
Mingang Kim, Brent A. Kaplan, Mikhail N. Koffarnus, Christopher T. Franck
Subjects:
Methodology (stat.ME)
Beta regression is frequently used when the outcome variable y is bounded within a specific interval, transformed to the (0, 1) domain if necessary. However, standard beta regression cannot handle data observed at the boundary values of 0 or 1, as the likelihood function takes on values of either 0 or infinity. To address this issue, we propose the Scale-Location-Truncated beta (SLTB) regression model, which extends the beta distribution's domain to the [0, 1] interval. By using scale-location transformation and truncation, SLTB distribution allows positive finite mass to the boundary values, offering a flexible approach for handling values at 0 and 1. In this paper, we demonstrate the effectiveness of the SLTB regression model in comparison to standard beta regression models and other approaches like the Zero-One Inflated Beta (ZOIB) mixture model and XBX regression. Using empirical and simulated data, we compare the performance including predictive accuracy of the SLTB regression model with other methods, particularly in cases with observed boundary data values for y. The SLTB model is shown to offer great flexibility, supporting both linear and nonlinear relationships. Additionally, we implement the SLTB model within maximum likelihood and Bayesian frameworks, employing both hierarchical and non-hierarchical models. These comprehensive implementations demonstrate the broad applicability of SLTB model for modeling data with bounded values in a variety of contexts.
[26]
arXiv:2509.13169
[pdf, html, other]
Title:
Robust Sensitivity Analysis via Augmented Percentile Bootstrap under Simultaneous Violations of Unconfoundedness and Overlap
Han Cui, Xinran Li
Subjects:
Methodology (stat.ME)
The identification of causal effects in observational studies typically relies on two standard assumptions: unconfoundedness and overlap. However, both assumptions are often questionable in practice: unconfoundedness is inherently untestable, and overlap may fail in the presence of extreme unmeasured confounding. While various approaches have been developed to address unmeasured confounding and extreme propensity scores separately, few methods accommodate simultaneous violations of both assumptions. In this paper, we propose a sensitivity analysis framework that relaxes both unconfoundedness and overlap, building upon the marginal sensitivity model. Specifically, we allow the bound on unmeasured confounding to hold for only a subset of the population, thereby accommodating heterogeneity in confounding and allowing treatment probabilities to be zero or one. Moreover, unlike prior work, our approach does not require bounded outcomes and focuses on overlap-weighted average treatment effects, which are both practically meaningful and robust to non-overlap. We develop computationally efficient methods to obtain worst-case bounds via linear programming, and introduce a novel augmented percentile bootstrap procedure for statistical inference. This bootstrap method handles parameters defined through over-identified estimating equations involving unobserved variables and may be of independent interest. Our work provides a unified and flexible framework for sensitivity analysis under violations of both unconfoundedness and overlap.
[27]
arXiv:2509.13174
[pdf, html, other]
Title:
PDE-Based Bayesian Hierarchical Modeling for Event Spread, with Application to COVID-19 Infection
Mengqi Cen, Xuejing Meng, X. Joan Hu, Juxin Liu, Jianhong Wu
Subjects:
Applications (stat.AP)
We extended the Wikle's Bayesian hierarchical model based on a diffusion-reaction equation [Wikle, 2003] to investigate the COVID-19 spatio-temporal spread events across the USA from Mar 2020 to Feb 2022. Our model incorporated an advection term to account for the intra-state spread trend. We applied a Markov chain Monte Carlo (MCMC) method to obtain samples from the posterior distribution of the parameters. We implemented the approach via the collection of the COVID-19 infections across the states overtime from the New York Times. Our analysis shows that our approach can be robust to model misspecification to a certain extent and outperforms a few other approaches in the simulation settings. Our analysis results confirm that the diffusion rate is heterogeneous across the USA, and both the growth rate and the advection velocity are time-varying.
[28]
arXiv:2509.13176
[pdf, html, other]
Title:
Semiparametric Causal Inference for Right-Censored Outcomes with Many Weak Invalid Instruments
Qiushi Bu, Wen Su, Xingqiu Zhao, Zhonghua Liu
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
We propose a semiparametric framework for causal inference with right-censored survival outcomes and many weak invalid instruments, motivated by Mendelian randomization in biobank studies where classical methods may fail. We adopt an accelerated failure time model and construct a moment condition based on augmented inverse probability of censoring weighting, incorporating both uncensored and censored observations. Under a heteroscedasticity-based condition on the treatment model, we establish point identification of the causal effect despite censoring and invalid instruments. We propose GEL-NOW (Generalized Empirical Likelihood with Non-Orthogonal and Weak moments) for valid inference under these conditions. A divergent number of Neyman orthogonal nuisance functions is estimated using deep neural networks. A key challenge is that the conditional censoring distribution is a non-Neyman orthogonal nuisance, contributing to the first-order asymptotics of the estimator for the target causal effect parameter. We derive the asymptotic distribution and explicitly incorporate this additional uncertainty into the asymptotic variance formula. We also introduce a censoring-adjusted over-identification test that accounts for this variance component. Simulation studies and UK Biobank applications demonstrate the method's robustness and practical utility.
[29]
arXiv:2509.13189
[pdf, html, other]
Title:
SURGIN: SURrogate-guided Generative INversion for subsurface multiphase flow with quantified uncertainty
Zhao Feng, Bicheng Yan, Luanxiao Zhao, Xianda Shen, Renyu Zhao, Wenhao Wang, Fengshou Zhang
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn); Geophysics (physics.geo-ph)
We present a direct inverse modeling method named SURGIN, a SURrogate-guided Generative INversion framework tailed for subsurface multiphase flow data assimilation. Unlike existing inversion methods that require adaptation for each new observational configuration, SURGIN features a zero-shot conditional generation capability, enabling real-time assimilation of unseen monitoring data without task-specific retraining. Specifically, SURGIN synergistically integrates a U-Net enhanced Fourier Neural Operator (U-FNO) surrogate with a score-based generative model (SGM), framing the conditional generation as a surrogate prediction-guidance process in a Bayesian perspective. Instead of directly learning the conditional generation of geological parameters, an unconditional SGM is first pretrained in a self-supervised manner to capture the geological prior, after which posterior sampling is performed by leveraging a differentiable U-FNO surrogate to enable efficient forward evaluations conditioned on unseen observations. Extensive numerical experiments demonstrate SURGIN's capability to decently infer heterogeneous geological fields and predict spatiotemporal flow dynamics with quantified uncertainty across diverse measurement settings. By unifying generative learning with surrogate-guided Bayesian inference, SURGIN establishes a new paradigm for inverse modeling and uncertainty quantification in parametric functional spaces.
[30]
arXiv:2509.13267
[pdf, html, other]
Title:
Learning Discrete Bayesian Networks with Hierarchical Dirichlet Shrinkage
Alexander Dombowsky, David B. Dunson
Subjects:
Methodology (stat.ME); Machine Learning (stat.ML)
Discrete Bayesian networks (DBNs) provide a broadly useful framework for modeling dependence structures in multivariate categorical data. There is a vast literature on methods for inferring conditional probabilities and graphical structure in DBNs, but data sparsity and parametric assumptions are major practical issues. In this article, we detail a comprehensive Bayesian framework for learning DBNs. First, we propose a hierarchical prior for the conditional probabilities that enables complicated interactions between parent variables and stability in sparse regimes. We give a novel Markov chain Monte Carlo (MCMC) algorithm utilizing parallel Langevin proposals to generate exact posterior samples, avoiding the pitfalls of variational approximations. Moreover, we verify that the full conditional distribution of the concentration parameters is log-concave under mild conditions, facilitating efficient sampling. We then propose two methods for learning network structures, including parent sets, Markov blankets, and DAGs, from categorical data. The first cycles through individual edges each MCMC iteration, whereas the second updates the entire structure as a single step. We evaluate the accuracy, power, and MCMC performance of our methods on several simulation studies. Finally, we apply our methodology to uncover prognostic network structure from primary breast cancer samples.
[31]
arXiv:2509.13283
[pdf, html, other]
Title:
De Finetti + Sanov = Bayes
Nicholas G. Polson, Daniel Zantedeschi
Comments:
20 pages
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME)
We develop a framework for the operationalization of models and parameters by combining de Finetti's representation theorem with a conditional form of Sanov's theorem. This synthesis, the tilted de Finetti theorem, shows that conditioning exchangeable sequences on empirical moment constraints yields predictive laws in exponential families via the I-projection of a baseline measure. Parameters emerge as limits of empirical functionals, providing a probabilistic foundation for maximum entropy (MaxEnt) principles. This explains why exponential tilting governs likelihood methods and Bayesian updating, connecting naturally to finite-sample concentration rates that anticipate PAC-Bayes bounds. Examples include Gaussian scale mixtures, where symmetry uniquely selects location-scale families, and Jaynes' Brandeis dice problem, where partial information tilts the uniform law. Broadly, the theorem unifies exchangeability, large deviations, and entropy concentration, clarifying the ubiquity of exponential families and MaxEnt's role as the inevitable predictive limit under partial information.
[32]
arXiv:2509.13293
[pdf, html, other]
Title:
Inferring Soil Drydown Behaviour with Adaptive Bayesian Online Changepoint Analysis
Mengyi Gong, Christopher Nemeth, Rebecca Killick, Peter Strauss, John Quinton
Comments:
21 pages of main manuscript and 3 pages if supplemental document
Subjects:
Applications (stat.AP); Computation (stat.CO)
Continuous soil-moisture measurements provide a direct lens on subsurface hydrological processes, notably the post-rainfall "drydown" phase. Because these records consist of distinct, segment-specific behaviours whose forms and scales vary over time, realistic inference demands a model that captures piecewise dynamics while accommodating parameters that are unknown a priori. Building on Bayesian Online Changepoint Detection (BOCPD), we introduce two complementary extensions: a particle-filter variant that substitutes exact marginalisation with sequential Monte Carlo to enable real-time inference when critical parameters cannot be integrated out analytically, and an online-gradient variant that embeds stochastic gradient updates within BOCPD to learn application-relevant parameters on the fly without prohibitive computational cost. After validating both algorithms on synthetic data that replicate the temporal structure of field observations-detailing hyperparameter choices, priors, and cost-saving strategies-we apply them to soil-moisture series from experimental sites in Austria and the United States, quantifying site-specific drydown rates and demonstrating the advantages of our adaptive framework over static models.
Cross submissions (showing 13 of 13 entries)
[33]
arXiv:2509.12387
(cross-list from cs.LG)
[pdf, html, other]
Title:
Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization
Mohamed Zayaan S
Comments:
10 pages, 4 figures
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Modern deep learning models excel at pattern recognition but remain fundamentally limited by their reliance on spurious correlations, leading to poor generalization and a demand for massive datasets. We argue that a key ingredient for human-like intelligence-robust, sample-efficient learning-stems from an understanding of causal mechanisms. In this work, we introduce Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer the latent causal structure of a task distribution. CSML comprises three key modules: a perception module that maps raw inputs to disentangled symbolic representations; a differentiable causal induction module that discovers the underlying causal graph governing these symbols and a graph-based reasoning module that leverages this graph to make predictions. By meta-learning a shared causal world model across a distribution of tasks, CSML can rapidly adapt to novel tasks, including those requiring reasoning about interventions and counterfactuals, from only a handful of examples. We introduce CausalWorld, a new physics-based benchmark designed to test these capabilities. Our experiments show that CSML dramatically outperforms state-of-the-art meta-learning and neuro-symbolic baselines, particularly on tasks demanding true causal inference.
[34]
arXiv:2509.12401
(cross-list from cond-mat.mtrl-sci)
[pdf, other]
Title:
Reduced Order Modeling of Energetic Materials Using Physics-Aware Recurrent Convolutional Neural Networks in a Latent Space (LatentPARC)
Zoë J. Gray, Joseph B. Choi, Youngsoo Choi, H. Keo Springer, H. S. Udaykumar, Stephen S. Baek
Subjects:
Materials Science (cond-mat.mtrl-sci); Machine Learning (stat.ML)
Physics-aware deep learning (PADL) has gained popularity for use in complex spatiotemporal dynamics (field evolution) simulations, such as those that arise frequently in computational modeling of energetic materials (EM). Here, we show that the challenge PADL methods face while learning complex field evolution problems can be simplified and accelerated by decoupling it into two tasks: learning complex geometric features in evolving fields and modeling dynamics over these features in a lower dimensional feature space. To accomplish this, we build upon our previous work on physics-aware recurrent convolutions (PARC). PARC embeds knowledge of underlying physics into its neural network architecture for more robust and accurate prediction of evolving physical fields. PARC was shown to effectively learn complex nonlinear features such as the formation of hotspots and coupled shock fronts in various initiation scenarios of EMs, as a function of microstructures, serving effectively as a microstructure-aware burn model. In this work, we further accelerate PARC and reduce its computational cost by projecting the original dynamics onto a lower-dimensional invariant manifold, or 'latent space.' The projected latent representation encodes the complex geometry of evolving fields (e.g. temperature and pressure) in a set of data-driven features. The reduced dimension of this latent space allows us to learn the dynamics during the initiation of EM with a lighter and more efficient model. We observe a significant decrease in training and inference time while maintaining results comparable to PARC at inference. This work takes steps towards enabling rapid prediction of EM thermomechanics at larger scales and characterization of EM structure-property-performance linkages at a full application scale.
[35]
arXiv:2509.12406
(cross-list from cs.LG)
[pdf, html, other]
Title:
Bayesian Parametric Matrix Models: Principled Uncertainty Quantification for Spectral Learning
Mohammad Nooraiepour
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Scientific machine learning increasingly uses spectral methods to understand physical systems. Current spectral learning approaches provide only point estimates without uncertainty quantification, limiting their use in safety-critical applications where prediction confidence is essential. Parametric matrix models have emerged as powerful tools for scientific machine learning, achieving exceptional performance by learning governing equations. However, their deterministic nature limits deployment in uncertainty quantification applications. We introduce Bayesian parametric matrix models (B-PMMs), a principled framework that extends PMMs to provide uncertainty estimates while preserving their spectral structure and computational efficiency. B-PMM addresses the fundamental challenge of quantifying uncertainty in matrix eigenvalue problems where standard Bayesian methods fail due to the geometric constraints of spectral decomposition. The theoretical contributions include: (i) adaptive spectral decomposition with regularized matrix perturbation bounds that characterize eigenvalue uncertainty propagation, (ii) structured variational inference algorithms using manifold-aware matrix-variate Gaussian posteriors that respect Hermitian constraints, and (iii) finite-sample calibration guarantees with explicit dependence on spectral gaps and problem conditioning. Experimental validation across matrix dimensions from 5x5 to 500x500 with perfect convergence rates demonstrates that B-PMMs achieve exceptional uncertainty calibration (ECE < 0.05) while maintaining favorable scaling. The framework exhibits graceful degradation under spectral ill-conditioning and provides reliable uncertainty estimates even in near-degenerate regimes. The proposed framework supports robust spectral learning in uncertainty-critical domains and lays the groundwork for broader Bayesian spectral machine learning.
[36]
arXiv:2509.12416
(cross-list from cs.LG)
[pdf, other]
Title:
Surrogate Representation Inference for Noisy Text and Image Annotations
Kentaro Nakamura
Subjects:
Machine Learning (cs.LG); Applications (stat.AP)
As researchers increasingly rely on machine learning models and LLMs to annotate unstructured data, such as texts or images, various approaches have been proposed to correct bias in downstream statistical analysis. However, existing methods tend to yield large standard errors and require some error-free human annotation. In this paper, I introduce Surrogate Representation Inference (SRI), which assumes that unstructured data fully mediate the relationship between human annotations and structured variables. The assumption is guaranteed by design provided that human coders rely only on unstructured data for annotation. Under this setting, I propose a neural network architecture that learns a low-dimensional representation of unstructured data such that the surrogate assumption remains to be satisfied. When multiple human annotations are available, SRI can further correct non-differential measurement errors that may exist in human annotations. Focusing on text-as-outcome settings, I formally establish the identification conditions and semiparametric efficient estimation strategies that enable learning and leveraging such a low-dimensional representation. Simulation studies and a real-world application demonstrate that SRI reduces standard errors by over 50% when machine learning prediction accuracy is moderate and provides valid inference even when human annotations contain non-differential measurement errors.
[37]
arXiv:2509.12527
(cross-list from cs.LG)
[pdf, html, other]
Title:
Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design
Sanjeda Akter, Ibne Farabi Shihab, Anuj Sharma
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Large language models often produce plausible but incorrect outputs. Existing heuristics such as HallBayes lack formal guarantees. We develop the first comprehensive theory of \emph{information-lift certificates} under selective classification. Our contributions are: (i) a PAC-Bayes \emph{sub-gamma} analysis extending beyond standard Bernstein bounds; (ii) explicit skeleton sensitivity theorems quantifying robustness to misspecification; (iii) failure-mode guarantees under assumption violations; and (iv) a principled variational method for skeleton construction. Across six datasets and multiple model families, we validate assumptions empirically, reduce abstention by 12--15\% at the same risk, and maintain runtime overhead below 20\% (further reduced via batching).
[38]
arXiv:2509.12558
(cross-list from q-fin.RM)
[pdf, html, other]
Title:
A Note on Subadditivity of Value at Risks (VaRs): A New Connection to Comonotonicity
Yuri Imamura, Takashi Kato
Comments:
5 pages
Subjects:
Risk Management (q-fin.RM); Probability (math.PR); Statistics Theory (math.ST)
In this paper, we provide a new property of value at risk (VaR), which is a standard risk measure that is widely used in quantitative financial risk management. We show that the subadditivity of VaR for given loss random variables holds for any confidence level if and only if those are comonotonic. This result also gives a new equivalent condition for the comonotonicity of random vectors.
[39]
arXiv:2509.12708
(cross-list from cs.LG)
[pdf, html, other]
Title:
Spatio-temporal DeepKriging in PyTorch: A Supplementary Application to Precipitation Data for Interpolation and Probabilistic Forecasting
Pratik Nag
Subjects:
Machine Learning (cs.LG); Applications (stat.AP)
A detailed analysis of precipitation data over Europe is presented, with a focus on interpolation and forecasting applications. A Spatio-temporal DeepKriging (STDK) framework has been implemented using the PyTorch platform to achieve these objectives. The proposed model is capable of handling spatio-temporal irregularities while generating high-resolution interpolations and multi-step forecasts. Reproducible code modules have been developed as standalone PyTorch implementations for the interpolation\footnote[2]{Interpolation - this https URL} and forecasting\footnote[3]{Forecasting - this https URL}, facilitating broader application to similar climate datasets. The effectiveness of this approach is demonstrated through extensive evaluation on daily precipitation measurements, highlighting predictive performance and robustness.
[40]
arXiv:2509.12783
(cross-list from q-bio.NC)
[pdf, html, other]
Title:
Fast reconstruction of degenerate populations of conductance-based neuron models from spike times
Julien Brandoit, Damien Ernst, Guillaume Drion, Arthur Fyon
Subjects:
Neurons and Cognition (q-bio.NC); Machine Learning (cs.LG); Dynamical Systems (math.DS); Machine Learning (stat.ML)
Neurons communicate through spikes, and spike timing is a crucial part of neuronal processing. Spike times can be recorded experimentally both intracellularly and extracellularly, and are the main output of state-of-the-art neural probes. On the other hand, neuronal activity is controlled at the molecular level by the currents generated by many different transmembrane proteins called ion channels. Connecting spike timing to ion channel composition remains an arduous task to date. To address this challenge, we developed a method that combines deep learning with a theoretical tool called Dynamic Input Conductances (DICs), which reduce the complexity of ion channel interactions into three interpretable components describing how neurons spike. Our approach uses deep learning to infer DICs directly from spike times and then generates populations of "twin" neuron models that replicate the observed activity while capturing natural variability in membrane channel composition. The method is fast, accurate, and works using only spike recordings. We also provide open-source software with a graphical interface, making it accessible to researchers without programming expertise.
[41]
arXiv:2509.12917
(cross-list from cs.LG)
[pdf, html, other]
Title:
Reversible Deep Equilibrium Models
Sam McCallum, Kamran Arora, James Foster
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Deep Equilibrium Models (DEQs) are an interesting class of implicit model where the model output is implicitly defined as the fixed point of a learned function. These models have been shown to outperform explicit (fixed-depth) models in large-scale tasks by trading many deep layers for a single layer that is iterated many times. However, gradient calculation through DEQs is approximate. This often leads to unstable training dynamics and requires regularisation or many function evaluations to fix. Here, we introduce Reversible Deep Equilibrium Models (RevDEQs) that allow for exact gradient calculation, no regularisation and far fewer function evaluations than DEQs. We show that RevDEQs achieve state-of-the-art performance on language modelling and image classification tasks against comparable implicit and explicit models.
[42]
arXiv:2509.12981
(cross-list from cs.LG)
[pdf, html, other]
Title:
Causal Discovery via Quantile Partial Effect
Yikang Chen, Xingzhe Sun, Dehui Du
Comments:
29 pages, 6 figures
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Quantile Partial Effect (QPE) is a statistic associated with conditional quantile regression, measuring the effect of covariates at different levels. Our theory demonstrates that when the QPE of cause on effect is assumed to lie in a finite linear span, cause and effect are identifiable from their observational distribution. This generalizes previous identifiability results based on Functional Causal Models (FCMs) with additive, heteroscedastic noise, etc. Meanwhile, since QPE resides entirely at the observational level, this parametric assumption does not require considering mechanisms, noise, or even the Markov assumption, but rather directly utilizes the asymmetry of shape characteristics in the observational distribution. By performing basis function tests on the estimated QPE, causal directions can be distinguished, which is empirically shown to be effective in experiments on a large number of bivariate causal discovery datasets. For multivariate causal discovery, leveraging the close connection between QPE and score functions, we find that Fisher Information is sufficient as a statistical measure to determine causal order when assumptions are made about the second moment of QPE. We validate the feasibility of using Fisher Information to identify causal order on multiple synthetic and real-world multivariate causal discovery datasets.
[43]
arXiv:2509.12991
(cross-list from cs.LG)
[pdf, html, other]
Title:
Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder
Ya Zhou, Yujie Yang, Xiaohan Fan, Wei Zhao
Comments:
A simple yet effective strategy for ECG foundation models
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Applications (stat.AP)
ECG foundation models are increasingly popular due to their adaptability across various tasks. However, their clinical applicability is often limited by performance gaps compared to task-specific models, even after pre-training on large ECG datasets and fine-tuning on target data. This limitation is likely due to the lack of an effective post-training strategy. In this paper, we propose a simple yet effective post-training approach to enhance ECGFounder, a state-of-the-art ECG foundation model pre-trained on over 7 million ECG recordings. Experiments on the PTB-XL benchmark show that our approach improves the baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% in macro AUPRC. Additionally, our method outperforms several recent state-of-the-art approaches, including task-specific and advanced architectures. Further evaluation reveals that our method is more stable and sample-efficient compared to the baseline, achieving a 9.1% improvement in macro AUROC and a 34.9% improvement in macro AUPRC using just 10% of the training data. Ablation studies identify key components, such as stochastic depth and preview linear probing, that contribute to the enhanced performance. These findings underscore the potential of post-training strategies to improve ECG foundation models, and we hope this work will contribute to the continued development of foundation models in the ECG domain.
[44]
arXiv:2509.13141
(cross-list from econ.GN)
[pdf, html, other]
Title:
A hidden benefit of incomplete round-robin tournaments: Encouraging offensive play
László Csató
Subjects:
General Economics (econ.GN); Physics and Society (physics.soc-ph); Applications (stat.AP)
This paper aims to explore the impact of tournament design on the incentives of the contestants. We develop a simulation framework to quantify the potential gain and loss from attacking based on changes in the probability of reaching the critical ranking thresholds. The model is applied to investigate the 2024/25 UEFA Champions League reform. The novel incomplete round-robin league phase is found to create more powerful incentives for offensive play than the previous group stage, with an average increase of 119\% (58\%) regarding the first (second) prize. Our study provides the first demonstration that the tournament format itself can strongly influence team behaviour in sports.
[45]
arXiv:2509.13232
(cross-list from cs.LG)
[pdf, html, other]
Title:
Single-stream Policy Optimization
Zhongwen Xu, Zihan Ding
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO's gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@$k$ across the evaluated $k$ values. SPO's success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning.
Replacement submissions (showing 31 of 31 entries)
[46]
arXiv:2103.10231
(replaced)
[pdf, html, other]
Title:
Identification of Partial-Differential-Equations-Based Models from Noisy Data via Splines
Yujie Zhao, Xiaoming Huo, Yajun Mei
Subjects:
Methodology (stat.ME)
We propose a two-stage method called \textit{Spline Assisted Partial Differential Equation based Model Identification (SAPDEMI)} to identify partial differential equation (PDE)-based models from noisy data. In the first stage, we employ the cubic splines to estimate unobservable derivatives. The underlying PDE is based on a subset of these derivatives. This stage is computationally efficient: its computational complexity is a product of a constant with the sample size; this is the lowest possible order of computational complexity. In the second stage, we apply the Least Absolute Shrinkage and Selection Operator (Lasso) to identify the underlying PDE-based model. Statistical properties are developed, including the model identification accuracy. We validate our theory through various numerical examples and a real data case study. The case study is based on a National Aeronautics and Space Administration (NASA) data set.
[47]
arXiv:2209.01754
(replaced)
[pdf, html, other]
Title:
Learning from a Biased Sample
Roshni Sahoo, Lihua Lei, Stefan Wager
Subjects:
Methodology (stat.ME); Machine Learning (cs.LG); Machine Learning (stat.ML)
The empirical risk minimization approach to data-driven decision making requires access to training data drawn under the same conditions as those that will be faced when the decision rule is deployed. However, in a number of settings, we may be concerned that our training sample is biased in the sense that some groups (characterized by either observable or unobservable attributes) may be under- or over-represented relative to the general population; and in this setting empirical risk minimization over the training set may fail to yield rules that perform well at deployment. We propose a model of sampling bias called conditional $\Gamma$-biased sampling, where observed covariates can affect the probability of sample selection arbitrarily much but the amount of unexplained variation in the probability of sample selection is bounded by a constant factor. Applying the distributionally robust optimization framework, we propose a method for learning a decision rule that minimizes the worst-case risk incurred under a family of test distributions that can generate the training distribution under $\Gamma$-biased sampling. We apply a result of Rockafellar and Uryasev to show that this problem is equivalent to an augmented convex risk minimization problem. We give statistical guarantees for learning a model that is robust to sampling bias via the method of sieves, and propose a deep learning algorithm whose loss function captures our robust learning target. We empirically validate our proposed method in a case study on prediction of mental health scores from health survey data and a case study on ICU length of stay prediction.
[48]
arXiv:2305.06116
(replaced)
[pdf, html, other]
Title:
Merging Rate of Opinions via Optimal Transport on Random Measures
Marta Catalano, Hugo Lavenant
Comments:
Substantial modifications compared to v1 of this preprint
Subjects:
Statistics Theory (math.ST); Probability (math.PR)
Random measures provide flexible parameters for Bayesian nonparametric models. Given two different priors for a random measure, we develop a natural framework to investigate the rate at which the corresponding posteriors merge, as the sample size increases. We define a new distance between the laws of random measures that is built as a Wasserstein distance on the ground space of unbalanced measures, endowed with the bounded Lipschitz metric. We develop tight analytical bounds for its specification to completely random measures, including the special case of Poisson and gamma random measures. The bounds are interpreted in terms of an adapted extended Wasserstein distance between the Lévy measures and are used to investigate the merging between the posteriors of normalized gamma and generalized gamma priors. After a careful study on the identifiability of the law of the random measure, interesting asymptotic and finite-sample insights are derived without putting any assumption on the true data generating process.
[49]
arXiv:2309.08808
(replaced)
[pdf, html, other]
Title:
Adaptive Neyman Allocation
Jinglong Zhao
Subjects:
Methodology (stat.ME); Econometrics (econ.EM)
In the experimental design literature, Neyman allocation refers to the practice of allocating units into treated and control groups, potentially in unequal numbers proportional to their respective standard deviations, with the objective of minimizing the variance of the treatment effect estimator. This widely recognized approach increases statistical power in scenarios where the treated and control groups have different standard deviations, as is often the case in social experiments, clinical trials, marketing research, and online A/B testing. However, Neyman allocation cannot be implemented unless the standard deviations are known in advance. Fortunately, the multi-stage nature of the aforementioned applications allows the use of earlier stage observations to estimate the standard deviations, which further guide allocation decisions in later stages. In this paper, we introduce a competitive analysis framework to study this multi-stage experimental design problem. We propose a simple adaptive Neyman allocation algorithm, which almost matches the information-theoretic limit of conducting experiments. We provide theory for estimation and inference using data collected from our adaptive Neyman allocation algorithm. Using online A/B testing data from a social media site, we demonstrate the effectiveness of our adaptive Neyman allocation algorithm, highlighting its practicality especially when applied with only a limited number of stages.
[50]
arXiv:2309.15828
(replaced)
[pdf, html, other]
Title:
Multi-task and few-shot learning in virtual flow metering
Kristian Løvland, Bjarne Grimstad, Lars S. Imsland
Comments:
17 pages, 12 figures. Updates consist of extended dataset decriptions and a study on the role of context parameter dimension
Journal-ref:
Nordic Machine Intelligence, Vol. 5 No. 1, 2025, pp. 1-17
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Recent literature has explored various ways to improve soft sensors by utilizing learning algorithms with transferability. A performance gain is generally attained when knowledge is transferred among strongly related soft sensor learning tasks. One setting where it is reasonable to expect strongly related tasks, is when learning soft sensors for separate process units that are of the same type. Applying methods that exploit transferability in this setting leads to what we call multi-unit soft sensing.
This paper formulates a probabilistic, hierarchical model for multi-unit soft sensing. The model is implemented using a deep neural network. The proposed learning method is studied empirically on a large-scale industrial case by developing virtual flow meters (a type of soft sensor) for 80 petroleum wells. We investigate how the model generalizes with the number of wells/units. We demonstrate that multi-unit models learned from data from many wells permit few-shot learning of virtual flow meters for new wells. Surprisingly, regarding the difficulty of the tasks, few-shot learning on 1-3 data points often leads to high performance on new wells.
[51]
arXiv:2311.18699
(replaced)
[pdf, other]
Title:
Correlated Bayesian Additive Regression Trees with Gaussian Process for Regression Analysis of Dependent Data
Xuetao Lu a, Robert E. McCulloch
Subjects:
Methodology (stat.ME)
Bayesian Additive Regression Trees (BART) has gained widespread popularity, inspiring numerous extensions across diverse applications. However, relatively little attention has been given to modeling dependent data. To fill this gap, we introduce Correlated BART (CBART), which extends BART to account for correlated errors. With a dummy representation, efficient matrix computation was developed for the estimation of CBART. Building on CBART, we propose CBART$\unicode{0x2010}$GP, a nonparametric regression model that integrates CBART with a Gaussian process (GP) in an additive framework. In CBART$\unicode{0x2010}$GP, CBART retrieves the true signal of covariates$\unicode{0x2010}$response relationship, while the GP extracts the dependency structure of residuals. To enable scalable inference of CBART$\unicode{0x2010}$GP, we develop a two$\unicode{0x2010}$stage analysis of variance with weighted residuals approach to substantially reduce the computational complexity. Simulation studies demonstrate that CBART-GP not only accurately recovers the true covariate$\unicode{0x2010}$response relationship but also achieves strong predictive performance. A real world application further illustrates its practical utility.
[52]
arXiv:2312.02513
(replaced)
[pdf, html, other]
Title:
Asymptotic Theory of the Best-Choice Rerandomization using the Mahalanobis Distance
Yuhao Wang, Xinran Li
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
Rerandomization, a design that utilizes pretreatment covariates and improves their balance between different treatment groups, has received attention recently in both theory and practice. From a survey by Bruhn and McKenzie (2009), there are at least two types of rerandomization that are used in practice: the first rerandomizes the treatment assignment until covariate imbalance is below a prespecified threshold; the second randomizes the treatment assignment multiple times and chooses the one with the best covariate balance. In this paper we will consider the second type of rerandomization, namely the best-choice rerandomization, whose theory and inference are still lacking in the literature. In particular, we will focus on the best-choice rerandomization that uses the Mahalanobis distance to measure covariate imbalance, which is one of the most commonly used imbalance measure for multivariate covariates and is invariant to affine transformations of covariates. We will study the large-sample repeatedly sampling properties of the best-choice rerandomization, allowing both the number of covariates and the number of tried complete randomizations to increase with the sample size. We show that the asymptotic distribution of the difference-in-means estimator is more concentrated around the true average treatment effect under rerandomization than under the complete randomization, and propose large-sample accurate confidence intervals for rerandomization that are shorter than that for the completely randomized experiment. We further demonstrate that, with moderate number of covariates and with the number of tried randomizations increasing polynomially with the sample size, the best-choice rerandomization can achieve the ideally optimal precision that one can expect even with perfectly balanced covariates. The developed theory and methods are also illustrated using real field experiments.
[53]
arXiv:2401.12865
(replaced)
[pdf, html, other]
Title:
fdrSAFE: Selective Aggregation for Local False Discovery Rate Estimation
Jenna M. Landy, Giovanni Parmigiani
Comments:
11 pages, 3 figures (+ references and supplement). For open-source R software package, see this https URL. For all code used in the simulation studies and experimental application, see this https URL. Note: this method was previously named gridsemblefdr
Subjects:
Methodology (stat.ME)
Estimating local false discovery rates (fdr) is central to large-scale multiple hypothesis testing, yet different methods often produce divergent results, and there is little guidance for selecting among them. Because ground truth hypothesis labels are unobservable, standard model selection cannot be used. We present fdrSAFE (selective aggregation for fdr estimation), a data-driven selective ensembling approach that estimates model performances on synthetic datasets designed to resemble the observed data but with known ground truth. With simulation studies and an experimental spike-in transcriptomic dataset, we show that fdrSAFE achieves robust near-optimality, performing well across diverse settings where baseline model performances vary. Along with improved fdr estimates, this framework enhances replicability by replacing arbitrary model choice with a principled, data-adaptive procedure. An open-source R software package is available on GitHub at jennalandy/fdrSAFE
[54]
arXiv:2404.00319
(replaced)
[pdf, html, other]
Title:
Direction Preferring Confidence Intervals
Tzviel Frostig, Yoav Benjamini, Ruth Heller
Comments:
11 figures, 45 pages
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Confidence intervals (CIs) are instrumental in statistical analysis, providing a range estimate of the parameters. In modern statistics, selective inference is common, where only certain parameters are highlighted. However, this selective approach can bias the inference, leading some to advocate for the use of CIs over p-values. To increase the flexibility of confidence intervals, we introduce direction-preferring CIs, enabling analysts to focus on parameters trending in a particular direction. We present these types of CIs in two settings: First, when there is no selection of parameters; and second, for situations involving parameter selection, where we offer a conditional version of the direction-preferring CIs. Both of these methods build upon the foundations of Modified Pratt CIs, which rely on non-equivariant acceptance regions to achieve longer intervals in exchange for improved sign exclusions. We show that for selected parameters out of m > 1 initial parameters of interest, CIs aimed at controlling the false coverage rate, have higher power to determine the sign compared to conditional CIs. We also show that conditional confidence intervals control the marginal false coverage rate (mFCR) under any dependency.
[55]
arXiv:2406.01849
(replaced)
[pdf, html, other]
Title:
Conditional uncorrelation equals independence
Dawid Tarłowski
Subjects:
Statistics Theory (math.ST); Probability (math.PR)
We show that the stochastic independence of real-valued random variables is equivalent to the conditional uncorrelation, where the conditioning takes place over the Cartesian products of intervals. Next, we express the mutual independence in terms of the conditional correlation matrix. Our results extend the results of Jaworski et al. (Electron. J. Stat., 18(1), 653-673, 2024), which are based on the copula functions and assume the existence of the joint density of the variables. We relax this assumption and show that the independence characterization via conditional uncorrelation is valid in full generality - that is, for all kinds of random variables and any dependencies between them. Additionally, we analyse the assumptions under which the independence is determined by the local uncorrelation. The measure-theoretic methodology we present uses the Radon-Nikodym derivative to reduce the multidimensional characterization problem to the simple one-dimensional conditioning. To demonstrate the potential usefulness of the presented results, various numerical examples are presented.
[56]
arXiv:2408.09341
(replaced)
[pdf, html, other]
Title:
Approximate independence of permutation mixtures
Yanjun Han, Jonathan Niles-Weed
Comments:
Added a new section on statistical applications and improved a few results
Subjects:
Statistics Theory (math.ST); Information Theory (cs.IT); Probability (math.PR)
We prove bounds on statistical distances between high-dimensional exchangeable mixture distributions (which we call \emph{permutation mixtures}) and their i.i.d. counterparts. Our results are based on a novel method for controlling $\chi^2$ divergences between exchangeable mixtures, which is tighter than the existing methods of moments or cumulants. At a technical level, a key innovation in our proofs is a new Maclaurin-type inequality for elementary symmetric polynomials of variables that sum to zero and an upper bound on permanents of doubly-stochastic positive semidefinite matrices. We obtain as a corollary a new de Finetti-style theorem (in the language of Diaconis and Freedman, 1987), as well as several new statistical results, including a differential privacy guarantee for the ``shuffled privacy model'' with Gaussian noise and improved generic consistency guarantees for empirical Bayes procedures in compound decision problems.
[57]
arXiv:2410.20659
(replaced)
[pdf, html, other]
Title:
A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data
Saptarshi Chakraborty, Peter L. Bartlett
Subjects:
Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Statistics Theory (math.ST)
Despite significant research on the optimization aspects of federated learning, the exploration of generalization error, especially in the realm of heterogeneous federated learning, remains an area that has been insufficiently investigated, primarily limited to developments in the parametric regime. This paper delves into the generalization properties of deep federated regression within a two-stage sampling model. Our findings reveal that the intrinsic dimension, characterized by the entropic dimension, plays a pivotal role in determining the convergence rates for deep learners when appropriately chosen network sizes are employed. Specifically, when the true relationship between the response and explanatory variables is described by a $\beta$-Hölder function and one has access to $n$ independent and identically distributed (i.i.d.) samples from $m$ participating clients, for participating clients, the error rate scales at most as $\Tilde{O}((mn)^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))})$, whereas for non-participating clients, it scales as $\Tilde{O}(\Delta \cdot m^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))} + (mn)^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))})$. Here $\bar{d}_{2\beta}(\lambda)$ denotes the corresponding $2\beta$-entropic dimension of $\lambda$, the marginal distribution of the explanatory variables. The dependence between the two stages of the sampling scheme is characterized by $\Delta$. Consequently, our findings not only explicitly incorporate the ``heterogeneity" of the clients, but also highlight that the convergence rates of errors of deep federated learners are not contingent on the nominal high dimensionality of the data but rather on its intrinsic dimension.
[58]
arXiv:2411.10623
(replaced)
[pdf, html, other]
Title:
Sensitivity Analysis for Observational Studies with Flexible Matched Designs
Xinran Li
Subjects:
Methodology (stat.ME)
Observational studies provide invaluable opportunities to draw causal inference, but they may suffer from biases due to pretreatment difference between treated and control units. Matching is a popular approach to reduce observed covariate imbalance. To tackle unmeasured confounding, a sensitivity analysis is often conducted to investigate how robust a causal conclusion is to the strength of unmeasured confounding. For matched observational studies, Rosenbaum proposed a sensitivity analysis framework that uses the randomization of treatment assignments as the ``reasoned basis'' and imposes no model assumptions on the potential outcomes as well as their dependence on the observed and unobserved confounding factors. However, this otherwise appealing framework requires exact matching to guarantee its validity, which is hard to achieve in practice. In this paper we provide an alternative inferential framework that shares the same procedure as Rosenbaum's approach but relies on a different justification. Our framework allows flexible matching algorithms and utilizes alternative source of randomness, in particular random permutations of potential outcomes instead of treatment assignments, to guarantee statistical validity.
[59]
arXiv:2412.12233
(replaced)
[pdf, html, other]
Title:
Russian roulette: The need for stochastic potential outcomes when utilities depend on counterfactuals
Andrew Gelman, Jonas M. Mikhaeil
Subjects:
Other Statistics (stat.OT)
It has been proposed in medical decision analysis to express the ``first do no harm'' principle as an asymmetric utility function in which the loss from killing a patient would count more than the gain from saving a life. Such a utility depends on unrealized potential outcomes, and we show how this yields a paradoxical decision recommendation in a simple hypothetical example involving games of Russian roulette. The problem is resolved if we abandon the stable unit treatment value assumption (SUTVA) and allow the potential outcomes to be random variables. This leads us to conclude that, if you are interested in this sort of asymmetric utility function, you need to move to the stochastic potential outcome framework. We discuss the implications of the choice of parameterization in this setting.
[60]
arXiv:2502.16758
(replaced)
[pdf, html, other]
Title:
Stabilizing the Splits through Minimax Decision Trees
Zhenyuan Zhang, Hengrui Luo
Comments:
58 pages, 12 figures; a substantial expansion upon the first version
Subjects:
Statistics Theory (math.ST); Probability (math.PR)
By revisiting the end-cut preference (ECP) phenomenon associated with a single CART (Breiman et al. (1984)), we introduce MinimaxSplit decision trees, a robust alternative to CART that selects splits by minimizing the worst-case child risk rather than the average risk. For regression, we minimize the maximum within-child squared error; for classification, we minimize the maximum child entropy, yielding a C4.5-compatible criterion. We also study a cyclic variant that deterministically cycles coordinates, leading to our main method of cyclic MinimaxSplit decision trees. We prove oracle inequalities that cover both regression and classification, under mild marginal non-atomicity conditions. The bounds control the tree's global excess risk by local worst-case impurities and yield fast convergence rates compared to CART. We extend the analysis to ensembles that subsample coordinates per node. Empirically, (cyclic) MinimaxSplit trees and their forests improve on structured heterogeneity data such as EEG amplitude regression over fixed time horizons, seasonal air quality forecasting, and image denoising framed as non-parametric regression on spatial coordinates.
[61]
arXiv:2503.07343
(replaced)
[pdf, html, other]
Title:
Robust a posteriori estimation of probit-lognormal seismic fragility curves via sequential design of experiments and constrained reference prior
Antoine Van Biesbroeck, Clément Gauchy, Cyril Feau, Josselin Garnier
Subjects:
Applications (stat.AP)
A seismic fragility curve expresses the probability of failure of a structure conditional to an intensity measure (IM) derived from seismic signals. When only limited data is available, the practitioner often refers to the probit-lognormal model coupled with maximum likelihood estimation (MLE) to obtain estimates of these curves. This means that only a binary indicator of the state (BIS) of the structure is known, namely a failure or non-failure state indicator, when it is subjected to a seismic signal with an intensity measure IM. In this context, the objective of this work is to propose a method for optimally estimating such curves by obtaining the most precise estimate possible with the minimum of data. The novelty of our work is twofold. First, we present and show how to mitigate the likelihood degeneracy problem which is ubiquitous with small data sets and hampers frequentist approaches such as MLE. Second, we propose a novel strategy for sequential design of experiments (DoE) that selects seismic signals from a large database of synthetic or real signals via their IM values, to be applied to structures to evaluate the corresponding BISs. This strategy relies on a criterion based on information theory in a Bayesian framework. It therefore aims to sequentially designate the IM value such that the pair (IM, BIS) has on average, with respect to the BIS of the structure, the greatest impact on the posterior distribution of the fragility curve. The methodology is applied to a case study from the nuclear industry. The results demonstrate its ability to efficiently and robustly estimate the fragility curve, and to avoid degeneracy even with a limited amount of data. Furthermore, we demonstrate that the estimates quickly reach the model bias induced by the probit-lognormal modeling. Eventually, two criteria are suggested to help the user stop the DoE algorithm.
[62]
arXiv:2503.15704
(replaced)
[pdf, html, other]
Title:
Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence Minimization
Kyurae Kim, Zuheng Xu, Jacob R. Gardner, Trevor Campbell
Comments:
Accepted to ICML'25; v4, v5: fixed typos
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO)
The performance of sequential Monte Carlo (SMC) samplers heavily depends on the tuning of the Markov kernels used in the path proposal. For SMC samplers with unadjusted Markov kernels, standard tuning objectives, such as the Metropolis-Hastings acceptance rate or the expected-squared jump distance, are no longer applicable. While stochastic gradient-based end-to-end optimization has been explored for tuning SMC samplers, they often incur excessive training costs, even for tuning just the kernel step sizes. In this work, we propose a general adaptation framework for tuning the Markov kernels in SMC samplers by minimizing the incremental Kullback-Leibler (KL) divergence between the proposal and target paths. For step size tuning, we provide a gradient- and tuning-free algorithm that is generally applicable for kernels such as Langevin Monte Carlo (LMC). We further demonstrate the utility of our approach by providing a tailored scheme for tuning kinetic LMC used in SMC samplers. Our implementations are able to obtain a full schedule of tuned parameters at the cost of a few vanilla SMC runs, which is a fraction of gradient-based approaches.
[63]
arXiv:2505.09001
(replaced)
[pdf, html, other]
Title:
Causal Feedback Discovery using Convergence Cross Mapping on Sea Ice Data
Francis Nji, Seraj Al Mahmud Mostafa, Jianwu Wang
Comments:
In ACM Sigspatial Conference Workshop, 8 pages
Subjects:
Applications (stat.AP); Atmospheric and Oceanic Physics (physics.ao-ph)
Identifying causal relationships in climate systems remains challenging due to nonlinear, coupled dynamics that limit the effectiveness of linear and stochastic causal discovery approaches. This study benchmarks Convergence Cross Mapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both synthetic datasets with ground truth causal links and 41 years of Arctic climate data (1979--2021). Unlike stochastic models that rely on autoregressive residual dependence, CCM leverages Takens' state-space reconstruction and delay-embedding to reconstruct attractor manifolds from time series. Cross mapping between reconstructed manifolds exploits deterministic signatures of causation, enabling the detection of weak and bidirectional causal links that linear models fail to resolve. Results demonstrate that CCM achieves higher specificity and fewer false positives on synthetic benchmarks, while maintaining robustness under observational noise and limited sample lengths. On Arctic data, CCM reveals significant causal interactions between sea ice extent and atmospheric variables like specific humidity, longwave radiation, and surface temperature with a $p$-value of $0.009$, supporting ice-albedo feedbacks and moisture-radiation couplings central to Arctic amplification. In contrast, stochastic approaches miss these nonlinear dependencies or infer spurious causal relations. This work establishes CCM as a robust causal inference tool for nonlinear climate dynamics and provides the first systematic benchmarking framework for method selection in climate research.
[64]
arXiv:2509.04852
(replaced)
[pdf, html, other]
Title:
Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment
Wei Chen, Shigui Li, Jiacheng Li, Jian Xu, Zhiqi Lin, Junmei Yang, Delu Zeng, John Paisley, Qibin Zhao
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Estimating density ratios is a fundamental problem in machine learning, but existing methods often trade off accuracy for efficiency. We propose \textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)}, a framework that enables accurate, any-step estimation without numerical integration.
Instead of modeling infinitesimal tangents as in prior methods, ISA-DRE learns a global secant function, defined as the expectation of all tangents over an interval, with provably lower variance, making it more suitable for neural approximation. This is made possible by the \emph{Secant Alignment Identity}, a self-consistency condition that formally connects the secant with its underlying tangent representations.
To mitigate instability during early training, we introduce \emph{Contraction Interval Annealing}, a curriculum strategy that gradually expands the alignment interval during training. This process induces a contraction mapping, which improves convergence and training stability.
Empirically, ISA-DRE achieves competitive accuracy with significantly fewer function evaluations compared to prior methods, resulting in much faster inference and making it well suited for real-time and interactive applications.
[65]
arXiv:2509.06308
(replaced)
[pdf, html, other]
Title:
Minimax optimal transfer learning for high-dimensional additive regression
Seung Hyun Moon
Comments:
This is a draft version of the paper. All responsibilities are assigned to the first author
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)
This paper studies high-dimensional additive regression under the transfer learning framework, where one observes samples from a target population together with auxiliary samples from different but potentially related regression models. We first introduce a target-only estimation procedure based on the smooth backfitting estimator with local linear smoothing. In contrast to previous work, we establish general error bounds under sub-Weibull($\alpha$) noise, thereby accommodating heavy-tailed error distributions. In the sub-exponential case ($\alpha=1$), we show that the estimator attains the minimax lower bound under regularity conditions, which requires a substantial departure from existing proof strategies. We then develop a novel two-stage estimation method within a transfer learning framework, and provide theoretical guarantees at both the population and empirical levels. Error bounds are derived for each stage under general tail conditions, and we further demonstrate that the minimax optimal rate is achieved when the auxiliary and target distributions are sufficiently close. All theoretical results are supported by simulation studies and real data analysis.
[66]
arXiv:2509.07885
(replaced)
[pdf, other]
Title:
Clustering methods for Categorical Time Series and Sequences : A scoping review
Ottavio Khalifa, Viet-Thi Tran, Alan Balendran, François Petit
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Objective: To provide an overview of clustering methods for categorical time series (CTS), a data structure commonly found in epidemiology, sociology, biology, and marketing, and to support method selection in regards to data characteristics.
Methods: We searched PubMed, Web of Science, and Google Scholar, from inception up to November 2024 to identify articles that propose and evaluate clustering techniques for CTS. Methods were classified according to three major families -- distance-based, feature-based, and model-based -- and assessed on their ability to handle data challenges such as variable sequence length, multivariate data, continuous time, missing data, time-invariant covariates, and large data volumes.
Results: Out of 14607 studies, we included 124 articles describing 129 methods, spanning domains such as artificial intelligence, social sciences, and epidemiology. Distance-based methods, particularly those using Optimal Matching, were most prevalent, with 56 methods. We identified 28 model-based methods, which demonstrated superior flexibility for handling complex data structures such as multivariate data, continuous time and time-invariant covariates. We also recorded 45 feature-based approaches, which were on average more scalable but less flexible. A searchable Web application was developed to facilitate method selection based on dataset characteristics ( this https URL )
Discussion: While distance-based methods dominate, model-based approaches offer the richest modeling potential but are less scalable. Feature-based methods favor performance over flexibility, with limited support for complex data structures.
Conclusion: This review highlights methodological diversity and gaps in CTS clustering. The proposed typology aims to guide researchers in selecting methods for their specific use cases.
[67]
arXiv:2509.11435
(replaced)
[pdf, html, other]
Title:
A Particle-Flow Algorithm for Free-Support Wasserstein Barycenters
Kisung You
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO)
The Wasserstein barycenter extends the Euclidean mean to the space of probability measures by minimizing the weighted sum of squared 2-Wasserstein distances. We develop a free-support algorithm for computing Wasserstein barycenters that avoids entropic regularization and instead follows the formal Riemannian geometry of Wasserstein space. In our approach, barycenter atoms evolve as particles advected by averaged optimal-transport displacements, with barycentric projections of optimal transport plans used in place of Monge maps when the latter do not exist. This yields a geometry-aware particle-flow update that preserves sharp features of the Wasserstein barycenter while remaining computationally tractable. We establish theoretical guarantees, including consistency of barycentric projections, monotone descent and convergence to stationary points, stability with respect to perturbations of the inputs, and resolution consistency as the number of atoms increases. Empirical studies on averaging probability distributions, Bayesian posterior aggregation, image prototypes and classification, and large-scale clustering demonstrate accuracy and scalability of the proposed particle-flow approach, positioning it as a principled alternative to both linear programming and regularized solvers.
[68]
arXiv:2009.05908
(replaced)
[pdf, html, other]
Title:
Understanding Boolean Function Learnability on Deep Neural Networks: PAC Learning Meets Neurosymbolic Models
Marcio Nicolau, Anderson R. Tavares, Zhiwei Zhang, Pedro Avelar, João M. Flach, Luis C. Lamb, Moshe Y. Vardi
Comments:
Version accepted for NeSy 2025
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Computational learning theory states that many classes of boolean formulas are learnable in polynomial time. This paper addresses the understudied subject of how, in practice, such formulas can be learned by deep neural networks. Specifically, we analyze boolean formulas associated with model-sampling benchmarks, combinatorial optimization problems, and random 3-CNFs with varying degrees of constrainedness. Our experiments indicate that: (i) neural learning generalizes better than pure rule-based systems and pure symbolic approach; (ii) relatively small and shallow neural networks are very good approximators of formulas associated with combinatorial optimization problems; (iii) smaller formulas seem harder to learn, possibly due to the fewer positive (satisfying) examples available; and (iv) interestingly, underconstrained 3-CNF formulas are more challenging to learn than overconstrained ones. Such findings pave the way for a better understanding, construction, and use of interpretable neurosymbolic AI methods.
[69]
arXiv:2407.18707
(replaced)
[pdf, html, other]
Title:
Finite Neural Networks as Mixtures of Gaussian Processes: From Provable Error Bounds to Prior Selection
Steven Adams, Andrea Patanè, Morteza Lahijanian, Luca Laurenti
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Infinitely wide or deep neural networks (NNs) with independent and identically distributed (i.i.d.) parameters have been shown to be equivalent to Gaussian processes. Because of the favorable properties of Gaussian processes, this equivalence is commonly employed to analyze neural networks and has led to various breakthroughs over the years. However, neural networks and Gaussian processes are equivalent only in the limit; in the finite case there are currently no methods available to approximate a trained neural network with a Gaussian model with bounds on the approximation error. In this work, we present an algorithmic framework to approximate a neural network of finite width and depth, and with not necessarily i.i.d. parameters, with a mixture of Gaussian processes with error bounds on the approximation error. In particular, we consider the Wasserstein distance to quantify the closeness between probabilistic models and, by relying on tools from optimal transport and Gaussian processes, we iteratively approximate the output distribution of each layer of the neural network as a mixture of Gaussian processes. Crucially, for any NN and $\epsilon >0$ our approach is able to return a mixture of Gaussian processes that is $\epsilon$-close to the NN at a finite set of input points. Furthermore, we rely on the differentiability of the resulting error bound to show how our approach can be employed to tune the parameters of a NN to mimic the functional behavior of a given Gaussian process, e.g., for prior selection in the context of Bayesian inference. We empirically investigate the effectiveness of our results on both regression and classification problems with various neural network architectures. Our experiments highlight how our results can represent an important step towards understanding neural network predictions and formally quantifying their uncertainty.
[70]
arXiv:2409.13745
(replaced)
[pdf, html, other]
Title:
Context-Aware Membership Inference Attacks against Pre-trained Large Language Models
Hongyan Chang, Ali Shahin Shamsabadi, Kleomenis Katevas, Hamed Haddadi, Reza Shokri
Subjects:
Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML)
Membership Inference Attacks (MIAs) on pre-trained Large Language Models (LLMs) aim at determining if a data point was part of the model's training set. Prior MIAs that are built for classification models fail at LLMs, due to ignoring the generative nature of LLMs across token sequences. In this paper, we present a novel attack on pre-trained LLMs that adapts MIA statistical tests to the perplexity dynamics of subsequences within a data point. Our method significantly outperforms prior approaches, revealing context-dependent memorization patterns in pre-trained LLMs.
[71]
arXiv:2411.19193
(replaced)
[pdf, html, other]
Title:
Convex Regularization and Convergence of Policy Gradient Flows under Safety Constraints
Pekka Malo, Lauri Viitasaari, Antti Suominen, Eeva Vilkkumaa, Olli Tahvonen
Comments:
29 pages
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Probability (math.PR); Machine Learning (stat.ML)
This paper examines reinforcement learning (RL) in infinite-horizon decision processes with almost-sure safety constraints, crucial for applications like autonomous systems, finance, and resource management. We propose a doubly-regularized RL framework combining reward and parameter regularization to address safety constraints in continuous state-action spaces. The problem is formulated as a convex regularized objective with parametrized policies in the mean-field regime. Leveraging mean-field theory and Wasserstein gradient flows, policies are modeled on an infinite-dimensional statistical manifold, with updates governed by parameter distribution gradient flows. Key contributions include solvability conditions for safety-constrained problems, smooth bounded approximations for gradient flows, and exponential convergence guarantees under sufficient regularization. General regularization conditions, including entropy regularization, support practical particle method implementations. This framework provides robust theoretical insights and guarantees for safe RL in complex, high-dimensional settings.
[72]
arXiv:2501.02817
(replaced)
[pdf, html, other]
Title:
A Stable Measure for Conditional Periodicity of Time Series using Persistent Homology
Bala Krishnamoorthy, Elizabeth P. Thompson
Subjects:
Algebraic Topology (math.AT); Statistics Theory (math.ST); Machine Learning (stat.ML)
Given a pair of time series, we study how the periodicity of one influences the periodicity of the other. There are several known methods to measure the similarity between a pair of time series, but we have yet to find any measures with theoretical stability results. Persistence homology has been utilized to construct a scoring function with theoretical guarantees of stability that quantifies the periodicity of a single univariate time series f1, denoted score(f1). Building on this concept, we propose a conditional periodicity score that quantifies the periodicity similarity of two univariate time series, denoted score(f1|f2), and derive theoretical stability results for the same. We prove stability of score(f1|f2) under orthogonal projection of the time series embeddings onto their first K principal components. We show that the change in our score is bounded by a function of the eigenvalues corresponding to the remaining (unused) N-K principal components and hence is small when the first K principal components capture most of the variation in the time series embeddings. We derive a lower bound on the embedding dimension to use in our pipeline which guarantees that any two such embeddings produce scores that are linearly within epsilon of each other. We present a procedure for computing conditional periodicity scores and implement it on several types of synthetic signals. We experimentally compare our similarity measure to the most-similar statistical measure of percent determinism (%DET) and show greater stability of score(f1|f2). We also compare both measures on several pairs of real time series describing monthly proportions of incoming calls to a police agency and highlight the decreased stability of %DET on the same.
[73]
arXiv:2501.18879
(replaced)
[pdf, html, other]
Title:
Understanding Generalization in Physics Informed Models through Affine Variety Dimensions
Takeshi Koshizuka, Issei Sato
Subjects:
Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
Physics-informed machine learning is gaining significant traction for enhancing statistical performance and sample efficiency through the integration of physical knowledge. However, current theoretical analyses often presume complete prior knowledge in non-hybrid settings, overlooking the crucial integration of observational data, and are frequently limited to linear systems, unlike the prevalent nonlinear nature of many real-world applications. To address these limitations, we introduce a unified residual form that unifies collocation and variational methods, enabling the incorporation of incomplete and complex physical constraints in hybrid learning settings. Within this formulation, we establish that the generalization performance of physics-informed regression in such hybrid settings is governed by the dimension of the affine variety associated with the physical constraint, rather than by the number of parameters. This enables a unified analysis that is applicable to both linear and nonlinear equations. We also present a method to approximate this dimension and provide experimental validation of our theoretical findings.
[74]
arXiv:2502.07047
(replaced)
[pdf, html, other]
Title:
A Closed-Form Transition Density Expansion for Elliptic and Hypo-Elliptic SDEs
Yuga Iguchi, Alexandros Beskos
Subjects:
Numerical Analysis (math.NA); Statistics Theory (math.ST)
We introduce a closed-form expansion for the transition density of elliptic and hypo-elliptic multivariate Stochastic Differential Equations (SDEs), over a period $\Delta\in (0,1)$, in terms of powers of $\Delta^{j/2}$, $j\ge 0$. Our methodology provides approximations of the transition density, easily evaluated via any software that performs symbolic calculations. A major part of the paper is devoted to an analytical control of the remainder in our expansion for fixed $\Delta\in(0,1)$. The obtained error bounds validate theoretically the methodology, by characterising the size of the distance from the true value. It is the first time that such a closed-form expansion becomes available for the important class of hypo-elliptic SDEs, to the best of our knowledge. For elliptic SDEs, closed-form expansions are available, with some works identifying the size of the error for fixed $\Delta$, as per our contribution. Our methodology allows for a uniform treatment of elliptic and hypo-elliptic SDEs, when earlier works are intrinsically restricted to an elliptic setting. We show numerical applications highlighting the effectiveness of our method, by carrying out parameter inference for hypo-elliptic SDEs that do not satisfy stated conditions. The latter are sufficient for controlling the remainder terms, but the closed-form expansion itself is applicable in general settings.
[75]
arXiv:2502.19788
(replaced)
[pdf, other]
Title:
Semiparametric Triple Difference Estimators
Sina Akbari, Negar Kiyavash, AmirEmad Ghassami
Comments:
59 pages
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
The triple difference causal inference framework is an extension of the well-known difference-in-differences framework. It relaxes the parallel trends assumption of the difference-in-differences framework through leveraging data from an auxiliary domain. Despite being commonly applied in empirical research, the triple difference framework has received relatively limited attention in the statistics literature. Specifically, investigating the intricacies of identification and the design of robust and efficient estimators for this framework has remained largely unexplored. This work aims to address these gaps in the literature. From the identification standpoint, we present outcome regression and weighting methods to identify the average treatment effect on the treated in both panel data and repeated cross-section settings. For the latter, we relax the commonly made assumption of time-invariant composition of units. From the estimation perspective, we develop semiparametric estimators for the triple difference framework in both panel data and repeated cross-sections settings. These estimators are based on the cross-fitting technique, and flexible machine learning tools can be used to estimate the nuisance components. We characterize conditions under which our proposed estimators are efficient, doubly robust, root-n consistent and asymptotically normal. As an application of our proposed methodology, we examined the effect of mandated maternity benefits on the hourly wages of women of childbearing age and found that these mandates result in a 2.6% drop in hourly wages.
[76]
arXiv:2504.08802
(replaced)
[pdf, html, other]
Title:
InfoGain Wavelets: Furthering the Design of Graph Diffusion Wavelets
David R. Johnson, Smita Krishnaswamy, Michael Perlmutter
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Diffusion wavelets extract information from graph signals at different scales of resolution by utilizing graph diffusion operators raised to various powers, known as diffusion scales. Traditionally, these scales are chosen to be dyadic integers, $2^j$. Here, we propose a novel, unsupervised method for selecting the diffusion scales based on ideas from information theory. We then show that our method can be incorporated into wavelet-based GNNs, which are modeled after the geometric scattering transform, via graph classification experiments.
Total of 76 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack