Statistics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
stat
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Statistics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Tuesday, 16 September 2025
Total of 128 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 49 of 49 entries)
[1]
arXiv:2509.10664
[pdf, html, other]
Title:
Estimating Global HIV Prevalence in Key Populations: A Cross-Population Hierarchical Modeling Approach
Jiahao Zhang, Keith Sabin, Le Bao
Subjects:
Applications (stat.AP)
Key populations at high risk of HIV infection are critical for understanding and monitoring HIV epidemics, but global estimation is hampered by sparse, uneven data. We analyze data from 199 countries for female sex workers (FSW), men who have sex with men (MSM), and people who inject drugs (PWID) over 2011-2021, and introduce a cross-population hierarchical model that borrows strength across countries, years, and populations. The model combines region- and population-specific means with country random effects, temporal dependence, and cross-population correlations in a Gaussian Markov random-field formulation on the log-prevalence scale. In 5-fold cross-validation, the approach outperforms a regional-median baseline and reduced variants (65 percent reduction in cross-validated MSE) with well-calibrated posterior predictive coverage (93 percent). We map the 2021 prevalence and quantify the change between 2011 and 2021 using posterior prevalence ratios to identify countries with substantial increases or decreases. The framework yields globally comparable and uncertainty-quantified country-by-year prevalence estimates, enhancing evidence for resource allocation and targeted interventions for marginalized populations where routine data are limited.
[2]
arXiv:2509.10668
[pdf, html, other]
Title:
Modelling Under-Reported Data: Pitfalls of Na√Øve Approaches and a New Statistical Framework for Epidemic Curve Reconstruction
Justin J. Slater, Sindi Bebeziqi
Subjects:
Applications (stat.AP)
Count-valued autoregressions are widely used to analyse time-series of reported infectious-disease cases because of their close connection with discrete-time transmission models. However, when such models are applied directly to under-reported case counts, their mechanistic interpretation can break down. We establish new theoretical results quantifying the consequences of ignoring under-reporting in these models. To address this issue, reported cases are often modelled as a binomially thinned version of an underlying count process, but such models are difficult to fit because the unobserved true counts are serially correlated and integer-valued. We develop a new statistical framework for under-reported infectious-disease data that uses a normal-normal approximation to a broad class of thinned count autoregressions and then accurately maps this continuous process back to the integers. Through simulations and applications to rotavirus incidence in a German state and Covid-19 incidence in English conurbations, we demonstrate that our approach both retains the mechanistic appeal of thinned autoregressions and substantially simplifies inference.
[3]
arXiv:2509.10733
[pdf, other]
Title:
Using Drift Diffusion Model to Analyze Cars' Lane Change Decisions behind Heavy Vehicles
Nachuan Li, Hani S. Mahmassani, Soyoung Ahn, Anupam Srivastava
Comments:
This manuscript was submitted to and accepted by Transportation Research Board 2025 Conference
Subjects:
Applications (stat.AP)
Heavy vehicles (HVs) pose a significant challenge to maintaining a smooth traffic flow on the freeway because they are slower moving and create large blind spots. It is therefore desirable for the followers of HVs to perform lane changes (LCs) to achieve a higher speed and a safer driving environment. Understanding LC behaviors of vehicles behind HVs is important because LCs can lead to highway capacity drop and induce safety risks. In this paper, a drift-diffusion model (DDM) is proposed to model the LC behavior of cars behind HVs. In this drift-diffusion (DD) process, vehicles consider the surrounding traffic environment and accumulate evidence over time. A LC is made if the evidence threshold is exceeded. By obtaining vehicle trajectories with LC intentions in the Third Generation Simulation (TGSIM) dataset through clustering and fitting them with the DDM, we find that a lower initial headway makes the drivers more likely to LC. Furthermore, a larger distance to the follower on the target lane, an increasing target gap size, and a higher speed difference between the target lane and the leading HV increases the rate of evidence accumulation and leads to a LC execution sooner.
[4]
arXiv:2509.10736
[pdf, html, other]
Title:
Adaptive Bayesian computation for efficient biobank-scale genomic inference
Yiran Li, John Whittaker, Sylvia Richardson, Helene Ruffieux
Subjects:
Applications (stat.AP)
Motivation: Modern biobanks, with unprecedented sample sizes and phenotypic diversity, have become foundational resources for genomic studies, enabling powerful cross-phenotype and population-scale analyses. As studies grow in complexity, Bayesian hierarchical models offer a principled framework for jointly modeling multiple units such as cells, traits, and experimental conditions, increasing statistical power through information sharing. However, adoption of Bayesian hierarchical models in biobank-scale studies remains limited due to computational inefficiencies, particularly in posterior inference over high-dimensional parameter spaces. Deterministic approximations such as variational inference provide scalable alternatives to Markov Chain Monte Carlo, yet current implementations do not fully exploit the structure of genome-wide multi-unit modeling, especially when biological effects of interest are concentrated in a few units.
Results: We propose an adaptive focus (AF) strategy within a block coordinate ascent variational inference (CAVI) framework that selectively updates subsets of parameters at each iteration, corresponding to units deemed relevant based on current estimates. We illustrate this approach in protein quantitative trait locus (pQTL) mapping using a joint model of hierarchically linked regressions with shared parameters across traits. In both simulated data and real proteomic data from the UK Biobank, AF-CAVI achieves up to a 50\% reduction in runtime while maintaining statistical performance. We also provide a genome-wide pipeline for multi-trait pQTL mapping across thousands of traits, demonstrating AF-CAVI as an efficient scheme for large-scale, multi-unit Bayesian analysis in biobanks.
[5]
arXiv:2509.10787
[pdf, html, other]
Title:
Outlier-Resistant Heterogeneous Treatment Effect Estimation in HDLSS Settings via GAT--CVAE Framework
Byeonghee Lee, Joonsung Kang
Comments:
12 pages
Subjects:
Methodology (stat.ME)
We introduce a robust framework for heterogeneous treatment effect (HTE) estimation tailored to high-dimensional low sample size (HDLSS) settings. By combining Graph Attention Networks (GAT) to capture structural dependencies among confounders with a Conditional Variational Autoencoder (CVAE) for latent representation learning, our method expands the sample space and performs clustering that integrates even outlier sets into coherent subgroups. Clusterwise causal effects are then estimated using a doubly robust outlier-resistant estimator, yielding stable and generalizable results. Simulations and real-world applications confirm superior performance compared with existing HTE methods, highlighting the framework's potential for precision medicine and policy evaluation.
[6]
arXiv:2509.10794
[pdf, html, other]
Title:
Closed-form parameter estimation for the bivariate gamma distribution: New approaches
Roberto Vila, Helton Saulo
Comments:
22 pages, 2 figures
Subjects:
Methodology (stat.ME)
We propose new closed-form estimators for the parameters of McKay's bivariate gamma distribution by exploiting monotone transformations of the likelihood equations. As a special case, our framework recovers the estimators recently introduced by Zhao et al. (2022) [Zhao, J., Jang, Y.-H., and Kim, H. (2022). Closed-form and bias-corrected estimators for the bivariate gamma distribution. Journal of Multivariate Analysis, 191:105009]. Theoretical properties, including strong consistency and asymptotic normality, are established. We further introduce a second family of closed-form estimators that is explicitly built from the stochastic relationship between gamma random variables. Our second approach encompasses the estimators of Nawa and Nadarajah (2023) [Nawa, V. M. and Nadarajah, S. (2023). New closed form estimators for a bivariate gamma distribution. Statistics, 57(1):150-160]. Monte Carlo experiments are conducted to assess finite-sample performance, showing that the new estimators perform comparably to maximum likelihood estimators while avoiding iterative optimization, and improve upon the existing closed-form approach by Zhao et al. (2022) and Nawa and Nadarajah (2023). A real hydrological data set is analyzed to illustrate the proposed approaches.
[7]
arXiv:2509.10817
[pdf, html, other]
Title:
Efficient High-Dimensional Conditional Independence Testing
Bilol Banerjee
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME)
This article deals with the problem of testing conditional independence between two random vectors ${\bf X}$ and ${\bf Y}$ given a confounding random vector ${\bf Z}$. Several authors have considered this problem for multivariate data. However, most of the existing tests has poor performance against local contiguous alternatives beyond linear dependency. In this article, an Energy distance type measure of conditional dependence is developed, borrowing ideas from the model-X framework. A consistent estimator of the measure is proposed, and its theoretical properties are studied under general assumptions. Using the estimator as a test statistic a test of conditional independence is developed, and a suitable resampling algorithm is designed to calibrate the test. The test turns out to be not only large sample consistent, but also Pitman efficient against local contiguous alternatives, and is provably consistent when the dimension of the data diverges to infinity with the sample size. Several empirical studies are conducted to demonstrate the efficacy of the test against state-of-the-art methods.
[8]
arXiv:2509.10853
[pdf, html, other]
Title:
Variable Selection Using Relative Importance Rankings
Tien-En Chang, Argon Chen
Comments:
26 pages, 9 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Although conceptually related, variable selection and relative importance (RI) analysis have been treated quite differently in the literature. While RI is typically used for post-hoc model explanation, this paper explores its potential for variable ranking and filter-based selection before model creation. Specifically, we anticipate strong performance from the RI measures because they incorporate both direct and combined effects of predictors, addressing a key limitation of marginal correlation that ignores dependencies among predictors. We implement and evaluate the RI-based variable selection methods using general dominance (GD), comprehensive relative importance (CRI), and a newly proposed, computationally efficient variant termed CRI.Z.
We first demonstrate how the RI measures more accurately rank the variables than the marginal correlation, especially when there are suppressed or weak predictors. We then show that predictive models built on these rankings are highly competitive, often outperforming state-of-the-art methods such as the lasso and relaxed lasso. The proposed RI-based methods are particularly effective in challenging cases involving clusters of highly correlated predictors, a setting known to cause failures in many benchmark methods. Although lasso methods have dominated the recent literature on variable selection, our study reveals that the RI-based method is a powerful and competitive alternative. We believe these underutilized tools deserve greater attention in statistics and machine learning communities. The code is available at: this https URL.
[9]
arXiv:2509.10905
[pdf, html, other]
Title:
Two-Stage Least Squares Instrumental Variable Estimation for Semiparametric Accelerated Failure Time Models with Right-Censored Data
Zian Zhuang, Hua Zhou, Jin Zhou, Gang Li
Comments:
29 pages, 6 figures
Subjects:
Methodology (stat.ME)
Instrumental variable (IV) analysis is widely used in fields such as economics and epidemiology to address unobserved confounding and measurement error when estimating the causal effects of intermediate covariates on outcomes. However, extending the commonly used two-stage least squares (TSLS) approach to survival settings is nontrivial due to censoring. This paper introduces a novel extension of TSLS to the semiparametric accelerated failure time (AFT) model with right-censored data, supported by rigorous theoretical justification. Specifically, we propose an iterative reweighted generalized estimating equation (GEE) approach that incorporates Leurgans' synthetic variable method, establish the asymptotic properties of the resulting estimator, and derive a consistent variance estimator, enabling valid causal inference. Simulation studies are conducted to evaluate the finite-sample performance of the proposed method across different scenarios. The results show that it outperforms the naive unweighted GEE method, a parametric IV approach, and a one-stage estimator without IV. The proposed method is also highly scalable to large datasets, achieving a 300- to 1500-fold speedup relative to a Bayesian parametric IV approach in both simulations and the real-data example. We further illustrate the utility of the proposed method through a real-data application using the UK Biobank data.
[10]
arXiv:2509.10916
[pdf, html, other]
Title:
A Tutorial on Conducting Mediation Analysis with Exposure Mixtures
Yiran Wang, Yi-Ting Lin, Sean McGrath, John D. Meeker, Sung Kyun Park, Joshua L. Warren, Bhramar Mukherjee
Comments:
47 pages, 1 table and 5 figures in the main text; 5 tables and 13 figures in the supplementary material
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Causal mediation analysis is a powerful tool in environmental health research, allowing researchers to uncover the pathways through which exposures influence health outcomes. While traditional mediation methods have been widely applied to individual exposures, real-world scenarios often involve complex mixtures. Such mixtures introduce unique methodological challenges, including multicollinearity, sparsity of active exposures, and potential nonlinear and interactive effects. This paper provides an overview of several commonly used approaches for mediation analysis under exposure mixture settings with clear strategies and code for implementation. The methods include: single exposure mediation analysis (SE-MA), principal component-based mediation analysis, environmental risk score-based mediation analysis, and Bayesian kernel machine regression causal mediation analysis. While SE-MA serves as a baseline that analyzes each exposure individually, the other methods are designed to address the correlation and complexity inherent in exposure mixtures. For each method, we aim to clarify the target estimand and the assumptions that each method is making to render a causal interpretation of the estimates obtained. We conduct a simulation study to systematically evaluate the operating characteristics of these four methods to estimate global indirect effects and to identify individual exposures contributing to the global mediation under varying sample sizes, effect sizes, and exposure-mediator-outcome structures. We also illustrate their real-world applicability by examining data from the PROTECT birth cohort, specifically analyzing the relationship between prenatal exposure to phthalate mixtures and neonatal head circumference Z-score, with leukotriene E4 as a mediator. This example offers practical guidance for conducting mediation analysis in complex environmental contexts.
[11]
arXiv:2509.10963
[pdf, html, other]
Title:
Testing for LLM response differences: the case of a composite null consisting of semantically irrelevant query perturbations
Aranyak Acharyya, Carey E. Priebe, Hayden S. Helm
Subjects:
Statistics Theory (math.ST); Artificial Intelligence (cs.AI); Methodology (stat.ME)
Given an input query, generative models such as large language models produce a random response drawn from a response distribution. Given two input queries, it is natural to ask if their response distributions are the same. While traditional statistical hypothesis testing is designed to address this question, the response distribution induced by an input query is often sensitive to semantically irrelevant perturbations to the query, so much so that a traditional test of equality might indicate that two semantically equivalent queries induce statistically different response distributions. As a result, the outcome of the statistical test may not align with the user's requirements. In this paper, we address this misalignment by incorporating into the testing procedure consideration of a collection of semantically similar queries. In our setting, the mapping from the collection of user-defined semantically similar queries to the corresponding collection of response distributions is not known a priori and must be estimated, with a fixed budget. Although the problem we address is quite general, we focus our analysis on the setting where the responses are binary, show that the proposed test is asymptotically valid and consistent, and discuss important practical considerations with respect to power and computation.
[12]
arXiv:2509.10974
[pdf, html, other]
Title:
A Latent Factor Panel Approach to Spatiotemporal Causal Inference
Jiaxi Wu, Alexander Franks
Subjects:
Methodology (stat.ME)
Unmeasured confounding can severely bias causal effect estimates from spatiotemporal observational data, especially when the confounders do not vary smoothly in time and space. In this work, we develop a method for addressing unmeasured confounding in spatiotemporal contexts by building on models from the panel data literature and methods in multivariate causal inference. Our method is based on a factor confounding assumption, which posits that effects of unmeasured confounders on exposures and outcomes can be captured by a shared latent factor model. Factor confounding is sufficient to partially identify causal effects, even when there is interference between units. Additional assumptions that limit the degree of spatiotemporal interference, reasonable in most applications, are sufficient to point identify the effects. Simulation studies demonstrate that the proposed approach can substantially reduce omitted variable bias relative to other spatial smoothing and panel data baselines. We illustrate our method in a case study of the effect of prenatal PM2.5 exposure on birth weight in California.
[13]
arXiv:2509.11061
[pdf, html, other]
Title:
Varying-Coefficient Fr√©chet Regression
Yanzhao Wang, Jianqiang Zhang, Wangli Xu
Subjects:
Methodology (stat.ME)
As a growing number of problems involve variables that are random objects, the development of models for such data has become increasingly important. This paper introduces a novel varying-coefficient Fr√©chet regression model that extends the classical varying-coefficient framework to accommodate random objects as responses. The proposed model provides a unified methodology for analyzing both Euclidean and non-Euclidean response variables. We develop a comprehensive estimation procedure that accommodates diverse predictor settings. Specifically, the model allows the effect-modifier variable U to be either Euclidean or non-Euclidean, while the predictors X are assumed to be Euclidean. Tailored estimation methods are provided for each scenario. To examine the asymptotic properties of the estimators, we introduce a smoothed version of the model and establish convergence rates through separate theoretical analyses of the bias and stochastic terms. The effectiveness and practical utility of the proposed methodology are demonstrated through extensive simulation studies and a real-data application.
[14]
arXiv:2509.11070
[pdf, html, other]
Title:
Kernel-based Stochastic Approximation Framework for Nonlinear Operator Learning
Jia-Qi Yang, Lei Shi
Comments:
34 pages, 3 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Functional Analysis (math.FA); Numerical Analysis (math.NA); Statistics Theory (math.ST)
We develop a stochastic approximation framework for learning nonlinear operators between infinite-dimensional spaces utilizing general Mercer operator-valued kernels. Our framework encompasses two key classes: (i) compact kernels, which admit discrete spectral decompositions, and (ii) diagonal kernels of the form $K(x,x')=k(x,x')T$, where $k$ is a scalar-valued kernel and $T$ is a positive operator on the output space. This broad setting induces expressive vector-valued reproducing kernel Hilbert spaces (RKHSs) that generalize the classical $K=kI$ paradigm, thereby enabling rich structural modeling with rigorous theoretical guarantees. To address target operators lying outside the RKHS, we introduce vector-valued interpolation spaces to precisely quantify misspecification error. Within this framework, we establish dimension-free polynomial convergence rates, demonstrating that nonlinear operator learning can overcome the curse of dimensionality. The use of general operator-valued kernels further allows us to derive rates for intrinsically nonlinear operator learning, going beyond the linear-type behavior inherent in diagonal constructions of $K=kI$. Importantly, this framework accommodates a wide range of operator learning tasks, ranging from integral operators such as Fredholm operators to architectures based on encoder-decoder representations. Moreover, we validate its effectiveness through numerical experiments on the two-dimensional Navier-Stokes equations.
[15]
arXiv:2509.11089
[pdf, other]
Title:
What is in a Price? Estimating Willingness-to-Pay with Bayesian Hierarchical Models
Srijesh Pillai, Rajesh Kumar Chandrawat
Comments:
7 pages, 6 figures, 1 table. Accepted for publication in the proceedings of the 2025 Advances in Science and Engineering Technology International Conferences (ASET)
Subjects:
Applications (stat.AP); Machine Learning (cs.LG); Econometrics (econ.EM); Machine Learning (stat.ML)
For premium consumer products, pricing strategy is not about a single number, but about understanding the perceived monetary value of the features that justify a higher cost. This paper proposes a robust methodology to deconstruct a product's price into the tangible value of its constituent parts. We employ Bayesian Hierarchical Conjoint Analysis, a sophisticated statistical technique, to solve this high-stakes business problem using the Apple iPhone as a universally recognizable case study. We first simulate a realistic choice based conjoint survey where consumers choose between different hypothetical iPhone configurations. We then develop a Bayesian Hierarchical Logit Model to infer consumer preferences from this choice data. The core innovation of our model is its ability to directly estimate the Willingness-to-Pay (WTP) in dollars for specific feature upgrades, such as a "Pro" camera system or increased storage. Our results demonstrate that the model successfully recovers the true, underlying feature valuations from noisy data, providing not just a point estimate but a full posterior probability distribution for the dollar value of each feature. This work provides a powerful, practical framework for data-driven product design and pricing strategy, enabling businesses to make more intelligent decisions about which features to build and how to price them.
[16]
arXiv:2509.11103
[pdf, html, other]
Title:
KOO Method-based Consistent Clustering for Group-wise Linear Regression with Graph Structure
M. Ohishi, R. Oda
Comments:
18 pages, 1 figure
Subjects:
Methodology (stat.ME)
The kick-one-out (KOO) method is a variable selection method based on a model selection criterion. The method is very simple, and yet it has consistency in variable selection under a high-dimensional asymptotic framework with a specific model selection criterion. This paper proposes the join-twotogether (JTT) method, which is a clustering method based on the KOO method for group-wise linear regression with graph structure. The JTT method formulates the clustering problem as an edge selection problem for a graph and determines whether to select each edge based on the KOO method. We can employ network Lasso to perform such a clustering. However, network Lasso is somewhat cumbersome because there is no good algorithm for solving the associated optimization problem and the tuning is complicated. Therefore, by deriving a model selection criterion such that the JTT method has consistency in clustering under a high-dimensional asymptotic framework, we propose a simple yet powerful method that outperforms network Lasso.
[17]
arXiv:2509.11146
[pdf, html, other]
Title:
Maximum diversity, weighting and invariants of time series
Byungchang So
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Signal Processing (eess.SP); Metric Geometry (math.MG)
Magnitude, obtained as a special case of Euler characteristic of enriched category, represents a sense of the size of metric spaces and is related to classical notions such as cardinality, dimension, and volume. While the studies have explained the meaning of magnitude from various perspectives, continuity also gives a valuable view of magnitude. Based on established results about continuity of magnitude and maximum diversity, this article focuses on continuity of weighting, a distribution whose totality is magnitude, and its variation corresponding to maximum diversity. Meanwhile, recent studies also illuminated the connection between magnitude and data analysis by applying magnitude theory to point clouds representing the data or the set of model parameters. This article will also provide an application for time series analysis by introducing a new kind of invariants of periodic time series, where the invariance follows directly from the continuity results. As a use-case, a simple machine learning experiment is conducted with real-world data, in which the suggested invariants improved the performance.
[18]
arXiv:2509.11192
[pdf, other]
Title:
Time-varying Vine Copula model based on R-Vine structure and its application in financial risk research
XueZeng Yu
Subjects:
Applications (stat.AP)
The time-varying Vine Copula model has become a new direction in the Vine Copula class of models due to its time-varying structural parameters. We have observed that the Vine structures of the time-varying Vine Copula model currently used in economics and business research are C-Vine and D-Vine. These two structures are simpler than the R-Vine structure in modeling but will lose more details. Although truncation and simplification of the Vine structure are necessary when the number of variables is large, the number of variables in economics and business research is often small. Therefore, the R-Vine structure is definitely more suitable for constructing time-varying Vine Copula for economic research. Therefore, this paper uses the GAS (Generalized Autoregressive Score) model to dynamically parameterize the R-Vine structure to construct a time-varying Vine Copula model. The application of this model to the study of liquidity risks between China and Southeast Asian countries, including during the pandemic period, reveals that the time-varying model based on the R-Vine structure not only achieves better statistical test results but also better reflects economic and even political realities compared to the other two structural time-varying models.
[19]
arXiv:2509.11204
[pdf, html, other]
Title:
Bayesian model updating via streamlined Bayesian active learning cubature
Pei-Pei Li, Chao Dang, Crist√≥bal H. Acevedo, Marcos A. Valdebenito, Matthias G.R. Faes
Subjects:
Computation (stat.CO)
This paper proposes a novel Bayesian active learning method for Bayesian model updating, which is termed as "Streamlined Bayesian Active Learning Cubature" (SBALC). The core idea is to approximate the log-likelihood function using Gaussian process (GP) regression in a streamlined Bayesian active learning way. Rather than generating many samples from the posterior GP, we only use its mean and variance function to form the model evidence estimator, stopping criterion, and learning function. Specifically, the estimation of model evidence is first treated as a Bayesian cubature problem, with a GP prior assigned over the log-likelihood function. Second, a plug-in estimator for model evidence is proposed based on the posterior mean function of the GP. Third, an upper bound on the expected absolute error between the posterior model evidence and its plug-in estimator is derived. Building on this result, a novel stopping criterion and learning function are proposed using only the posterior mean and standard deviation functions of the GP. Finally, we can obtain the model evidence based on the posterior mean function of the log-likelihood function in conjunction with Monte Carlo simulation, as well as the samples for the posterior distribution of model parameters as a by-product. Four numerical examples are presented to demonstrate the accuracy and efficiency of the proposed method compared to several existing approaches. The results show that the method can significantly reduce the number of model evaluations and the computational time without compromising accuracy.
[20]
arXiv:2509.11208
[pdf, html, other]
Title:
Predictable Compression Failures: Why Language Models Actually Hallucinate
Leon Chlon, Ahmed Karim, Maggie Chlon
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Large language models perform near-Bayesian inference yet violate permutation invariance on exchangeable data. We resolve this by showing transformers minimize expected conditional description length (cross-entropy) over orderings, $\mathbb{E}_\pi[\ell(Y \mid \Gamma_\pi(X))]$, which admits a Kolmogorov-complexity interpretation up to additive constants, rather than the permutation-invariant description length $\ell(Y \mid X)$. This makes them Bayesian in expectation, not in realization. We derive (i) a Quantified Martingale Violation bound showing order-induced deviations scale as $O(\log n)$ with constants; (ii) the Expectation-level Decompression Law linking information budgets to reliability for Bernoulli predicates; and (iii) deployable planners (B2T/RoH/ISR) for answer/abstain decisions. Empirically, permutation dispersion follows $a+b\ln n$ (Qwen2-7B $b \approx 0.377$, Llama-3.1-8B $b \approx 0.147$); permutation mixtures improve ground-truth likelihood/accuracy; and randomized dose-response shows hallucinations drop by $\sim 0.13$ per additional nat. A pre-specified audit with a fixed ISR=1.0 achieves near-0\% hallucinations via calibrated refusal at 24\% abstention. The framework turns hallucinations into predictable compression failures and enables principled information budgeting.
[21]
arXiv:2509.11316
[pdf, html, other]
Title:
Contrastive Network Representation Learning
Zihan Dong, Xin Zhou, Ryumei Nakada, Lexin Li, Linjun Zhang
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
Network representation learning seeks to embed networks into a low-dimensional space while preserving the structural and semantic properties, thereby facilitating downstream tasks such as classification, trait prediction, edge identification, and community detection. Motivated by challenges in brain connectivity data analysis that is characterized by subject-specific, high-dimensional, and sparse networks that lack node or edge covariates, we propose a novel contrastive learning-based statistical approach for network edge embedding, which we name as Adaptive Contrastive Edge Representation Learning (ACERL). It builds on two key components: contrastive learning of augmented network pairs, and a data-driven adaptive random masking mechanism. We establish the non-asymptotic error bounds, and show that our method achieves the minimax optimal convergence rate for edge representation learning. We further demonstrate the applicability of the learned representation in multiple downstream tasks, including network classification, important edge detection, and community detection, and establish the corresponding theoretical guarantees. We validate our method through both synthetic data and real brain connectivities studies, and show its competitive performance compared to the baseline method of sparse principal components analysis.
[22]
arXiv:2509.11333
[pdf, html, other]
Title:
BE-BOIN: A Dose Optimization Design Accommodating Backfill and Late-Onset Toxicity
Kai Chen, Yixuan Zhao, Kentaro Takeda, Ying Yuan
Subjects:
Methodology (stat.ME)
The US Food and Drug Administration (FDA) launched Project Optimus and issued guidance to reform dose-finding and selection trials, shifting the paradigm from identifying the maximum tolerable dose (MTD) to determining the optimal biological dose (OBD), which optimizes the risk and benefit of treatments. The FDA's guidance emphasizes the importance of collecting sufficient toxicity and efficacy data across multiple doses and considering late-onset cumulative toxicity that often results in tolerability issues. To address these challenges, we propose the BE-BOIN (Backfill time-to-Event Bayesian Optimal INterval) design, which allows backfilling patients into safe and effective doses during dose escalation and accommodates late-onset toxicities. BE-BOIN enables the collection of additional safety and efficacy data to enhance the accuracy and reliability of OBD selection and supports real-time dose decisions for new patients. Our simulation studies show that BE-BOIN accurately identifies the MTD and OBD while significantly reducing trial duration.
[23]
arXiv:2509.11338
[pdf, html, other]
Title:
Next-Generation Reservoir Computing for Dynamical Inference
Rok Cestnik, Erik A. Martens
Comments:
10 pages, 10 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We present a simple and scalable implementation of next-generation reservoir computing for modeling dynamical systems from time series data. Our approach uses a pseudorandom nonlinear projection of time-delay embedded input, allowing an arbitrary dimension of the feature space, thus providing a flexible alternative to the polynomial-based projections used in previous next-generation reservoir computing variants. We apply the method to benchmark tasks -- including attractor reconstruction and bifurcation diagram estimation -- using only partial and noisy observations. We also include an exploratory example of estimating asymptotic oscillation phases. The models remain stable over long rollouts and generalize beyond training data. This framework enables the precise control of system state and is well suited for surrogate modeling and digital twin applications.
[24]
arXiv:2509.11379
[pdf, html, other]
Title:
Some Robustness Properties of Label Cleaning
Chen Cheng, John Duchi
Comments:
39 pages
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)
We demonstrate that learning procedures that rely on aggregated labels, e.g., label information distilled from noisy responses, enjoy robustness properties impossible without data cleaning. This robustness appears in several ways. In the context of risk consistency -- when one takes the standard approach in machine learning of minimizing a surrogate (typically convex) loss in place of a desired task loss (such as the zero-one mis-classification error) -- procedures using label aggregation obtain stronger consistency guarantees than those even possible using raw labels. And while classical statistical scenarios of fitting perfectly-specified models suggest that incorporating all possible information -- modeling uncertainty in labels -- is statistically efficient, consistency fails for ``standard'' approaches as soon as a loss to be minimized is even slightly mis-specified. Yet procedures leveraging aggregated information still converge to optimal classifiers, highlighting how incorporating a fuller view of the data analysis pipeline, from collection to model-fitting to prediction time, can yield a more robust methodology by refining noisy signals.
[25]
arXiv:2509.11381
[pdf, html, other]
Title:
The Honest Truth About Causal Trees: Accuracy Limits for Heterogeneous Treatment Effect Estimation
Matias D. Cattaneo, Jason M. Klusowski, Ruiqi Rae Yu
Subjects:
Statistics Theory (math.ST); Econometrics (econ.EM); Methodology (stat.ME); Machine Learning (stat.ML)
Recursive decision trees have emerged as a leading methodology for heterogeneous causal treatment effect estimation and inference in experimental and observational settings. These procedures are fitted using the celebrated CART (Classification And Regression Tree) algorithm [Breiman et al., 1984], or custom variants thereof, and hence are believed to be "adaptive" to high-dimensional data, sparsity, or other specific features of the underlying data generating process. Athey and Imbens [2016] proposed several "honest" causal decision tree estimators, which have become the standard in both academia and industry. We study their estimators, and variants thereof, and establish lower bounds on their estimation error. We demonstrate that these popular heterogeneous treatment effect estimators cannot achieve a polynomial-in-$n$ convergence rate under basic conditions, where $n$ denotes the sample size. Contrary to common belief, honesty does not resolve these limitations and at best delivers negligible logarithmic improvements in sample size or dimension. As a result, these commonly used estimators can exhibit poor performance in practice, and even be inconsistent in some settings. Our theoretical insights are empirically validated through simulations.
[26]
arXiv:2509.11435
[pdf, html, other]
Title:
A Particle-Flow Algorithm for Free-Support Wasserstein Barycenters
Kisung You
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO)
The Wasserstein barycenter extends the Euclidean mean to the space of probability measures by minimizing the weighted sum of squared 2-Wasserstein distances. We develop a free-support algorithm for computing Wasserstein barycenters that avoids entropic regularization and instead follows the formal Riemannian geometry of Wasserstein space. In our approach, barycenter atoms evolve as particles advected by averaged optimal-transport displacements, with barycentric projections of optimal transport plans used in place of Monge maps when the latter do not exist. This yields a geometry-aware particle-flow update that preserves sharp features of the Wasserstein barycenter while remaining computationally tractable. We establish theoretical guarantees, including consistency of barycentric projections, monotone descent and convergence to stationary points, stability with respect to perturbations of the inputs, and resolution consistency as the number of atoms increases. Empirical studies on averaging probability distributions, Bayesian posterior aggregation, image prototypes and classification, and large-scale clustering demonstrate accuracy and scalability of the proposed particle-flow approach, positioning it as a principled alternative to both linear programming and regularized solvers.
[27]
arXiv:2509.11455
[pdf, html, other]
Title:
Unified Distributed Estimation Framework for Sufficient Dimension Reduction Based on Conditional Moments
Hongying Li, Minyi Zhu, Yaqi Cao, Xinyi Xu
Subjects:
Methodology (stat.ME); Computation (stat.CO)
Nowadays, massive datasets are typically dispersed across multiple locations, encountering dual challenges of high dimensionality and huge sample size. Therefore, it is necessary to explore sufficient dimension reduction (SDR) methods for distributed data. In this paper, we first propose an exact distributed estimation of sliced inverse regression, which substantially improves computational efficiency while obtaining identical estimation as that on the full sample. Then, we propose a unified distributed framework for general conditional-moment-based inverse regression methods. This framework allows for distinct population structure for data distributed at different locations, thus addressing the issue of heterogeneity. To assess the effectiveness of our proposed methods, we conduct simulations incorporating various data generation mechanisms, and examine scenarios where samples are homogeneous equally, heterogeneous equally, and heterogeneous unequally scattered across local nodes. Our findings highlight the versatility and applicability of the unified framework. Meanwhile, the communication cost is practically acceptable and the computation cost is greatly reduced. Sensitivity analysis verifies the robustness of the algorithm under extreme conditions where the SDR method locally fails on some nodes. A real data analysis also demonstrates the superior performance of the algorithm.
[28]
arXiv:2509.11472
[pdf, html, other]
Title:
A New Class of Mark-Specific Proportional Hazards Models for Recurrent Events: Application to Opioid Refills Among Post-Surgical Patients
Eileen Yang, Donglin Zeng, Mark Bicket, Yi Li
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Prescription opioids relieve moderate-to-severe pain after surgery, but overprescription can lead to misuse and overdose. Understanding factors associated with post-surgical opioid refills is crucial for improving pain management and reducing opioid-related harms. Conventional methods often fail to account for refill size or dosage and capture patient risk dynamics. We address this gap by treating dosage as a continuously varying mark for each refill event and proposing a new class of mark-specific proportional hazards models for recurrent events. Our marginal model, developed on the gap-time scale with a dual weighting scheme, accommodates event proximity to dosage of interest while accounting for the informative number of recurrences. We establish consistency and asymptotic normality of the estimator and provide a sandwich variance estimator for robust inference. Simulations show improved finite-sample performance over competing methods. We apply the model to data from the Michigan Surgical Quality Collaborative and Michigan Automated Prescription System. Results show that high BMI, smoking, cancer, and open surgery increase hazards of high-dosage refills, while inpatient surgeries elevate refill hazards across all dosages. Black race is associated with higher hazards of low-dosage but lower hazards of high-dosage refills. These findings may inform personalized, dosage-specific pain management strategies.
[29]
arXiv:2509.11479
[pdf, html, other]
Title:
Learn-As-you-GO (LAGO) Trials: Optimizing Trials for Effectiveness and Power to Prevent Failed Trials
Ante Bing (Department of Mathematics and Statistics, Boston University), Donna Spiegelman (Department of Biostatistics, Yale University), Judith J. Lok (Department of Mathematics and Statistics, Boston University)
Subjects:
Methodology (stat.ME)
The Learn-As-you-GO (LAGO) design provides a rigorous framework for adapting the intervention package based on accumulating data while the trial is ongoing. This article improves the flexibility of the LAGO design by incorporating statistical power as an optimization criterion (power goal) in LAGO optimizations. We propose the unconditional and conditional power approaches to add a power goal. Both approaches estimate the power at the end of the LAGO trial using data from prior stages, and increase the power at the end of the LAGO trial when the original trial was underpowered. Including a power goal maintains the asymptotic properties of the estimators of the treatment effect while preserving the asymptotic level of the statistical test at the end of the trial. We illustrate the benefits of our methods through a retrospective application to the BetterBirth Study, a large-scale study of maternal-newborn care that failed to show a significant effect on its primary outcome. This analysis demonstrates how our methods could have led to more intensive interventions and potentially significant results. The LAGO design with power goal optimizations provides investigators with a powerful tool to reduce the risk of failed trials due to insufficient power.
[30]
arXiv:2509.11511
[pdf, html, other]
Title:
Learning Majority-to-Minority Transformations with MMD and Triplet Loss for Imbalanced Classification
Suman Cha, Hyunjoong Kim
Comments:
.19 pages, 6 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Class imbalance in supervised classification often degrades model performance by biasing predictions toward the majority class, particularly in critical applications such as medical diagnosis and fraud detection. Traditional oversampling techniques, including SMOTE and its variants, generate synthetic minority samples via local interpolation but fail to capture global data distributions in high-dimensional spaces. Deep generative models based on GANs offer richer distribution modeling yet suffer from training instability and mode collapse under severe imbalance. To overcome these limitations, we introduce an oversampling framework that learns a parametric transformation to map majority samples into the minority distribution. Our approach minimizes the maximum mean discrepancy (MMD) between transformed and true minority samples for global alignment, and incorporates a triplet loss regularizer to enforce boundary awareness by guiding synthesized samples toward challenging borderline regions. We evaluate our method on 29 synthetic and real-world datasets, demonstrating consistent improvements over classical and generative baselines in AUROC, G-mean, F1-score, and MCC. These results confirm the robustness, computational efficiency, and practical utility of the proposed framework for imbalanced classification tasks.
[31]
arXiv:2509.11519
[pdf, html, other]
Title:
Mendelian Randomization Methods for Causal Inference: Estimands, Identification and Inference
Minhao Yao, Anqi Wang, Xihao Li, Zhonghua Liu
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Mendelian randomization (MR) has become an essential tool for causal inference in biomedical and public health research. By using genetic variants as instrumental variables, MR helps address unmeasured confounding and reverse causation, offering a quasi-experimental framework to evaluate causal effects of modifiable exposures on health outcomes. Despite its promise, MR faces substantial methodological challenges, including invalid instruments, weak instrument bias, and design complexities across different data structures. In this tutorial review, we provide a comprehensive overview of MR methods for causal inference, emphasizing clarity of causal interpretation, study design comparisons, availability of software tools, and practical guidance for applied scientists. We organize the review around causal estimands, ensuring that analyses are anchored to well-defined causal questions. We discuss the problems of invalid and weak instruments, comparing available strategies for their detection and correction. We integrate discussions of population-based versus family-based MR designs, analyses based on individual-level versus summary-level data, and one-sample versus two-sample MR designs, highlighting their relative advantages and limitations. We also summarize recent methodological advances and software developments that extend MR to settings with many weak or invalid instruments and to modern high-dimensional omics data. Real-data applications, including UK Biobank and Alzheimer's disease proteomics studies, illustrate the use of these methods in practice. This review aims to serve as a tutorial-style reference for both methodologists and applied scientists.
[32]
arXiv:2509.11532
[pdf, html, other]
Title:
E-ROBOT: a dimension-free method for robust statistics and machine learning via Schr√∂dinger bridge
Davide La Vecchia, Hang Liu
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We propose the Entropic-regularized Robust Optimal Transport (E-ROBOT) framework, a novel method that combines the robustness of ROBOT with the computational and statistical benefits of entropic regularization. We show that, rooted in the Schr√∂dinger bridge problem theory, E-ROBOT defines the robust Sinkhorn divergence $\overline{W}_{\varepsilon,\lambda}$, where the parameter $\lambda$ controls robustness and $\varepsilon$ governs the regularization strength. Letting $n\in \mathbb{N}$ denote the sample size, a central theoretical contribution is establishing that the sample complexity of $\overline{W}_{\varepsilon,\lambda}$ is $\mathcal{O}(n^{-1/2})$, thereby avoiding the curse of dimensionality that plagues standard ROBOT. This dimension-free property unlocks the use of $\overline{W}_{\varepsilon,\lambda}$ as a loss function in large-dimensional statistical and machine learning tasks. With this regard, we demonstrate its utility through four applications: goodness-of-fit testing; computation of barycenters for corrupted 2D and 3D shapes; definition of gradient flows; and image colour transfer. From the computation standpoint, a perk of our novel method is that it can be easily implemented by modifying existing (\texttt{Python}) routines. From the theoretical standpoint, our work opens the door to many research directions in statistics and machine learning: we discuss some of them.
[33]
arXiv:2509.11546
[pdf, other]
Title:
Association between Air Pollutants and Hospitalizations for Cardiovascular Diseases: Time-Series Analysis in S√£o Paulo, 2010-2019
Carlos Souto dos Santos Filho (1), Ana J√∫lia Alves C√¢mara (2), Guilherme Aparecido Santos Aguilar (1) ((1) S√£o Paulo State University, (2) Federal University of Esp√≠rito Santo)
Comments:
18 pages with 4 figures
Subjects:
Applications (stat.AP); Methodology (stat.ME)
Cardiovascular diseases (CVD) remain one of the leading causes of hospitalization in Brazil. Exposure to air pollutants such as PM$_{10}$ $\mu$m, NO$_2$, and SO$_2$ has been associated with the worsening of these diseases, especially in urban areas. This study evaluated the association between the daily concentration of these pollutants and daily hospitalizations for acute myocardial infarction and cerebrovascular diseases in S√£o Paulo (2010-2019), using generalized additive models with a lag of 0 to 4 days. Two approaches for choosing the degrees of freedom in temporal smoothing were compared: based on pollutant prediction and based on outcome prediction (hospitalizations). Data were obtained from official government databases. The modeling used the quasi-Poisson family in R software (v. 4.4.0). Models with exposure-based smoothing generated more consistent estimates. For PM10{\mu}m, the cumulative risk estimate for exposure was 1.08%, while for hospitalization, it was 1.20%. For NO$_2$, the estimated risk was 1.47% (exposure) versus 1.33% (hospitalization). For SO$_2$, a striking difference was observed: 7.66% (exposure) versus 14.31% (hospitalization). The significant lags were on days 0, 1, and 2. The results show that smoothing based on outcome prediction can generate bias, masking the true effect of pollutants. The appropriate choice of df in the smoothing function is crucial. Smoothing by the pollutant series was more robust and accurate, contributing to methodological improvements in time-series studies and reinforcing the importance of public policies for pollution control.
[34]
arXiv:2509.11650
[pdf, other]
Title:
About the Multiplicative Inverse of a Non-Zero-Mean Gaussian Process
Marco Lanucara
Subjects:
Statistics Theory (math.ST)
We study the spectral properties of a stochastic process obtained by multiplicative inversion of a non-zero-mean Gaussian process. We show that its autocorrelation and power spectrum exist for most regular processes, and we find a convergent series expansion of the autocorrelation function in powers of the ratio between mean and standard deviation of the underlying Gaussian process. We apply the results to two sample processes, and we validate the theoretical results with simulations based on standard signal processing techniques.
[35]
arXiv:2509.11675
[pdf, other]
Title:
SpaPool: Soft Partition Assignment Pooling for__Graph Neural Networks
Rodrigue Govan (ISEA), Romane Scherrer (ISEA), Philippe Fournier-Viger, Nazha Selmaoui-Folcher (ISEA)
Journal-ref:
International Conference on Big Data Analytics and Knowledge Discovery, Aug 2025, Bangkok, Thailand. pp.332-340
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
This paper introduces SpaPool, a novel pooling method that combines the strengths of both dense and sparse techniques for a graph neural network. SpaPool groups vertices into an adaptive number of clusters, leveraging the benefits of both dense and sparse approaches. It aims to maintain the structural integrity of the graph while reducing its size efficiently. Experimental results on several datasets demonstrate that SpaPool achieves competitive performance compared to existing pooling techniques and excels particularly on small-scale graphs. This makes SpaPool a promising method for applications requiring efficient and effective graph processing.
[36]
arXiv:2509.11741
[pdf, html, other]
Title:
Tidy simulation: Designing robust, reproducible, and scalable Monte Carlo simulations
Erik-Jan van Kesteren
Comments:
16 pages, 3 figures
Subjects:
Computation (stat.CO)
Monte Carlo simulation studies are at the core of the modern applied, computational, and theoretical statistical literature. Simulation is a broadly applicable research tool, used to collect data on the relative performance of methods or data analysis approaches under a well-defined data-generating process. However, extant literature focuses largely on design aspects of simulation, rather than implementation strategies aligned with the current state of (statistical) programming languages, portable data formats, and multi-node cluster computing.
In this work, I propose tidy simulation: a simple, language-agnostic, yet flexible functional framework for designing, writing, and running simulation studies. It has four components: a tidy simulation grid, a data generation function, an analysis function, and a results table. Using this structure, even the smallest simulations can be written in a consistent, modular way, yet they can be readily scaled to thousands of nodes in a computer cluster should the need arise. Tidy simulation also supports the iterative, sometimes exploratory nature of simulation-based experiments. By adopting the tidy simulation approach, researchers can implement their simulations in a robust, reproducible, and scalable way, which contributes to high-quality statistical science.
[37]
arXiv:2509.11751
[pdf, html, other]
Title:
Scalable Variable Selection and Model Averaging for Latent Regression Models Using Approximate Variational Bayes
Gregor Zens, Mark F.J. Steel
Subjects:
Methodology (stat.ME)
We propose a fast and theoretically grounded method for Bayesian variable selection and model averaging in latent variable regression models. Our framework addresses three interrelated challenges: (i) intractable marginal likelihoods, (ii) exponentially large model spaces, and (iii) computational costs in large samples. We introduce a novel integrated likelihood approximation based on mean-field variational posterior approximations and establish its asymptotic model selection consistency under broad conditions. To reduce the computational burden, we develop an approximate variational Bayes scheme that fixes the latent regression outcomes for all models at initial estimates obtained under a baseline null model. Despite its simplicity, this approach locally and asymptotically preserves the model-selection behavior of the full variational Bayes approach to first order, at a fraction of the computational cost. Extensive numerical studies - covering probit, tobit, semi-parametric count data models and Poisson log-normal regression - demonstrate accurate inference and large speedups, often reducing runtime from days to hours with comparable accuracy. Applications to real-world data further highlight the practical benefits of the methods for Bayesian inference in large samples and under model uncertainty.
[38]
arXiv:2509.11821
[pdf, html, other]
Title:
Covering Unknown Correlations in Bayesian Priors by Inflating Uncertainties
Lukas Koch
Subjects:
Methodology (stat.ME)
Bayesian analyses require that all variable model parameters are given a prior probability distribution. This can pose a challenge for analyses where multiple experiments are combined if these experiments use different parametrisations for their nuisance parameters. If the parameters in the two models describe exactly the same physics, they should be 100% correlated in the prior. If the parameters describe independent physics, they should be uncorrelated. But if they describe related or overlapping physics, it is not trivial to determine what the joint prior distribution should look like. Even if the priors for each experiment are well motivated, the unknown correlations between them can have unintended consequences for the posterior probability of the parameters of interest, potentially leading to underestimated uncertainties. In this paper we show that it is possible to choose a prior parametrisation that ensures conservative posterior uncertainties for the parameters of interest under some very general assumptions.
[39]
arXiv:2509.11859
[pdf, other]
Title:
Statistical Model Checking Beyond Means: Quantiles, CVaR, and the DKW Inequality (extended version)
Carlos E. Budde, Arnd Hartmanns, Tobias Meggendorfer, Maximilian Weininger, Patrick Wienh√∂ft
Comments:
Extended version of the article "Statistical Model Checking Beyond Means: Quantiles, CVaR, and the DKW Inequality" presented/published at the 2nd International Joint Conference on Quantitative Evaluation of Systems and Formal Modeling and Analysis of Timed Systems (QEST+FORMATS 2025), 26-28 August 2025, Aarhus, Denmark (this https URL)
Subjects:
Methodology (stat.ME); Discrete Mathematics (cs.DM); Logic in Computer Science (cs.LO)
Statistical model checking (SMC) randomly samples probabilistic models to approximate quantities of interest with statistical error guarantees. It is traditionally used to estimate probabilities and expected rewards, i.e. means of different random variables on paths. In this paper, we develop methods using the Dvoretzky-Kiefer-Wolfowitz-Massart inequality (DKW) to extend SMC beyond means to compute quantities such as quantiles, conditional value-at-risk, and entropic risk. The DKW provides confidence bounds on the random variable's entire cumulative distribution function, a much more versatile guarantee compared to the statistical methods prevalent in SMC today. We have implemented support for computing new quantities via the DKW in the 'modes' simulator of the Modest Toolset. We highlight the implementation and its versatility on benchmarks from the quantitative verification literature.
[40]
arXiv:2509.11903
[pdf, html, other]
Title:
Wavelet-SARIMA-Transformer: A Hybrid Model for Rainfall Forecasting
Junmoni Saikia, Kuldeep Goswami, Sarat C. Kakaty
Subjects:
Applications (stat.AP); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)
This study develops and evaluates a novel hybridWavelet SARIMA Transformer, WST framework to forecast using monthly rainfall across five meteorological subdivisions of Northeast India over the 1971 to 2023 period. The approach employs the Maximal Overlap Discrete Wavelet Transform, MODWT with four wavelet families such as, Haar, Daubechies, Symlet, Coiflet etc. to achieve shift invariant, multiresolution decomposition of the rainfall series. Linear and seasonal components are modeled using Seasonal ARIMA, SARIMA, while nonlinear components are modeled by a Transformer network, and forecasts are reconstructed via inverse MODWT. Comprehensive validation using an 80 is to 20 train test split and multiple performance indices such as, RMSE, MAE, SMAPE, Willmotts d, Skill Score, Percent Bias, Explained Variance, and Legates McCabes E1 demonstrates the superiority of the Haar-based hybrid model, WHST. Across all subdivisions, WHST consistently achieved lower forecast errors, stronger agreement with observed rainfall, and unbiased predictions compared with stand alone SARIMA, stand-alone Transformer, and two-stage wavelet hybrids. Residual adequacy was confirmed through the Ljung Box test, while Taylor diagrams provided an inte- grated assessment of correlation, variance fidelity, and RMSE, further reinforcing the robustness of the proposed approach. The results highlight the effectiveness of integrating multiresolution signal decomposition with complementary linear and deep learning models for hydroclimatic forecasting. Beyond rainfall, the proposed WST framework offers a scalable methodology for forecasting complex environmental time series, with direct implications for flood risk management, water resources planning, and climate adaptation strategies in data-sparse and climate-sensitive regions.
[41]
arXiv:2509.11962
[pdf, html, other]
Title:
Identifiable Autoregressive Variational Autoencoders for Nonlinear and Nonstationary Spatio-Temporal Blind Source Separation
Mika Sipil√§, Klaus Nordhausen, Sara Taskinen
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
The modeling and prediction of multivariate spatio-temporal data involve numerous challenges. Dimension reduction methods can significantly simplify this process, provided that they account for the complex dependencies between variables and across time and space. Nonlinear blind source separation has emerged as a promising approach, particularly following recent advances in identifiability results. Building on these developments, we introduce the identifiable autoregressive variational autoencoder, which ensures the identifiability of latent components consisting of nonstationary autoregressive processes. The blind source separation efficacy of the proposed method is showcased through a simulation study, where it is compared against state-of-the-art methods, and the spatio-temporal prediction performance is evaluated against several competitors on air pollution and weather datasets.
[42]
arXiv:2509.12028
[pdf, html, other]
Title:
Modeling Non-Uniform Hypergraphs Using Determinantal Point Processes
Yichao Chen, Jingfei Zhang, Ji Zhu
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
Most statistical models for networks focus on pairwise interactions between nodes. However, many real-world networks involve higher-order interactions among multiple nodes, such as co-authors collaborating on a paper. Hypergraphs provide a natural representation for these networks, with each hyperedge representing a set of nodes. The majority of existing hypergraph models assume uniform hyperedges (i.e., edges of the same size) or rely on diversity among nodes. In this work, we propose a new hypergraph model based on non-symmetric determinantal point processes. The proposed model naturally accommodates non-uniform hyperedges, has tractable probability mass functions, and accounts for both node similarity and diversity in hyperedges. For model estimation, we maximize the likelihood function under constraints using a computationally efficient projected adaptive gradient descent algorithm. We establish the consistency and asymptotic normality of the estimator. Simulation studies confirm the efficacy of the proposed model, and its utility is further demonstrated through edge predictions on several real-world datasets.
[43]
arXiv:2509.12051
[pdf, html, other]
Title:
A comparison between geostatistical and machine learning models for spatio-temporal prediction of PM2.5 data
Zeinab Mohamed, Wenlong Gong
Subjects:
Applications (stat.AP); Machine Learning (stat.ML)
Ambient air pollution poses significant health and environmental challenges. Exposure to high concentrations of PM$_{2.5}$ have been linked to increased respiratory and cardiovascular hospital admissions, more emergency department visits and deaths. Traditional air quality monitoring systems such as EPA-certified stations provide limited spatial and temporal data. The advent of low-cost sensors has dramatically improved the granularity of air quality data, enabling real-time, high-resolution monitoring. This study exploits the extensive data from PurpleAir sensors to assess and compare the effectiveness of various statistical and machine learning models in producing accurate hourly PM$_{2.5}$ maps across California. We evaluate traditional geostatistical methods, including kriging and land use regression, against advanced machine learning approaches such as neural networks, random forests, and support vector machines, as well as ensemble model. Our findings enhanced the predictive accuracy of PM2.5 concentration by correcting the bias in PurpleAir data with an ensemble model, which incorporating both spatiotemporal dependencies and machine learning models.
[44]
arXiv:2509.12066
[pdf, html, other]
Title:
On the universal calibration of Pareto-type linear combination tests
Parijat Chakraborty, F. Richard Guo, Kerby Shedden, Stilian Stoev
Subjects:
Statistics Theory (math.ST); Probability (math.PR); Applications (stat.AP); Methodology (stat.ME)
It is often of interest to test a global null hypothesis using multiple, possibly dependent, $p$-values by combining their strengths while controlling the Type I error. Recently, several heavy-tailed combinations tests, such as the harmonic mean test and the Cauchy combination test, have been proposed: they map $p$-values into heavy-tailed random variables before combining them in some fashion into a single test statistic. The resulting tests, which are calibrated under the assumption of independence of the $p$-values, have shown to be rather robust to dependence. The complete understanding of the calibration properties of the resulting combination tests of dependent and possibly tail-dependent $p$-values has remained an important open problem in the area. In this work, we show that the powerful framework of multivariate regular variation (MRV) offers a nearly complete solution to this problem.
We first show that the precise asymptotic calibration properties of a large class of homogeneous combination tests can be expressed in terms of the angular measure -- a characteristic of the asymptotic tail-dependence under MRV. Consequently, we show that under MRV, the Pareto-type linear combination tests, which are equivalent to the harmonic mean test, are universally calibrated regardless of the tail-dependence structure of the underlying $p$-values. In contrast, the popular Cauchy combination test is shown to be universally honest but often conservative; the Tippet combination test, while being honest, is calibrated if and only if the underlying $p$-values are tail-independent.
One of our major findings is that the Pareto-type linear combination tests are the only universally calibrated ones among the large family of possibly non-linear homogeneous heavy-tailed combination tests.
[45]
arXiv:2509.12122
[pdf, html, other]
Title:
Least squares-based methods to bias adjustment in scalar-on-function regression model using a functional instrumental variable
Xiwei Chen, Ufuk Beyaztas, Caihong Qin, Heyang Ji, Gilson Honvoh, Roger S. Zoh, Lan Xue, Carmen D. Tekwe
Subjects:
Methodology (stat.ME)
Instrumental variables are widely used to adjust for measurement error bias when assessing associations of health outcomes with ME prone independent variables. IV approaches addressing ME in longitudinal models are well established, but few methods exist for functional regression. We develop two methods to adjust for ME bias in scalar on function linear models. We regress a scalar outcome on an ME prone functional variable using a functional IV for model identification and propose two least squares based methods to adjust for ME bias. Our methods alleviate potential computational challenges encountered when applying classical regression calibration methods for bias adjustment in high dimensional settings and adjust for potential serial correlations across time. Simulations demonstrate faster run times, lower bias, and lower AIMSE for the proposed methods when compared to existing approaches. The proposed methods were applied to investigate the association between body mass index and wearable device-based physical activity intensity among community dwelling adults living in the United States.
[46]
arXiv:2509.12135
[pdf, html, other]
Title:
Evidencing preferential attachment in dependency network evolution
Clement Lee
Comments:
34 pages, 10 figures, 1 table
Subjects:
Applications (stat.AP); Social and Information Networks (cs.SI)
Preferential attachment is often suggested to be the underlying mechanism of the growth of a network, largely due to that many real networks are, to a certain extent, scale-free. However, such attribution is usually made under debatable practices of determining scale-freeness and when only snapshots of the degree distribution are observed. In the presence of the evolution history of the network, modelling the increments of the evolution allows us to measure preferential attachment directly. Therefore, we propose a generalised linear model for such purpose, where the in-degrees and their increments are the covariate and response, respectively. Not only are the parameters that describe the preferential attachment directly incorporated, they also ensure that the tail heaviness of the asymptotic degree distribution is realistic. The Bayesian approach to inference enables the hierarchical version of the model to be implemented naturally. The application to the dependency network of R packages reveals subtly different behaviours between new dependencies by new and existing packages, and between addition and removal of dependencies.
[47]
arXiv:2509.12166
[pdf, html, other]
Title:
MMM: Clustering Multivariate Longitudinal Mixed-type Data
Francesco Amato, Julien Jacques
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
Multivariate longitudinal data of mixed-type are increasingly collected in many science domains. However, algorithms to cluster this kind of data remain scarce, due to the challenge to simultaneously model the within- and between-time dependence structures for multivariate data of mixed kind. We introduce the Mixture of Mixed-Matrices (MMM) model: reorganizing the data in a three-way structure and assuming that the non-continuous variables are observations of underlying latent continuous variables, the model relies on a mixture of matrix-variate normal distributions to perform clustering in the latent dimension. The MMM model is thus able to handle continuous, ordinal, binary, nominal and count data and to concurrently model the heterogeneity, the association among the responses and the temporal dependence structure in a parsimonious way and without assuming conditional independence. The inference is carried out through an MCMC-EM algorithm, which is detailed. An evaluation of the model through synthetic data shows its inference abilities. A real-world application on financial data is presented.
[48]
arXiv:2509.12173
[pdf, other]
Title:
Extrapolation of Tempered Posteriors
Mengxin Xi, Zheyang Shen, Marina Riabiz, Nicolas Chopin, Chris J. Oates
Comments:
52 pages, 10 figures
Subjects:
Computation (stat.CO); Statistics Theory (math.ST); Methodology (stat.ME)
Tempering is a popular tool in Bayesian computation, being used to transform a posterior distribution $p_1$ into a reference distribution $p_0$ that is more easily approximated. Several algorithms exist that start by approximating $p_0$ and proceed through a sequence of intermediate distributions $p_t$ until an approximation to $p_1$ is obtained. Our contribution reveals that high-quality approximation of terms up to $p_1$ is not essential, as knowledge of the intermediate distributions enables posterior quantities of interest to be extrapolated. Specifically, we establish conditions under which posterior expectations are determined by their associated tempered expectations on any non-empty $t$ interval. Harnessing this result, we propose novel methodology for approximating posterior expectations based on extrapolation and smoothing of tempered expectations, which we implement as a post-processing variance-reduction tool for sequential Monte Carlo.
[49]
arXiv:2509.12185
[pdf, html, other]
Title:
The Morgan-Pitman Test of Equality of Variances and its Application to Machine Learning Model Evaluation and Selection
Argimiro Arratia, Alejandra Caba√±a, Ernesto Mordecki, Gerard Rovira-Parra
Comments:
29 pages, 4 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)
Model selection in non-linear models often prioritizes performance metrics over statistical tests, limiting the ability to account for sampling variability. We propose the use of a statistical test to assess the equality of variances in forecasting errors. The test builds upon the classic Morgan-Pitman approach, incorporating enhancements to ensure robustness against data with heavy-tailed distributions or outliers with high variance, plus a strategy to make residuals from machine learning models statistically independent. Through a series of simulations and real-world data applications, we demonstrate the test's effectiveness and practical utility, offering a reliable tool for model evaluation and selection in diverse contexts.
Cross submissions (showing 24 of 24 entries)
[50]
arXiv:2505.02879
(cross-list from physics.soc-ph)
[pdf, html, other]
Title:
(Un)biased data and spin glasses reveal clustering for Turing phase transitions within human-transformer interactions
Jackson George, Zachariah Yusaf, Stephanie Zoltick, Linh Huynh
Subjects:
Physics and Society (physics.soc-ph); Disordered Systems and Neural Networks (cond-mat.dis-nn); Probability (math.PR); Applications (stat.AP)
This paper studies a Large Language Model's ability to exhibit intelligence equivalent to that of a human by analyzing temperature-induced phase transitions, abrupt changes in the macroscopic behavior of a system, in the Turing test. We utilize three approaches: statistical analysis and bias quantification of a human evaluation survey, information retrieval from real human-written versus AI-generated text data using cosine similarity as a comparison metric, and mathematical spin glass model and simulation. We collect text data in the case study of Flitzing, a tradition of emailing poem-like romantic invitations at Dartmouth College because of its richness in information. Across the three approaches, we obtain consistency in phase transition and clustering results, which also align with literature on the mathematics of transformers and metastability. Our work inspires utilizing spin glass theory for the mathematical foundations of artificial intelligence, especially under environmental stochasticity from human interactions, with justification from real data.
[51]
arXiv:2509.10527
(cross-list from eess.IV)
[pdf, html, other]
Title:
An Interpretable Ensemble Framework for Multi-Omics Dementia Biomarker Discovery Under HDLSS Conditions
Byeonghee Lee, Joonsung Kang
Comments:
11 pages, 1 figure
Subjects:
Image and Video Processing (eess.IV); Computers and Society (cs.CY); Machine Learning (cs.LG); Methodology (stat.ME)
Biomarker discovery in neurodegenerative diseases requires robust, interpretable frameworks capable of integrating high-dimensional multi-omics data under low-sample conditions. We propose a novel ensemble approach combining Graph Attention Networks (GAT), MultiOmics Variational AutoEncoder (MOVE), Elastic-net sparse regression, and Storey's False Discovery Rate (FDR). This framework is benchmarked against state-of-the-art methods including DIABLO, MOCAT, AMOGEL, and MOMLIN. We evaluate performance using both simulated multi-omics data and the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our method demonstrates superior predictive accuracy, feature selection precision, and biological relevance. Biomarker gene maps derived from both datasets are visualized and interpreted, offering insights into latent molecular mechanisms underlying dementia.
[52]
arXiv:2509.10613
(cross-list from cs.LG)
[pdf, html, other]
Title:
pySigLib - Fast Signature-Based Computations on CPU and GPU
Daniil Shmelev, Cristopher Salvi
Subjects:
Machine Learning (cs.LG); Mathematical Software (cs.MS); Machine Learning (stat.ML)
Signature-based methods have recently gained significant traction in machine learning for sequential data. In particular, signature kernels have emerged as powerful discriminators and training losses for generative models on time-series, notably in quantitative finance. However, existing implementations do not scale to the dataset sizes and sequence lengths encountered in practice. We present pySigLib, a high-performance Python library offering optimised implementations of signatures and signature kernels on CPU and GPU, fully compatible with PyTorch's automatic differentiation. Beyond an efficient software stack for large-scale signature-based computation, we introduce a novel differentiation scheme for signature kernels that delivers accurate gradients at a fraction of the runtime of existing libraries.
[53]
arXiv:2509.10626
(cross-list from cs.LG)
[pdf, html, other]
Title:
Optimal Multimarginal Schr√∂dinger Bridge: Minimum Spanning Tree over Measure-valued Vertices
Georgiy A. Bondar, Abhishek Halder
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)
The Multimarginal Schr√∂dinger Bridge (MSB) finds the optimal coupling among a collection of random vectors with known statistics and a known correlation structure. In the MSB formulation, this correlation structure is specified \emph{a priori} as an undirected connected graph with measure-valued vertices. In this work, we formulate and solve the problem of finding the optimal MSB in the sense we seek the optimal coupling over all possible graph structures. We find that computing the optimal MSB amounts to solving the minimum spanning tree problem over measure-valued vertices. We show that the resulting problem can be solved in two steps. The first step constructs a complete graph with edge weight equal to a sum of the optimal value of the corresponding bimarginal SB and the entropies of the endpoints. The second step solves a standard minimum spanning tree problem over that complete weighted graph. Numerical experiments illustrate the proposed solution.
[54]
arXiv:2509.10632
(cross-list from cs.LG)
[pdf, html, other]
Title:
Interpretable neural network system identification method for two families of second-order systems based on characteristic curves
Federico J. Gonzalez, Luis P. Lara
Journal-ref:
Nonlinear Dynamics 2025
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Nonlinear system identification often involves a fundamental trade-off between interpretability and flexibility, often requiring the incorporation of physical constraints. We propose a unified data-driven framework that combines the mathematical structure of the governing differential equations with the flexibility of neural networks (NNs). At the core of our approach is the concept of characteristic curves (CCs), which represent individual nonlinear functions (e.g., friction and restoring components) of the system. Each CC is modeled by a dedicated NN, enabling a modular and interpretable representation of the system equation. To demonstrate the versatility of the CC-based formalism, we introduce three identification strategies: (1) SINDy-CC, which extends the sparse regression approach of SINDy by incorporating the mathematical structure of the governing equations as constraints; (2) Poly-CC, which represents each CC using high-degree polynomials; and (3) NN-CC, which uses NNs without requiring prior assumptions about basis functions. Our results show that all three approaches are well-suited for systems with simple polynomial nonlinearities, such as the van der Pol oscillator. In contrast, NN-CC demonstrates superior performance in modeling systems with complex nonlinearities and discontinuities, such as those observed in stick-slip systems. The key contribution of this work is to demonstrate that the CC-based framework, particularly the NN-CC approach, can capture complex nonlinearities while maintaining interpretability through the explicit representation of the CCs. This balance makes it well-suited for modeling systems with discontinuities and complex nonlinearities that are challenging to assess using traditional polynomial or sparse regression methods, providing a powerful tool for nonlinear system identification.
[55]
arXiv:2509.10693
(cross-list from cs.LG)
[pdf, html, other]
Title:
Learning Concave Bid Shading Strategies in Online Auctions via Measure-valued Proximal Optimization
Iman Nodozi, Djordje Gligorijevic, Abhishek Halder
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)
This work proposes a bid shading strategy for first-price auctions as a measure-valued optimization problem. We consider a standard parametric form for bid shading and formulate the problem as convex optimization over the joint distribution of shading parameters. After each auction, the shading parameter distribution is adapted via a regularized Wasserstein-proximal update with a data-driven energy functional. This energy functional is conditional on the context, i.e., on publisher/user attributes such as domain, ad slot type, device, or location. The proposed algorithm encourages the bid distribution to place more weight on values with higher expected surplus, i.e., where the win probability and the value gap are both large. We show that the resulting measure-valued convex optimization problem admits a closed form solution. A numerical example illustrates the proposed method.
[56]
arXiv:2509.10825
(cross-list from cs.LG)
[pdf, html, other]
Title:
FACTORS: Factorial Approximation for Complementary Two-factor Optimization with Risk-aware Scoring
Dongseok Kim, Wonjun Jeong, Gisung Oh
Comments:
43 pages, 8 figures
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
We propose FACTORS, a framework that combines design of experiments with Shapley decomposition to address performance and stability issues that are sensitive to combinations of training factors. Our approach consistently estimates main effects and two-factor interactions, then integrates them into a risk-adjusted objective function that jointly accounts for uncertainty and cost, enabling reliable selection of configurations under a fixed budget. Effect estimation is implemented through two complementary paths: a plug-in path based on conditional means, and a least-squares path that reconstructs Shapley contributions from samples. These paths are designed to work complementarily even when design density and bias levels differ. By incorporating standardization of estimates, bias correction, and uncertainty quantification, our procedure ensures comparability across heterogeneous factor spaces and designs, while a lightweight search routine yields configurations within practical time even for large factor spaces. On the theoretical side, we provide error decompositions, sample complexity analysis, and upper bounds on optimality gaps. On the interpretive side, we summarize main effects and interactions in map form, highlighting adjustment priorities and safe improvement pathways. Across diverse datasets and design conditions, our approach improves rank preservation and optimal configuration identification, reduces decision-making risks, and offers a tuning foundation that delivers interpretable justification alongside stable performance gains even under budget constraints.
[57]
arXiv:2509.10857
(cross-list from eess.SP)
[pdf, html, other]
Title:
Online simplex-structured matrix factorization
Hugues Kouakou, Jos√© Henrique de Morais Goulart, Raffaele Vitale, Thomas Oberlin, David Rousseau, Cyril Ruckebusch, Nicolas Dobigeon
Subjects:
Signal Processing (eess.SP); Chemical Physics (physics.chem-ph); Methodology (stat.ME)
Simplex-structured matrix factorization (SSMF) is a common task encountered in signal processing and machine learning. Minimum-volume constrained unmixing (MVCU) algorithms are among the most widely used methods to perform this task. While MVCU algorithms generally perform well in an offline setting, their direct application to online scenarios suffers from scalability limitations due to memory and computational demands. To overcome these limitations, this paper proposes an approach which can build upon any off-the-shelf MVCU algorithm to operate sequentially, i.e., to handle one observation at a time. The key idea of the proposed method consists in updating the solution of MVCU only when necessary, guided by an online check of the corresponding optimization problem constraints. It only stores and processes observations identified as informative with respect to the geometrical constraints underlying SSMF. We demonstrate the effectiveness of the approach when analyzing synthetic and real datasets, showing that it achieves estimation accuracy comparable to the offline MVCU method upon which it relies, while significantly reducing the computational cost.
[58]
arXiv:2509.10987
(cross-list from q-bio.PE)
[pdf, html, other]
Title:
A High-Order Cumulant Extension of Quasi-Linkage Equilibrium
Kai S. Shimagaki, Jorge Fernandez-de-Cossio-Diaz, R√©mi Monasson, Simona Cocco, John P. Barton
Subjects:
Populations and Evolution (q-bio.PE); Computation (stat.CO)
A central question in evolutionary biology is how to quantitatively understand the dynamics of genetically diverse populations. Modeling the genotype distribution is challenging, as it ultimately requires tracking all correlations (or cumulants) among alleles at different loci. The quasi-linkage equilibrium (QLE) approximation simplifies this by assuming that correlations between alleles at different loci are weak -- i.e., low linkage disequilibrium -- allowing their dynamics to be modeled perturbatively. However, QLE breaks down under strong selection, significant epistatic interactions, or weak recombination. We extend the multilocus QLE framework to allow cumulants up to order $K$ to evolve dynamically, while higher-order cumulants ($>K$) are assumed to equilibrate rapidly. This extended QLE (exQLE) framework yields a general equation of motion for cumulants up to order $K$, which parallels the standard QLE dynamics (recovered when $K = 1$). In this formulation, cumulant dynamics are driven by the gradient of average fitness, mediated by a geometrically interpretable matrix that stems from competition among genotypes. Our analysis shows that the exQLE with $K=2$ accurately captures cumulant dynamics even when the fitness function includes higher-order (e.g., third-- or fourth--order) epistatic interactions, capabilities that standard QLE lacks. We also applied the exQLE framework to infer fitness parameters from temporal sequence data. Overall, exQLE provides a systematic and interpretable approximation scheme, leveraging analytical cumulant dynamics and reducing complexity by progressively truncating higher-order cumulants.
[59]
arXiv:2509.11007
(cross-list from math.OC)
[pdf, html, other]
Title:
Gradient Methods with Online Scaling Part II. Practical Aspects
Ya-Chi Chu, Wenzhi Gao, Yinyu Ye, Madeleine Udell
Subjects:
Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML)
Part I of this work [Gao25] establishes online scaled gradient methods (OSGM), a framework that utilizes online convex optimization to adapt stepsizes in gradient methods. This paper focuses on the practical aspects of OSGM. We leverage the OSGM framework to design new adaptive first-order methods and provide insights into their empirical behavior. The resulting method, OSGM-Best, matches the performance of quasi-Newton variants while requiring less memory and cheaper iterations. We also extend OSGM to nonconvex optimization and outline directions that connect OSGM to existing branches of optimization theory and practice.
[60]
arXiv:2509.11060
(cross-list from econ.EM)
[pdf, html, other]
Title:
Large-Scale Curve Time Series with Common Stochastic Trends
Degui Li, Yu-Ning Li, Peter C.B. Phillips
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
This paper studies high-dimensional curve time series with common stochastic trends. A dual functional factor model structure is adopted with a high-dimensional factor model for the observed curve time series and a low-dimensional factor model for the latent curves with common trends. A functional PCA technique is applied to estimate the common stochastic trends and functional factor loadings. Under some regularity conditions we derive the mean square convergence and limit distribution theory for the developed estimates, allowing the dimension and sample size to jointly diverge to infinity. We propose an easy-to-implement criterion to consistently select the number of common stochastic trends and further discuss model estimation when the nonstationary factors are cointegrated. Extensive Monte-Carlo simulations and two empirical applications to large-scale temperature curves in Australia and log-price curves of S&P 500 stocks are conducted, showing finite-sample performance and providing practical implementations of the new methodology.
[61]
arXiv:2509.11085
(cross-list from cs.LG)
[pdf, other]
Title:
DemandLens: Enhancing Forecast Accuracy Through Product-Specific Hyperparameter Optimization
Srijesh Pillai, M. I. Jawid Nazir
Comments:
10 pages, 12 figures, 3 tables. Accepted for publication in the proceedings of the 2025 Advances in Science and Engineering Technology International Conferences (ASET)
Subjects:
Machine Learning (cs.LG); Applications (stat.AP)
DemandLens demonstrates an innovative Prophet based forecasting model for the mattress-in-a-box industry, incorporating COVID-19 metrics and SKU-specific hyperparameter optimization. This industry has seen significant growth of E-commerce players in the recent years, wherein the business model majorly relies on outsourcing Mattress manufacturing and related logistics and supply chain operations, focusing on marketing the product and driving conversions through Direct-to-Consumer sales channels. Now, within the United States, there are a limited number of Mattress contract manufacturers available, and hence, it is important that they manage their raw materials, supply chain, and, inventory intelligently, to be able to cater maximum Mattress brands. Our approach addresses the critical need for accurate Sales Forecasting in an industry that is heavily dependent on third-party Contract Manufacturing. This, in turn, helps the contract manufacturers to be prepared, hence, avoiding bottleneck scenarios, and aiding them to source raw materials at optimal rates. The model demonstrates strong predictive capabilities through SKU-specific Hyperparameter optimization, offering the Contract Manufacturers and Mattress brands a reliable tool to streamline supply chain operations.
[62]
arXiv:2509.11236
(cross-list from cs.LG)
[pdf, html, other]
Title:
Online Optimization on Hadamard Manifolds: Curvature Independent Regret Bounds on Horospherically Convex Objectives
Emre Sahinoglu, Shahin Shahrampour
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
We study online Riemannian optimization on Hadamard manifolds under the framework of horospherical convexity (h-convexity). Prior work mostly relies on the geodesic convexity (g-convexity), leading to regret bounds scaling poorly with the manifold curvature. To address this limitation, we analyze Riemannian online gradient descent for h-convex and strongly h-convex functions and establish $O(\sqrt{T})$ and $O(\log(T))$ regret guarantees, respectively. These bounds are curvature-independent and match the results in the Euclidean setting. We validate our approach with experiments on the manifold of symmetric positive definite (SPD) matrices equipped with the affine-invariant metric. In particular, we investigate online Tyler's $M$-estimation and online Fr√©chet mean computation, showing the application of h-convexity in practice.
[63]
arXiv:2509.11265
(cross-list from cs.LG)
[pdf, html, other]
Title:
SelectMix: Enhancing Label Noise Robustness through Targeted Sample Mixing
Qiuhao Liu, Ling Li, Yao Lu, Qi Xuan, Zhaowei Zhu, Jiaheng Wei
Subjects:
Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)
Deep neural networks tend to memorize noisy labels, severely degrading their generalization performance. Although Mixup has demonstrated effectiveness in improving generalization and robustness, existing Mixup-based methods typically perform indiscriminate mixing without principled guidance on sample selection and mixing strategy, inadvertently propagating noisy supervision. To overcome these limitations, we propose SelectMix, a confidence-guided mixing framework explicitly tailored for noisy labels. SelectMix first identifies potentially noisy or ambiguous samples through confidence based mismatch analysis using K-fold cross-validation, then selectively blends identified uncertain samples with confidently predicted peers from their potential classes. Furthermore, SelectMix employs soft labels derived from all classes involved in the mixing process, ensuring the labels accurately represent the composition of the mixed samples, thus aligning supervision signals closely with the actual mixed inputs. Through extensive theoretical analysis and empirical evaluations on multiple synthetic (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and real-world benchmark datasets (CIFAR-N, MNIST and Clothing1M), we demonstrate that SelectMix consistently outperforms strong baseline methods, validating its effectiveness and robustness in learning with noisy labels.
[64]
arXiv:2509.11397
(cross-list from eess.SP)
[pdf, html, other]
Title:
Solving ill-conditioned polynomial equations using score-based priors with application to multi-target detection
Rafi Beinhorn, Shay Kreymer, Amnon Balanov, Michael Cohen, Alon Zabatani, Tamir Bendory
Subjects:
Signal Processing (eess.SP); Machine Learning (stat.ML)
Recovering signals from low-order moments is a fundamental yet notoriously difficult task in inverse problems. This recovery process often reduces to solving ill-conditioned systems of polynomial equations. In this work, we propose a new framework that integrates score-based diffusion priors with moment-based estimators to regularize and solve these nonlinear inverse problems. This introduces a new role for generative models: stabilizing polynomial recovery from noisy statistical features. As a concrete application, we study the multi-target detection (MTD) model in the high-noise regime. We demonstrate two main results: (i) diffusion priors substantially improve recovery from third-order moments, and (ii) they make the super-resolution MTD problem, otherwise ill-posed, feasible. Numerical experiments on MNIST data confirm consistent gains in reconstruction accuracy across SNR levels. Our results suggest a promising new direction for combining generative priors with nonlinear polynomial inverse problems.
[65]
arXiv:2509.11426
(cross-list from cs.LG)
[pdf, html, other]
Title:
Long-time dynamics and universality of nonconvex gradient descent
Qiyang Han
Subjects:
Machine Learning (cs.LG); Information Theory (cs.IT); Optimization and Control (math.OC); Statistics Theory (math.ST); Machine Learning (stat.ML)
This paper develops a general approach to characterize the long-time trajectory behavior of nonconvex gradient descent in generalized single-index models in the large aspect ratio regime. In this regime, we show that for each iteration the gradient descent iterate concentrates around a deterministic vector called the `Gaussian theoretical gradient descent', whose dynamics can be tracked by a state evolution system of two recursive equations for two scalars. Our concentration guarantees hold universally for a broad class of design matrices and remain valid over long time horizons until algorithmic convergence or divergence occurs. Moreover, our approach reveals that gradient descent iterates are in general approximately independent of the data and strongly incoherent with the feature vectors, a phenomenon previously known as the `implicit regularization' effect of gradient descent in specific models under Gaussian data.
As an illustration of the utility of our general theory, we present two applications of different natures in the regression setting. In the first, we prove global convergence of nonconvex gradient descent with general independent initialization for a broad class of structured link functions, and establish universality of randomly initialized gradient descent in phase retrieval for large aspect ratios. In the second, we develop a data-free iterative algorithm for estimating state evolution parameters along the entire gradient descent trajectory, thereby providing a low-cost yet statistically valid tool for practical tasks such as hyperparameter tuning and runtime determination.
As a by-product of our analysis, we show that in the large aspect ratio regime, the Gaussian theoretical gradient descent coincides with a recent line of dynamical mean-field theory for gradient descent over the constant-time horizon.
[66]
arXiv:2509.11486
(cross-list from math.OC)
[pdf, html, other]
Title:
Preconditioned subgradient method for composite optimization: overparameterization and fast convergence
Mateo D√≠az, Liwei Jiang, Abdel Ghani Labassi
Comments:
84 pages, 8 figures
Subjects:
Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML)
Composite optimization problems involve minimizing the composition of a smooth map with a convex function. Such objectives arise in numerous data science and signal processing applications, including phase retrieval, blind deconvolution, and collaborative filtering. The subgradient method achieves local linear convergence when the composite loss is well-conditioned. However, if the smooth map is, in a certain sense, ill-conditioned or overparameterized, the subgradient method exhibits much slower sublinear convergence even when the convex function is well-conditioned. To overcome this limitation, we introduce a Levenberg-Morrison-Marquardt subgradient method that converges linearly under mild regularity conditions at a rate determined solely by the convex function. Further, we demonstrate that these regularity conditions hold for several problems of practical interest, including square-variable formulations, matrix sensing, and tensor factorization. Numerical experiments illustrate the benefits of our method.
[67]
arXiv:2509.11501
(cross-list from econ.GN)
[pdf, html, other]
Title:
The Price of Disaster: Estimating the Impact of Hurricane Harvey on the Texas Construction Labor Market
Kartik Ganesh
Comments:
16 pages, 6 figures, 4 tables
Subjects:
General Economics (econ.GN); Computation (stat.CO)
This paper estimates the effect of Hurricane Harvey on wages and employment in the construction labor industry across impacted counties in Texas. Based on data from the Quarterly Census of Employment and Wages (QCEW) for the period 2016-2019, I adopted a difference-in-differences event study approach by comparing results in 41 FEMA-designated disaster counties with a set of unaffected southern control counties. I find that Hurricane Harvey had a large and long-lasting impact on labor market outcomes in the construction industry. More precisely, average log wages in treated counties rose by around 7.2 percent compared to control counties two quarters after the hurricane and remained high for the next two years. Employment effects were more gradual, showing a statistically significant increase only after six quarters, in line with the lagged nature of large-scale reconstruction activities. These results imply that natural disasters can generate persistent labor demand shocks to local construction markets, with policy implications for disaster recovery planning and workforce mobilization.
[68]
arXiv:2509.11781
(cross-list from cs.MS)
[pdf, html, other]
Title:
A Computational Framework and Implementation of Implicit Priors in Bayesian Inverse Problems
Jasper M. Everink, Chao Zhang, Amal M. A. Alghamdi, R√©mi Laumont, Nicolai A. B. Riis, Jakob S. J√∏rgensen
Subjects:
Mathematical Software (cs.MS); Computation (stat.CO)
Solving Bayesian inverse problems typically involves deriving a posterior distribution using Bayes' rule, followed by sampling from this posterior for analysis. Sampling methods, such as general-purpose Markov chain Monte Carlo (MCMC), are commonly used, but they require prior and likelihood densities to be explicitly provided. In cases where expressing the prior explicitly is challenging, implicit priors offer an alternative, encoding prior information indirectly. These priors have gained increased interest in recent years, with methods like Plug-and-Play (PnP) priors and Regularized Linear Randomize-then-Optimize (RLRTO) providing computationally efficient alternatives to standard MCMC algorithms. However, the abstract concept of implicit priors for Bayesian inverse problems is yet to be systematically explored and little effort has been made to unify different kinds of implicit priors. This paper presents a computational framework for implicit priors and their distinction from explicit priors. We also introduce an implementation of various implicit priors within the CUQIpy Python package for Computational Uncertainty Quantification in Inverse Problems. Using this implementation, we showcase several implicit prior techniques by applying them to a variety of different inverse problems from image processing to parameter estimation in partial differential equations.
[69]
arXiv:2509.11907
(cross-list from eess.SY)
[pdf, html, other]
Title:
High Effort, Low Gain: Fundamental Limits of Active Learning for Linear Dynamical Systems
Nicolas Chatzikiriakos, Kevin Jamieson, Andrea Iannelli
Subjects:
Systems and Control (eess.SY); Machine Learning (cs.LG); Machine Learning (stat.ML)
In this work, we consider the problem of identifying an unknown linear dynamical system given a finite hypothesis class. In particular, we analyze the effect of the excitation input on the sample complexity of identifying the true system with high probability. To this end, we present sample complexity lower bounds that capture the choice of the selected excitation input. The sample complexity lower bound gives rise to a system theoretic condition to determine the potential benefit of experiment design. Informed by the analysis of the sample complexity lower bound, we propose a persistent excitation (PE) condition tailored to the considered setting, which we then use to establish sample complexity upper bounds. Notably, the \acs{PE} condition is weaker than in the case of an infinite hypothesis class and allows analyzing different excitation inputs modularly. Crucially, the lower and upper bounds share the same dependency on key problem parameters. Finally, we leverage these insights to propose an active learning algorithm that sequentially excites the system optimally with respect to the current estimate, and provide sample complexity guarantees for the presented algorithm. Concluding simulations showcase the effectiveness of the proposed algorithm.
[70]
arXiv:2509.11975
(cross-list from physics.ao-ph)
[pdf, html, other]
Title:
Probabilistic modelling of atmosphere-surface coupling with a copula Bayesian network
Laura Mack, Marvin K√§hnert, Norbert Pirk
Subjects:
Atmospheric and Oceanic Physics (physics.ao-ph); Applications (stat.AP)
Land-atmosphere coupling is an important process for correctly modelling near-surface temperature profiles, but it involves various uncertainties due to subgrid-scale processes, such as turbulent fluxes or unresolved surface heterogeneities, suggesting a probabilistic modelling approach. We develop a copula Bayesian network (CBN) to interpolate temperature profiles, acting as alternative to T2m-diagnostics used in numerical weather prediction (NWP) systems. The new CBN results in (1) a reduction of the warm bias inherent to NWP predictions of wintertime stable boundary layers allowing cold temperature extremes to be better represented, and (2) consideration of uncertainty associated with subgrid-scale spatial variability. The use of CBNs combines the advantages of uncertainty propagation inherent to Bayesian networks with the ability to model complex dependence structures between random variables through copulas. By combining insights from copula modelling and information entropy, criteria for the applicability of CBNs in the further development of parameterizations in NWP models are derived.
[71]
arXiv:2509.12031
(cross-list from math.PR)
[pdf, html, other]
Title:
Contractive kinetic Langevin samplers beyond global Lipschitz continuity
Iosif Lytras, Panagiotis Mertikopoulos
Comments:
30 pages
Subjects:
Probability (math.PR); Numerical Analysis (math.NA); Machine Learning (stat.ML)
In this paper, we examine the problem of sampling from log-concave distributions with (possibly) superlinear gradient growth under kinetic (underdamped) Langevin algorithms. Using a carefully tailored taming scheme, we propose two novel discretizations of the kinetic Langevin SDE, and we show that they are both contractive and satisfy a log-Sobolev inequality. Building on this, we establish a series of non-asymptotic bounds in $2$-Wasserstein distance between the law reached by each algorithm and the underlying target measure.
[72]
arXiv:2509.12119
(cross-list from econ.EM)
[pdf, html, other]
Title:
Fairness-Aware and Interpretable Policy Learning
Nora Bearth, Michael Lechner, Jana Mareckova, Fabian Muny
Subjects:
Econometrics (econ.EM); Applications (stat.AP)
Fairness and interpretability play an important role in the adoption of decision-making algorithms across many application domains. These requirements are intended to avoid undesirable group differences and to alleviate concerns related to transparency. This paper proposes a framework that integrates fairness and interpretability into algorithmic decision making by combining data transformation with policy trees, a class of interpretable policy functions. The approach is based on pre-processing the data to remove dependencies between sensitive attributes and decision-relevant features, followed by a tree-based optimization to obtain the policy. Since data pre-processing compromises interpretability, an additional transformation maps the parameters of the resulting tree back to the original feature space. This procedure enhances fairness by yielding policy allocations that are pairwise independent of sensitive attributes, without sacrificing interpretability. Using administrative data from Switzerland to analyze the allocation of unemployed individuals to active labor market programs (ALMP), the framework is shown to perform well in a realistic policy setting. Effects of integrating fairness and interpretability constraints are measured through the change in expected employment outcomes. The results indicate that, for this particular application, fairness can be substantially improved at relatively low cost.
[73]
arXiv:2509.12154
(cross-list from cs.LG)
[pdf, html, other]
Title:
Learning Neural Networks by Neuron Pursuit
Akshay Kumar, Jarvis Haupt
Subjects:
Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)
The first part of this paper studies the evolution of gradient flow for homogeneous neural networks near a class of saddle points exhibiting a sparsity structure. The choice of these saddle points is motivated from previous works on homogeneous networks, which identified the first saddle point encountered by gradient flow after escaping the origin. It is shown here that, when initialized sufficiently close to such saddle points, gradient flow remains near the saddle point for a sufficiently long time, during which the set of weights with small norm remain small but converge in direction. Furthermore, important empirical observations are made on the behavior of gradient descent after escaping these saddle points. The second part of the paper, motivated by these results, introduces a greedy algorithm to train deep neural networks called Neuron Pursuit (NP). It is an iterative procedure which alternates between expanding the network by adding neuron(s) with carefully chosen weights, and minimizing the training loss using this augmented network. The efficacy of the proposed algorithm is validated using numerical experiments.
Replacement submissions (showing 55 of 55 entries)
[74]
arXiv:2101.09875
(replaced)
[pdf, html, other]
Title:
Eigen-convergence of Gaussian kernelized graph Laplacian by manifold heat interpolation
Xiuyuan Cheng, Nan Wu
Comments:
This updated arXiv version is to correct a typo in the condition of Theorem 6.7 in the published version. Specifically, the assumption of density p is as in Assumption 1(A2), instead of assuming p uniform. Section 6 is to handle non-uniform density p, and the proved rates are same as in the density uniform case
Journal-ref:
Applied and Computational Harmonic Analysis, 61, 132-190 (2022)
Subjects:
Statistics Theory (math.ST); Machine Learning (cs.LG); Machine Learning (stat.ML)
This work studies the spectral convergence of graph Laplacian to the Laplace-Beltrami operator when the graph affinity matrix is constructed from $N$ random samples on a $d$-dimensional manifold embedded in a possibly high dimensional space. By analyzing Dirichlet form convergence and constructing candidate approximate eigenfunctions via convolution with manifold heat kernel, we prove that, with Gaussian kernel, one can set the kernel bandwidth parameter $\epsilon \sim (\log N/ N)^{1/(d/2+2)}$ such that the eigenvalue convergence rate is $N^{-1/(d/2+2)}$ and the eigenvector convergence in 2-norm has rate $N^{-1/(d+4)}$; When $\epsilon \sim (\log N/N)^{1/(d/2+3)}$, both eigenvalue and eigenvector rates are $N^{-1/(d/2+3)}$. These rates are up to a $\log N$ factor and proved for finitely many low-lying eigenvalues. The result holds for un-normalized and random-walk graph Laplacians when data are uniformly sampled on the manifold, as well as the density-corrected graph Laplacian (where the affinity matrix is normalized by the degree matrix from both sides) with non-uniformly sampled data. As an intermediate result, we prove new point-wise and Dirichlet form convergence rates for the density-corrected graph Laplacian. Numerical results are provided to verify the theory.
[75]
arXiv:2104.08300
(replaced)
[pdf, other]
Title:
Semiparametric sensitivity analysis: unmeasured confounding in observational studies
Razieh Nabi, Matteo Bonvini, Edward H. Kennedy, Ming-Yueh Huang, Marcela Smid, Daniel O. Scharfstein
Journal-ref:
Biometrics, Volume 80, Issue 4, December 2024
Subjects:
Methodology (stat.ME)
Establishing cause-effect relationships from observational data often relies on untestable assumptions. It is crucial to know whether, and to what extent, the conclusions drawn from non-experimental studies are robust to potential unmeasured confounding. In this paper, we focus on the average causal effect (ACE) as our target of inference. We generalize the sensitivity analysis approach developed by Robins et al. (2000), Franks et al. (2020), and Zhou and Yao (2023). We use semiparametric theory to derive the non-parametric efficient influence function of the ACE, for fixed sensitivity parameters. We use this influence function to construct a one-step, split sample, truncated estimator of the ACE. Our estimator depends on semiparametric models for the distribution of the observed data; importantly, these models do not impose any restrictions on the values of sensitivity analysis parameters. We establish sufficient conditions ensuring that our estimator has root-n asymptotics. We use our methodology to evaluate the causal effect of smoking during pregnancy on birth weight. We also evaluate the performance of estimation procedure in a simulation study.
[76]
arXiv:2203.03221
(replaced)
[pdf, html, other]
Title:
Generalized Dirichlet Energy and Graph Laplacians for Clustering Directed and Undirected Graphs
Harry Sevi, Gwendal Debaussart-Joniec, Malik Hacini, Matthieu Jonckheere, Argyris Kalogeratos
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Clustering in directed graphs remains a fundamental challenge due to the asymmetry in edge connectivity, which limits the applicability of classical spectral methods originally designed for undirected graphs. A common workaround is to symmetrize the adjacency matrix, but this often leads to losing critical directional information. In this work, we introduce the generalized Dirichlet energy (GDE), a novel energy functional that extends the classical Dirichlet energy to handle arbitrary positive vertex measures and Markov transition matrices. GDE provides a unified framework applicable to both directed and undirected graphs, and is closely tied to the diffusion dynamics of random walks. Building on this framework, we propose the generalized spectral clustering (GSC) method that enables the principled clustering of weakly connected digraphs without resorting to the introduction of teleportation to the random walk transition matrix. A key component of our approach is the utilization of a parametrized vertex measure encoding graph directionality and density. Experiments on real-world point-cloud datasets demonstrate that GSC consistently outperforms existing spectral clustering approaches in terms of clustering accuracy and robustness, offering a powerful new tool for graph-based data analysis.
[77]
arXiv:2209.01328
(replaced)
[pdf, html, other]
Title:
Optimal empirical Bayes estimation for the Poisson model via minimum-distance methods
Soham Jana, Yury Polyanskiy, Yihong Wu
Comments:
47 pages, 7 figures, 3 tables. Accepted for publication at Information and Inference - A Journal of the IMA
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME)
The Robbins estimator is the most iconic and widely used procedure in the empirical Bayes literature for the Poisson model. On one hand, this method has been recently shown to be minimax optimal in terms of the regret (excess risk over the Bayesian oracle that knows the true prior) for various nonparametric classes of priors. On the other hand, it has been long recognized in practice that the Robbins estimator lacks the desired smoothness and monotonicity of Bayes estimators and can be easily derailed by those data points that were rarely observed before. Based on the minimum-distance distance method, we propose a suite of empirical Bayes estimators, including the classical nonparametric maximum likelihood, that outperform the Robbins method in a variety of synthetic and real data sets and retain its optimality in terms of minimax regret.
[78]
arXiv:2211.14908
(replaced)
[pdf, other]
Title:
A Permutation-free Kernel Two-Sample Test
Shubhanshu Shekhar, Ilmun Kim, Aaditya Ramdas
Comments:
Published at the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), with an oral presentation. The current version on arXiv fixes a bug in the proof of Theorem 9
Subjects:
Methodology (stat.ME); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
The kernel Maximum Mean Discrepancy~(MMD) is a popular multivariate distance metric between distributions that has found utility in two-sample testing. The usual kernel-MMD test statistic is a degenerate U-statistic under the null, and thus it has an intractable limiting distribution. Hence, to design a level-$\alpha$ test, one usually selects the rejection threshold as the $(1-\alpha)$-quantile of the permutation distribution. The resulting nonparametric test has finite-sample validity but suffers from large computational cost, since every permutation takes quadratic time. We propose the cross-MMD, a new quadratic-time MMD test statistic based on sample-splitting and studentization. We prove that under mild assumptions, the cross-MMD has a limiting standard Gaussian distribution under the null. Importantly, we also show that the resulting test is consistent against any fixed alternative, and when using the Gaussian kernel, it has minimax rate-optimal power against local alternatives. For large sample sizes, our new cross-MMD provides a significant speedup over the MMD, for only a slight loss in power.
[79]
arXiv:2302.08724
(replaced)
[pdf, html, other]
Title:
Piecewise Deterministic Markov Processes for Bayesian Neural Networks
Ethan Goan, Dimitri Perrin, Kerrie Mengersen, Clinton Fookes
Comments:
Includes correction to software and corrigendum note (fix supplementary references)
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Other Statistics (stat.OT)
Inference on modern Bayesian Neural Networks (BNNs) often relies on a variational inference treatment, imposing violated assumptions of independence and the form of the posterior. Traditional MCMC approaches avoid these assumptions at the cost of increased computation due to its incompatibility to subsampling of the likelihood. New Piecewise Deterministic Markov Process (PDMP) samplers permit subsampling, though introduce a model specific inhomogenous Poisson Process (IPPs) which is difficult to sample from. This work introduces a new generic and adaptive thinning scheme for sampling from these IPPs, and demonstrates how this approach can accelerate the application of PDMPs for inference in BNNs. Experimentation illustrates how inference with these methods is computationally feasible, can improve predictive accuracy, MCMC mixing performance, and provide informative uncertainty measurements when compared against other approximate inference schemes.
[80]
arXiv:2306.15607
(replaced)
[pdf, html, other]
Title:
A method for empirically assessing small area estimators via bootstrap-weighted k-Nearest-Neighbor artificial populations, with applications to forest inventory
Grayson W. White, Jerzy A. Wieczorek, Zachariah W. Cody, Emily X. Tan, Jacqueline O. Chistolini, Kelly S. McConville, Tracey S. Frescino, Gretchen G. Moisen
Subjects:
Methodology (stat.ME)
National Forest Inventories (NFIs) monitor forest attributes across a variety of spatial and temporal scales in a given country. Increased interest in reporting and management at smaller scales has driven NFIs to investigate and adopt small area estimation (SAE) due to the promise of increased precision at these scales. However, comparing and evaluating SAE models for a given application is inherently difficult. Typically, many areas lack enough data to check unit-level modeling assumptions or to assess unit-level predictions empirically; and no ground truth is available for checking area-level estimates. Design-based simulation from artificial populations can help with each of these issues, but only if the artificial populations realistically represent the application at hand and are not built using assumptions that inherently favor one SAE model over another. In this paper, we borrow ideas from random hot deck, approximate Bayesian bootstrap (ABB), and k Nearest Neighbor (kNN) imputation methods to propose a kNN-based approximation to ABB (KBAABB), for generating an artificial population when rich unit-level auxiliary data is available. We introduce diagnostic checks on the process of building the artificial population, and we demonstrate how to use such an artificial population for design-based simulation studies to compare and evaluate SAE models, using real data from the United States Department of Agriculture, Forest Service, Forest Inventory and Analysis Program, the NFI of the United States.
[81]
arXiv:2308.14625
(replaced)
[pdf, html, other]
Title:
Models for temporal clustering of extreme events with applications to mid-latitude winter cyclones
Christina Mathieu, Katharina Hees, Roland Fried
Comments:
The supplementary material is not included
Subjects:
Methodology (stat.ME)
The occurrence of extreme events like heavy precipitation or storms at a certain location often shows a clustering behaviour and is thus not described well by a Poisson process. We construct a general model for the inter-exceedance times in between such events which combines different candidate models for such behaviour. This allows us to distinguish data generating mechanisms leading to clusters of dependent events with exponential inter-exceedance times in between clusters from independent events with heavy-tailed inter-exceedance times, and even allows us to combine these two mechanisms for better descriptions of such occurrences. We propose a modification of the Cram√©r-von Mises distance for model fitting. An application to mid-latitude winter cyclones illustrates the usefulness of our work.
[82]
arXiv:2405.06763
(replaced)
[pdf, html, other]
Title:
Post-selection inference for causal effects after causal discovery
Ting-Hsuan Chang, Zijian Guo, Daniel Malinsky
Subjects:
Methodology (stat.ME)
Algorithms for constraint-based causal discovery select graphical causal models among a space of possible candidates (e.g., all directed acyclic graphs) by executing a sequence of conditional independence tests. These may be used to inform the estimation of causal effects (e.g., average treatment effects) when there is uncertainty about which covariates ought to be adjusted for, or which variables act as confounders versus mediators. However, naively using the data twice, for model selection and estimation, would lead to invalid confidence intervals. Moreover, if the selected graph is incorrect, the inferential claims may apply to a selected functional that is distinct from the actual causal effect. We propose an approach to post-selection inference that is based on a resampling and screening procedure, which essentially performs causal discovery multiple times with randomly varying intermediate test statistics. Then, an estimate of the target causal effect and corresponding confidence sets are constructed from a union of individual graph-based estimates and intervals. We show that this construction has asymptotically correct coverage for the true causal effect parameter. Importantly, the guarantee holds for a fixed population-level effect, not a data-dependent or selection-dependent quantity. Most of our exposition focuses on the PC-algorithm for learning directed acyclic graphs and the multivariate Gaussian case for simplicity, but the approach is general and modular, so it may be used with other conditional independence based discovery algorithms and distributional families.
[83]
arXiv:2405.13342
(replaced)
[pdf, html, other]
Title:
Scalable Bayesian inference for heat kernel Gaussian processes on manifolds
Junhui He, Guoxuan Ma, Jian Kang, Ying Yang
Journal-ref:
Junhui He, Guoxuan Ma, Jian Kang, Ying Yang, Scalable Bayesian inference for heat kernel Gaussian processes on manifolds, Journal of the Royal Statistical Society Series B: Statistical Methodology, 2025
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
We establish a scalable manifold learning method and theory, motivated by the problem of estimating fMRI activation manifolds in the Human Connectome Project (HCP). Our primary contribution is the development of an efficient estimation technique for heat kernel Gaussian processes in the exponential family model. This approach handles large sample sizes $n$, preserves the intrinsic geometry of data, and significantly reduces computational complexity from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$ via a novel reduced-rank approximation of the graph Laplacian's transition matrix and a Truncated Singular Value Decomposition for the eigenpair computation. The numerical experiments demonstrate the scalability and improved accuracy of our method for manifold learning tasks involving complex large-scale data.
[84]
arXiv:2407.06835
(replaced)
[pdf, other]
Title:
A Flexible Model for Record Linkage
Kayan√© Robach, St√©phanie L van der Pas, Mark A van de Wiel, Michel H Hof
Comments:
Published in JRSSSC in February 2025
Journal-ref:
A flexible model for record linkage, JRSSSC, Volume 74, Issue 4, November 2025, Pages 1100-1127
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Combining data from various sources empowers researchers to explore innovative questions, for example those raised by conducting healthcare monitoring studies. However, the lack of a unique identifier often poses challenges. Record linkage procedures determine whether pairs of observations collected on different occasions belong to the same individual using partially identifying variables (e.g. birth year, postal code). Existing methodologies typically involve a compromise between computational efficiency and accuracy. Traditional approaches simplify this task by condensing information, yet they neglect dependencies among linkage decisions and disregard the one-to-one relationship required to establish coherent links. Modern approaches offer a comprehensive representation of the data generation process, at the expense of computational overhead and reduced flexibility. We propose a flexible method, that adapts to varying data complexities, addressing registration errors and accommodating changes of the identifying information over time. Our approach balances accuracy and scalability, estimating the linkage using a Stochastic Expectation Maximisation algorithm on a latent variable model. We illustrate the ability of our methodology to connect observations using large real data applications and demonstrate the robustness of our model to the linking variables quality in a simulation study. The proposed algorithm FlexRL is implemented and available in an open source R package.
[85]
arXiv:2407.12308
(replaced)
[pdf, html, other]
Title:
A Point on Discrete versus Continuous State-Space Markov Chains
Mathias N. Muia, Martial Longla
Comments:
21 pages, 3 figures
Subjects:
Statistics Theory (math.ST)
This paper examines the impact of discrete marginal distributions on copula-based Markov chains. We present results on mixing and parameter estimation for a copula-based Markov chain model with Bernoulli($p$) marginal distribution and highlight the differences between continuous and discrete state-space Markov chains. We derive estimators for model parameters using the maximum likelihood approach and discuss other estimators of $p$ that are asymptotically equivalent to its maximum likelihood estimator. The asymptotic distributions of the parameter estimators are provided. A simulation study showcases the performance of the different estimators of $p$. Additionally, statistical tests for model parameters are included.
[86]
arXiv:2407.20520
(replaced)
[pdf, html, other]
Title:
Raking mortality rates across cause, population group and geography with uncertainty quantification
Ariane Ducellier (1), Alexander Hsu (1), Parkes Kendrick (1), Bill Gustafson (1), Laura Dwyer-Lindgren (1), Christopher Murray (1), Peng Zheng (1), Aleksandr Aravkin (1) ((1) Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA)
Subjects:
Methodology (stat.ME); Numerical Analysis (math.NA); Applications (stat.AP); Computation (stat.CO)
The Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) is the single largest and most detailed scientific effort ever conducted to quantify levels and trends in health. This global health model to estimate mortality rates and other health metrics is run at different scales, leading to large data sets of results for a global region and its different sub-regions, or for a cause of death and different sub-causes for example. These models do not necessarily lead to consistent data tables where, for instance, the sum of the number of deaths for each of the sub-regions is equal to the number of deaths for the global region. Raking is widely used in survey inference and global health models to adjust the observations in contingency tables to given marginals, in the latter case reconciling estimates between models with different granularities. The results of global health models usually associate to the point estimates an uncertainty, such as standard deviations or confidence intervals. In this paper, we propose an uncertainty propagation approach that obtains, at the cost of a single solve, nearly the same uncertainty estimates as computationally intensive Monte Carlo techniques that pass thousands of observed and marginal samples through the entire raking process. We introduce a convex optimization approach that provides a unified framework to raking extensions such as uncertainty propagation, raking with differential weights, raking with different loss functions in order to ensure that bounds on estimates are respected, verifying the feasibility of the constraints, raking to margins either as hard constraints or as aggregate observations, and handling missing data.
[87]
arXiv:2410.01133
(replaced)
[pdf, html, other]
Title:
A subcopula characterization of dependence for the Multivariate Bernoulli Distribution
Arturo Erdely
Comments:
33 pages, 2 figures, 12 tables
Subjects:
Methodology (stat.ME)
By applying Sklar's theorem to the Multivariate Bernoulli Distribution (MBD), this paper proposes a framework to decouple marginal distributions from the dependence structure, clarifying interactions among binary variables. Explicit formulas are derived under the MBD using subcopulas to introduce dependence measures for interactions of all orders, not just pairwise. A Bayesian inference approach is also applied to estimate the parameters of the MBD, offering practical tools for parameter estimation and dependence analysis in real-world applications. The results obtained contribute to the application of subcopulas of multivariate binary data, with real data examples.
[88]
arXiv:2410.14090
(replaced)
[pdf, html, other]
Title:
Adapting Projection-Based Reduced-Order Models using Projected Gaussian Process
Xiao Liu, Jingyi Feng, Xinchao Liu
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Dynamical Systems (math.DS); Applications (stat.AP)
Projection-based model reduction is among the most widely adopted methods for constructing parametric Reduced-Order Models (ROM). Utilizing the snapshot data from solving full-order governing equations, the Proper Orthogonal Decomposition (POD) computes the optimal basis modes that represent the data, and a ROM can be constructed in the low-dimensional vector subspace spanned by the POD basis. For parametric governing equations, a potential challenge arises when there is a need to update the POD basis to adapt ROM that accurately capture the variation of a system's behavior over its parameter space (in design, control, uncertainty quantification, digital twins applications, etc.). In this paper, we propose a Projected Gaussian Process (pGP) and formulate the problem of adapting the POD basis as a supervised statistical learning problem, for which the goal is to learn a mapping from the parameter space to the Grassmann manifold that contains the optimal subspaces. A mapping is firstly established between the Euclidean space and the horizontal space of an orthogonal matrix that spans a reference subspace in the Grassmann manifold. A second mapping from the horizontal space to the Grassmann manifold is established through the Exponential/Logarithm maps between the manifold and its tangent space. Finally, given a new parameter, the conditional distribution of a vector can be found in the Euclidean space using the Gaussian Process (GP) regression, and such a distribution is then projected to the Grassmann manifold that enables us to predict the optimal subspace for the new parameter. As a statistical learning approach, the proposed pGP allows us to optimally estimate (or tune) the model parameters from data and quantify the statistical uncertainty associated with the prediction. The advantages of the proposed pGP are demonstrated by numerical experiments.
[89]
arXiv:2410.22253
(replaced)
[pdf, html, other]
Title:
Pedestrian crash causation analysis near bus stops: Insights from random parameters Negative Binomial-Lindley model
Mohammad Anis, Srinivas R. Geedipally, Dominique Lord
Comments:
26 pages, 8 figures
Subjects:
Applications (stat.AP)
Pedestrian safety remains a pressing concern near bus stops along urban transit, where frequent pedestrian-vehicle interactions occur. While prior research has primarily focused on intersections and midblock locations, bus stops have often been treated as secondary contributors rather than as distinct sites requiring targeted safety assessments. This has left a critical gap in understanding how traffic exposure, roadway characteristics, and bus stop design features specifically influence pedestrian crash risks around bus stop locations. To address these gaps, this study develops a comprehensive framework focused on pedestrian safety in the vicinity of bus stops. The proposed approach employs a Random Parameters Negative Binomial-Lindley (RPNB-L) model to account for unobserved heterogeneity and site-specific variability.
[90]
arXiv:2411.02763
(replaced)
[pdf, html, other]
Title:
Identifying nonlinear relations among random variables: A network analytic approach
Lindley R. Slipetz, Jiaxing Qiu, Siqi Sun, Teague R. Henry
Subjects:
Methodology (stat.ME)
Nonlinear relations, such as the curvilinear relationship between childhood trauma and resilience in patients with schizophrenia and the moderation relationship between mentalizing, and internalizing and externalizing symptoms and quality of life in youths, are more prevalent than our current methods have been able to detect. Although the use of network models has risen, network construction for the standard Gaussian graphical model depends solely upon linearity. While nonlinear models are an active field of study in psychological methodology, many models require the analyst to specify the functional form of the relation. When performing more exploratory modeling, such as with cross-sectional network psychometrics, specifying the functional form a nonlinear relation might take becomes infeasible given the number of possible relations modeled. Here, we apply a novel nonparametric approach to identifying nonlinear relations using partial distance correlations. We found that partial distance correlations excel overall at identifying nonlinear relations regardless of functional form when compared with Pearson's and Spearman's partial correlations and conditional mutual information. Through simulation studies and an empirical example, we show that partial distance correlations as a novel method can be used to identify possible nonlinear relations in psychometric networks, enabling researchers to then explore the shape of these relations with more confirmatory models.
[91]
arXiv:2412.04773
(replaced)
[pdf, html, other]
Title:
Robust Gradient Descent Estimation for Tensor Models under Heavy-Tailed Distributions
Xiaoyu Zhang, Di Wang, Guodong Li, Defeng Sun
Comments:
82 pages, 11 figures
Subjects:
Methodology (stat.ME)
Low-rank tensor models are widely used in statistics. However, most existing methods rely heavily on the assumption that data follows a sub-Gaussian distribution. To address the challenges associated with heavy-tailed distributions encountered in real-world applications, we propose a novel robust estimation procedure based on truncated gradient descent for general low-rank tensor models. We establish the computational convergence of the proposed method and derive optimal statistical rates under heavy-tailed distributional settings of both covariates and noise for various low-rank models. Notably, the statistical error rates are governed by a local moment condition, which captures the distributional properties of tensor variables projected onto certain low-dimensional local regions. Furthermore, we present numerical results to demonstrate the effectiveness of our method.
[92]
arXiv:2412.05998
(replaced)
[pdf, html, other]
Title:
B-MASTER: Scalable Bayesian Multivariate Regression for Master Predictor Discovery in Colorectal Cancer Microbiome-Metabolite Profiles
Priyam Das, Tanujit Dey, Christine Peterson, Sounak Chakraborty
Subjects:
Methodology (stat.ME)
The gut microbiome significantly influences responses to cancer therapies, including immunotherapies, primarily through its impact on the metabolome. Despite some studies on effects of specific microbial genera on individual metabolites, there is little prior work identifying key microbiome components at the genus level that shape the overall metabolome profile. To address this gap, we introduce B-MASTER (Bayesian Multivariate regression Analysis for Selecting Targeted Essential Regressors), a fully Bayesian framework with an L1 penalty to promote sparsity and an L2 penalty to shrink coefficients for non-major covariates, thereby isolating essential regressors. The method is paired with a scalable Gibbs sampling algorithm, whose computation grows linearly with the number of parameters and remains largely unaffected by sample size for models of fixed dimensions. Notably, B-MASTER enables full posterior inference for models with up to four million parameters within a practical time-frame. Its theoretical guarantees include posterior contraction, selection consistency, and robustness under mild misspecification. Using this approach, we identify key microbial genera shaping the metabolite profile, analyze their effects on the most abundant metabolites, and investigate metabolites differentially abundant in colorectal cancer (CRC) patients. These results provide foundational insights into microbiome-metabolite relationships relevant to cancer, a connection largely unexplored in existing literature.
[93]
arXiv:2412.15808
(replaced)
[pdf, html, other]
Title:
Deep learning joint extremes of metocean variables using the SPAR model
Ed Mackay, Callum Murphy-Barltrop, Jordan Richards, Philip Jonathan
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
This paper presents a novel deep learning framework for estimating multivariate joint extremes of metocean variables, based on the Semi-Parametric Angular-Radial (SPAR) model. When considered in polar coordinates, the problem of modelling multivariate extremes is transformed to one of modelling an angular density, and the tail of a univariate radial variable conditioned on angle. In the SPAR approach, the tail of the radial variable is modelled using a generalised Pareto (GP) distribution, providing a natural extension of univariate extreme value theory to the multivariate setting. In this work, we show how the method can be applied in higher dimensions, using a case study for five metocean variables: wind speed, wind direction, wave height, wave period, and wave direction. The angular variable is modelled using a kernel density method, while the parameters of the GP model are approximated using fully-connected deep neural networks. Our approach provides great flexibility in the dependence structures that can be represented, together with computationally efficient routines for training the model. Furthermore, the application of the method requires fewer assumptions about the underlying distribution(s) compared to existing approaches, and an asymptotically justified means for extrapolating outside the range of observations. Using various diagnostic plots, we show that the fitted models provide a good description of the joint extremes of the metocean variables considered.
[94]
arXiv:2501.19047
(replaced)
[pdf, html, other]
Title:
Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)
Maja Pavlovic
Comments:
this https URL
Journal-ref:
https://iclr-blogposts.github.io/2025/blog/calibration/
Subjects:
Methodology (stat.ME); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Machine Learning (stat.ML)
To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.
[95]
arXiv:2503.20736
(replaced)
[pdf, html, other]
Title:
Parameter estimation for fractional autoregressive process with periodic structure
Chunhao Cai, Yiwu Shang
Subjects:
Statistics Theory (math.ST)
This paper introduces a new periodic fractional autoregressive process (PFAR) driven by fractional Gaussian noise (fGn) to model time series of precipitation evapotranspiration. Compared with the similar model in [\emph{Water Resources Research}, \textbf{20} (1984) 1898--1908], the new model incorporates a periodic structure via specialized varying coefficients and captures long memory and rough voltality through fGn for $0<H<1$, rather than via fractional differencing. In this work, Generalized Least Squares Estimation (GLSE) and the GPH method are employed to construct an initial estimator for the joint estimation of model parameters. A One-Step procedure is then used to obtain a more asymptotically efficient estimator. The paper proves that both estimators are consistent and asymptotically normal, and their performance is demonstrated via Monte Carlo simulations with finite-size samples. Simulation studies suggest that, while both estimation methods can accurately estimate the model parameters, the One-Step estimator outperforms the initial estimator.
[96]
arXiv:2504.07321
(replaced)
[pdf, html, other]
Title:
A Unified Framework for Large-Scale Inference of Classification: Error Rate Control and Optimality
Yinrui Sun, Yin Xia
Subjects:
Methodology (stat.ME)
Classification is a fundamental task in supervised learning, while achieving valid misclassification rate control remains challenging due to possibly the limited predictive capability of the classifiers or the intrinsic complexity of the classification task. In this article, we address large-scale multi-class classification problems with general error rate guarantees to enhance algorithmic trustworthiness. To this end, we first introduce a notion of group-wise classification, which unifies the common class-wise and overall classifications as special cases. We then develop a unified inference framework for the general group-wise classification that consists of three steps: Pre-classification, Selective $p$-value construction, and large-scale Post-classification decisions (PSP). Theoretically, PSP is distribution-free and provides valid finite-sample guarantees for controlling general group-wise false decision rates at target levels. To show the power of PSP, we demonstrate that the step of post-classification decisions never degrades the power of pre-classification, provided that pre-classification has been sufficiently powerful to meet the target error levels. We further establish general power optimality theories for PSP from both non-asymptotic and asymptotic perspectives. Numerical results in both simulations and real data analysis validate the performance of the proposed PSP approach. In addition, we introduce an ePSP algorithm that integrates the idea of PSP with selective $e$-values. Finally, extensions of PSP are shown to demonstrate its feasibility and power in broader applications.
[97]
arXiv:2505.04613
(replaced)
[pdf, html, other]
Title:
Kernel Embeddings and the Separation of Measure Phenomenon
Leonardo V. Santoro, Kartik G. Waghmare, Victor M. Panaretos
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)
We prove that kernel covariance embeddings lead to information-theoretically perfect separation of distinct probability distributions. In statistical terms, we establish that testing for the equality of two probability measures on a compact and separable metric space is equivalent to testing for the singularity between two centered Gaussian measures on a reproducing kernel Hilbert Space. The corresponding Gaussians are defined via the notion of kernel covariance embedding of a probability measure, and the Hilbert space is that generated by the embedding kernel. Distinguishing singular Gaussians is fundamentally simpler from an information-theoretic perspective than non-parametric two-sample testing, particularly in complex or high-dimensional domains. This is because singular Gaussians are supported on essentially separate and affine subspaces. Our proof leverages the classical Feldman-Hajek dichotomy, and shows that even a small perturbation of a distribution will be maximally magnified through its Gaussian embedding. This ``separation of measure phenomenon'' appears to be a blessing of infinite dimensionality, by means of embedding, with the potential to inform the design of efficient inference tools in considerable generality. The elicitation of this phenomenon also appears to crystallize, in a precise and simple mathematical statement, the outstanding empirical effectiveness of the so-called ``kernel trick".
[98]
arXiv:2505.09105
(replaced)
[pdf, html, other]
Title:
Model-free High Dimensional Mediator Selection with False Discovery Rate Control
Runqiu Wang, Ran Dai, Jieqiong Wang, Kah Meng Soh, Ziyang Xu, Mohamed Azzam, Hongying Dai, Cheng Zheng
Subjects:
Methodology (stat.ME)
There is a challenge in selecting high-dimensional mediators when the mediators have complex correlation structures and interactions. In this work, we frame the high-dimensional mediator selection problem into a series of hypothesis tests with composite nulls, and develop a method to control the false discovery rate (FDR) which has mild assumptions on the mediation model. We show the theoretical guarantee that the proposed method and algorithm achieve FDR control. We present extensive simulation results to demonstrate the power and finite sample performance compared with existing methods. Lastly, we demonstrate the method for analyzing the Alzheimer's Disease Neuroimaging Initiative (ADNI) data, in which the proposed method selects the volume of the hippocampus and amygdala, as well as some other important MRI-derived measures as mediators for the relationship between gender and dementia progression.
[99]
arXiv:2506.19010
(replaced)
[pdf, other]
Title:
Simulation-Based Sensitivity Analysis in Optimal Treatment Regimes and Causal Decomposition with Individualized Interventions
Soojin Park, Suyeon Kang, Chioun Lee
Comments:
42 pages
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
Causal decomposition analysis aims to assess the effect of modifying risk factors on reducing social disparities in outcomes. Recently, this analysis has incorporated individual characteristics when modifying risk factors by utilizing optimal treatment regimes (OTRs). Since the newly defined individualized effects rely on the no omitted confounding assumption, developing sensitivity analyses to account for potential omitted confounding is essential. Moreover, OTRs and individualized effects are primarily based on binary risk factors, and no formal approach currently exists to benchmark the strength of omitted confounding using observed covariates for binary risk factors. To address this gap, we extend a simulation-based sensitivity analysis that simulates unmeasured confounders, addressing two sources of bias emerging from deriving OTRs and estimating individualized effects. Additionally, we propose a formal bounding strategy that benchmarks the strength of omitted confounding for binary risk factors. Using the High School Longitudinal Study 2009 (HSLS:09), we demonstrate this sensitivity analysis and benchmarking method.
[100]
arXiv:2507.13695
(replaced)
[pdf, other]
Title:
Intellectual Up-streams of Percentage Scale ($ps$) and Percentage Coefficient ($b_p$) -- Effect Size Analysis (Theory Paper 2)
Xinshu Zhao, Qinru Ruby Ju, Piper Liping Liu, Dianshi Moses Li, Luxi Zhang, Jizhou Francis Ye, Song Harris Ao, Ming Milano Li
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Percentage thinking, i.e., assessing quantities as parts per hundred, spread from Roman tax ledgers to modern algorithms. Building on Simon Stevin's La Thiende (1585) and the 19th-century metrication that institutionalized base-10 measurement (Cajori, 1925), this article traces how base-10 normalization, especially the 0-1 percentage scale, became a shared language for human and machine understanding. We retrace 1980s efforts at UW-Madison and UNC Chapel Hill to "percentize" variables to make regression coefficients interpretable, and relate these experiments to established indices, notably the Pearson (1895) correlation r (range -1 to 1) and the coefficient of determination r-squared (Wright, 1920). We also revisit Cohen et al.'s (1999) percent of maximum possible (POMP) metric. The lineage of 0-100 and 0-1 scales includes Roman fiscal practice, early American grading at Yale and Harvard, and recurring analyses of percent (0-100) and percentage (0-1, or -1 to 1) scales that repeatedly reinvent the same indices (Durm, 1993; Schneider and Hutt, 2014). In data mining and machine learning, min-max normalization maps any feature to [0, 1] (i.e., 0-100%), equalizing scale ranges and implied units across percentized variables, which improves comparability of predictors. Under the percentage theory of measurement indices, equality of units is the necessary and sufficient condition for comparing indices (Cohen et al., 1999; Zhao et al., 2024; Zhao and Zhang, 2014). Seen this way, the successes of machine learning and artificial intelligence over the past half century constitute large-scale evidence for the comparability of percentage-based indices, foremost the percentage coefficient (bp).
[101]
arXiv:2508.07982
(replaced)
[pdf, html, other]
Title:
Likelihood Ratio Tests by Kernel Gaussian Embedding
Leonardo V. Santoro, Victor M. Panaretos
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)
We propose a novel kernel-based nonparametric two-sample test, employing the combined use of kernel mean and kernel covariance embedding. Our test builds on recent results showing how such combined embeddings map distinct probability measures to mutually singular Gaussian measures on the kernel's RKHS. Leveraging this ``separation of measure phenomenon", we construct a test statistic based on the relative entropy between the Gaussian embeddings, in effect the likelihood ratio. The likelihood ratio is specifically tailored to detect equality versus singularity of two Gaussians, and satisfies a ``$0/\infty$" law, in that it vanishes under the null and diverges under the alternative. To implement the test in finite samples, we introduce a regularised version, calibrated by way of permutation. We prove consistency, establish uniform power guarantees under mild conditions, and discuss how our framework unifies and extends prior approaches based on spectrally regularized MMD. Empirical results on synthetic and real data demonstrate remarkable gains in power compared to state-of-the-art methods, particularly in high-dimensional and weak-signal regimes.
[102]
arXiv:2508.10342
(replaced)
[pdf, html, other]
Title:
Identifying Unmeasured Confounders in Panel Causal Models: A Two-Stage LM-Wald Approach
Bang Quan Zheng
Subjects:
Methodology (stat.ME)
Panel data are widely used in political science to draw causal inferences. However, these models often rely on the strong and untested assumption of sequential ignorability--that no unmeasured variables influence both the independent and outcome variables across time. Grounded in psychometric literature on latent variable modeling, this paper introduces the Two-Stage LM-Wald (2SLW) approach, a diagnostic tool that extends the Lagrange Multiplier (LM) and Wald tests to detect violations of this assumption in panel causal models. Using Monte Carlo simulations within the Random Intercept Cross-Lagged Panel Model (RI-CLPM), which separates within and between person effects, I demonstrate the 2SLW's ability to detect unmeasured confounding across three key scenarios: biased corrections, distorted direct effects, and altered mediation pathways. I also illustrate the approach with an empirical application to real-world panel data. By providing a practical and theoretically grounded diagnostic, the 2SLW approach enhances the robustness of causal inferences in the presence of potential time-varying confounders. Moreover, it can be readily implemented using the R package lavaan.
[103]
arXiv:2508.10447
(replaced)
[pdf, other]
Title:
BKP: An R Package for Beta Kernel Process Modeling
Jiangyan Zhao, Kunhai Qing, Jin Xu
Comments:
37 pages, 15 figures, and 2 tables
Subjects:
Computation (stat.CO); Machine Learning (stat.ML)
We present BKP, a user-friendly and extensible R package that implements the Beta Kernel Process (BKP) -- a fully nonparametric and computationally efficient framework for modeling spatially varying binomial probabilities. The BKP model combines localized kernel-weighted likelihoods with conjugate beta priors, resulting in closed-form posterior inference without requiring latent variable augmentation or intensive MCMC sampling. The package supports binary and aggregated binomial responses, allows flexible choices of kernel functions and prior specification, and provides loss-based kernel hyperparameter tuning procedures. In addition, BKP extends naturally to the Dirichlet Kernel Process (DKP) for modeling spatially varying multinomial or compositional data. To our knowledge, this is the first publicly available R package for implementing BKP-based methods. We illustrate the use of BKP through several synthetic and real-world datasets, highlighting its interpretability, accuracy, and scalability. The package aims to facilitate practical application and future methodological development of kernel-based beta modeling in statistics and machine learning.
[104]
arXiv:2508.12426
(replaced)
[pdf, html, other]
Title:
Asymptotic breakdown point analysis of the minimum density power divergence estimator under independent non-homogeneous setups
Suryasis Jana, Subhrajyoty Roy, Ayanendranath Basu, Abhik Ghosh
Comments:
Pre-print; under review
Subjects:
Statistics Theory (math.ST); Machine Learning (stat.ML)
The minimum density power divergence estimator (MDPDE) has gained significant attention in the literature of robust inference due to its strong robustness properties and high asymptotic efficiency; it is relatively easy to compute and can be interpreted as a generalization of the classical maximum likelihood estimator. It has been successfully applied in various setups, including the case of independent and non-homogeneous (INH) observations that cover both classification and regression-type problems with a fixed design. While the local robustness of this estimator has been theoretically validated through the bounded influence function, no general result is known about the global reliability or the breakdown behavior of this estimator under the INH setup, except for the specific case of location-type models. In this paper, we extend the notion of asymptotic breakdown point from the case of independent and identically distributed data to the INH setup and derive a theoretical lower bound for the asymptotic breakdown point of the MDPDE, under some easily verifiable assumptions. These results are further illustrated with applications to some fixed design regression models and corroborated through extensive simulation studies.
[105]
arXiv:2509.01924
(replaced)
[pdf, html, other]
Title:
Non-Linear Model-Based Sequential Decision-Making in Agriculture
Sakshi Arya, Wentao Lin
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP); Methodology (stat.ME)
Sequential decision-making is central to sustainable agricultural management and precision agriculture, where resource inputs must be optimized under uncertainty and over time. However, such decisions must often be made with limited observations, whereas classical bandit and reinforcement learning approaches typically rely on either linear or black-box reward models that may misrepresent domain knowledge or require large amounts of data. We propose a family of \emph{nonlinear, model-based bandit algorithms} that embed domain-specific response curves directly into the exploration-exploitation loop. By coupling (i) principled uncertainty quantification with (ii) closed-form or rapidly computable profit optima, these algorithms achieve sublinear regret and near-optimal sample complexity while preserving interpretability. Theoretical analysis establishes regret and sample complexity bounds, and extensive simulations emulating real-world fertilizer-rate decisions show consistent improvements over both linear and nonparametric baselines (such as linear UCB and $k$-NN UCB) in the low-sample regime, under both well-specified and shape-compatible misspecified models. Because our approach leverages mechanistic insight rather than large data volumes, it is especially suited to resource-constrained settings, supporting sustainable, inclusive, and transparent sequential decision-making across agriculture, environmental management, and allied applications.
[106]
arXiv:2509.02871
(replaced)
[pdf, html, other]
Title:
Learning from geometry-aware near-misses to real-time COR: A spatiotemporal grouped random GEV framework
Mohammad Anis, Yang Zhou, Dominique Lord
Comments:
13 figures, 8 Tables
Subjects:
Applications (stat.AP); Computation (stat.CO)
Real-time prediction of corridor-level crash occurrence risk (COR) remains challenging, as existing near-miss based extreme value models oversimplify collision geometry, exclude vehicle-infrastructure (V-I) interactions, and inadequately capture spatial heterogeneity in vehicle dynamics. This study introduces a geometry-aware two-dimensional time-to-collision (2D-TTC) indicator within a Hierarchical Bayesian spatiotemporal grouped random parameter (HBSGRP) framework using a non-stationary univariate generalized extreme value (UGEV) model to estimate short-term COR in urban corridors. High-resolution trajectories from the Argoverse-2 dataset, covering 28 locations along Miami's Biscayne Boulevard, were analyzed to extract extreme V-V and V-I near misses. The model incorporates dynamic variables and roadway features as covariates, with partial pooling across locations to address unobserved heterogeneity. Results show that the HBSGRP-UGEV framework outperforms fixed-parameter alternatives, reducing DIC by up to 7.5% for V-V and 3.1% for V-I near-misses. Predictive validation using ROC-AUC confirms strong performance: 0.89 for V-V segments, 0.82 for V-V intersections, 0.79 for V-I segments, and 0.75 for V-I intersections. Model interpretation reveals that relative speed and distance dominate V-V risks at intersections and segments, with deceleration critical in segments, while V-I risks are driven by speed, boundary proximity, and steering/heading adjustments. These findings highlight the value of a statistically rigorous, geometry-sensitive, and spatially adaptive modeling approach for proactive corridor-level safety management, supporting real-time interventions and long-term design strategies aligned with Vision Zero.
[107]
arXiv:2509.03702
(replaced)
[pdf, html, other]
Title:
A Case for a "Refutations and Critiques'' Track in Statistics Journals
Zhen Li
Comments:
9 pages. Comments on two papers, one with an earlier arXiv version (arXiv:2309.12872), both published in the Journal of the American Statistical Association (DOI: https://doi.org/10.1080/01621459.2024.2412364%3B DOI: https://doi.org/10.1080/01621459.2021.1999818)
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST)
The statistics community, which has traditionally lacked a transparent and open peer-review system, faces a challenge of inconsistent paper quality, with some published work containing substantial errors. This problem resonates with concerns raised by Schaeffer et al. (2025) regarding the rapid growth of machine learning research. They argue that peer review has proven insufficient to prevent the publication of ``misleading, incorrect, flawed or perhaps even fraudulent studies'' and that a ``dynamic self-correcting research ecosystem'' is needed. This note provides a concrete illustration of this problem by examining two published papers, Wang, Zhou and Lin (2025) and Liu et al. (2023), and exposing striking and critical errors in their proofs. The presence of such errors in major journals raises a fundamental question about the importance and verification of mathematical proofs in our field. Echoing the proposal from Schaeffer et al. (2025), we argue that reforming the peer-review system itself is likely impractical. Instead, we propose a more viable path forward: the creation of a high-profile, reputable platform, such as a ``Refutations and Critiques'' track on arXiv, to provide visibility to vital research that critically challenges prior work. Such a mechanism would be crucial for enhancing the reliability and credibility of statistical research.
[108]
arXiv:2509.09078
(replaced)
[pdf, html, other]
Title:
Scalable extensions to given-data Sobol' index estimators
Teresa Portone, Bert Debusschere, Samantha Yang, Emiliano Islas-Quinones, T. Patrick Xiao
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP); Computation (stat.CO)
Given-data methods for variance-based sensitivity analysis have significantly advanced the feasibility of Sobol' index computation for computationally expensive models and models with many inputs. However, the limitations of existing methods still preclude their application to models with an extremely large number of inputs. In this work, we present practical extensions to the existing given-data Sobol' index method, which allow variance-based sensitivity analysis to be efficiently performed on large models such as neural networks, which have $>10^4$ parameterizable inputs. For models of this size, holding all input-output evaluations simultaneously in memory -- as required by existing methods -- can quickly become impractical. These extensions also support nonstandard input distributions with many repeated values, which are not amenable to equiprobable partitions employed by existing given-data methods.
Our extensions include a general definition of the given-data Sobol' index estimator with arbitrary partition, a streaming algorithm to process input-output samples in batches, and a heuristic to filter out small indices that are indistinguishable from zero indices due to statistical noise. We show that the equiprobable partition employed in existing given-data methods can introduce significant bias into Sobol' index estimates even at large sample sizes and provide numerical analyses that demonstrate why this can occur. We also show that our streaming algorithm can achieve comparable accuracy and runtimes with lower memory requirements, relative to current methods which process all samples at once. We demonstrate our novel developments on two application problems in neural network modeling.
[109]
arXiv:2509.09758
(replaced)
[pdf, html, other]
Title:
A Path Signature Framework for Detecting Creative Fatigue in Digital Advertising
Charles Shaw
Comments:
version 2
Subjects:
Applications (stat.AP)
This paper introduces a novel framework for detecting advertising creative fatigue using path signature analysis, a geometric approach from rough path theory not previously applied in marketing. Creative fatigue - the decline in advertising effectiveness over time - poses a major risk to digital media spend. Our method transforms performance time-series into geometric paths, extracting high-dimensional signatures to capture complex dynamics: volatility shifts, trend reversals, and non-linear decay. Unlike traditional statistical methods, this approach detects subtle, multi-scale changes. We validate the framework using synthetic datasets replicating documented fatigue patterns from marketing literature. Results show superior early detection, enabling actionable insights before significant budget loss. A novel financial impact model quantifies opportunity costs from delayed detection, while computational analysis confirms linear scalability for real-time monitoring of large creative portfolios.
[110]
arXiv:1112.1768
(replaced)
[pdf, html, other]
Title:
Extended UCB Policies for Multi-armed Bandit Problems
Keqin Liu, Tianshuo Zheng, Zhi-Hua Zhou
Comments:
32 pages, 10 figures
Subjects:
Machine Learning (cs.LG); Probability (math.PR); Statistics Theory (math.ST)
The multi-armed bandit (MAB) problems are widely studied in fields of operations research, stochastic optimization, and reinforcement learning. In this paper, we consider the classical MAB model with heavy-tailed reward distributions and introduce the extended robust UCB policy, which is an extension of the results of Bubeck et al. [5] and Lattimore [22] that are further based on the pioneering idea of UCB policies [e.g. Auer et al. 3]. The previous UCB policies require some strict conditions on reward distributions, which can be difficult to guarantee in practical scenarios. Our extended robust UCB generalizes Lattimore's seminary work (for moments of orders $p=4$ and $q=2$) to arbitrarily chosen $p>q>1$ as long as the two moments have a known controlled relationship, while still achieving the optimal regret growth order $O(log T)$, thus providing a broadened application area of UCB policies for heavy-tailed reward distributions. Furthermore, we achieve a near-optimal regret order without any knowledge of the reward distributions as long as their $p$-th moments exist for some $p>1$. Finally, we briefly present our earlier work on light-tailed reward distributions for a complete illustration of the amazing simplicity and power of UCB policies.
[111]
arXiv:2309.14326
(replaced)
[pdf, html, other]
Title:
Efficient Pauli channel estimation with logarithmic quantum memory
Sitan Chen, Weiyuan Gong
Comments:
57 pages, 1 figure
Subjects:
Quantum Physics (quant-ph); Computational Complexity (cs.CC); Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST)
Here we revisit one of the prototypical tasks for characterizing the structure of noise in quantum devices: estimating every eigenvalue of an $n$-qubit Pauli noise channel to error $\epsilon$. Prior work [14] proved no-go theorems for this task in the practical regime where one has a limited amount of quantum memory, e.g. any protocol with $\le 0.99n$ ancilla qubits of quantum memory must make exponentially many measurements, provided it is non-concatenating. Such protocols can only interact with the channel by repeatedly preparing a state, passing it through the channel, and measuring immediately afterward.
This left open a natural question: does the lower bound hold even for general protocols, i.e. ones which chain together many queries to the channel, interleaved with arbitrary data-processing channels, before measuring? Surprisingly, in this work we show the opposite: there is a protocol that can estimate the eigenvalues of a Pauli channel to error $\epsilon$ using only $O(\log n/\epsilon^2)$ ancilla and $\tilde{O}(n^2/\epsilon^2)$ measurements. In contrast, we show that any protocol with zero ancilla, even a concatenating one, must make $\Omega(2^n/\epsilon^2)$ measurements, which is tight.
Our results imply, to our knowledge, the first quantum learning task where logarithmically many qubits of quantum memory suffice for an exponential statistical advantage. Our protocol can be naturally extended to a protocol that learns the eigenvalues of Pauli terms within any subset $A$ of a Pauli channel with $O(\log\log(|A|)/\epsilon^2)$ ancilla and $\tilde{O}(n^2/\epsilon^2)$ measurements.
[112]
arXiv:2401.10791
(replaced)
[pdf, html, other]
Title:
Early alignment in two-layer networks training is a two-edged sword
Etienne Boursier, Nicolas Flammarion
Comments:
Official JMLR version
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al. (2018). For small initialisation and one hidden ReLU layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and only converge to a spurious stationary point instead.
[113]
arXiv:2401.14883
(replaced)
[pdf, html, other]
Title:
Convergence of the Adapted Smoothed Empirical Measures
Songyan Hou
Subjects:
Probability (math.PR); Statistics Theory (math.ST)
The adapted Wasserstein distance controls the calibration errors of optimal values in various stochastic optimization problems, pricing and hedging problems, optimal stopping problems, etc. However, statistical aspects of the adapted Wasserstein distance are bottlenecked by the failure of empirical measures to converge under this distance. Kernel smoothing and adapted projection have been introduced to construct converging substitutes of empirical measures, known respectively as smoothed empirical measures and adapted empirical measures. However, both approaches have limitations. Specifically, smoothed empirical measures lack comprehensive convergence results, whereas adapted empirical measures in practical applications lead to fewer distinct samples compared to standard empirical measures.
In this work, we address both of the aforementioned issues. First, we develop comprehensive convergence results of smoothed empirical measures. We then introduce a smoothed version for adapted empirical measures, which provide as many distinct samples as desired. We refer them as adapted smoothed empirical measures and establish their convergence in mean, deviation, and almost sure convergence. The convergence estimation incorporates two results: the empirical analysis of the smoothed adapted Wasserstein distance and its bandwidth effects. Both results are novel and their proof techniques could be of independent interest.
[114]
arXiv:2404.19707
(replaced)
[pdf, html, other]
Title:
Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models
Savi Virolainen
Subjects:
Econometrics (econ.EM); Statistics Theory (math.ST); Methodology (stat.ME)
We show that structural smooth transition vector autoregressive models are statistically identified if the shocks are mutually independent and at most one of them is Gaussian. This extends a known identification result for linear structural vector autoregressions to a time-varying impact matrix. We also propose an estimation method, show how a blended identification strategy can be adopted to address weak identification, and establish a sufficient condition for ergodic stationarity. The introduced methods are implemented in the accompanying R package sstvars. Our empirical application finds that a positive climate policy uncertainty shock reduces production and raises inflation under both low and high economic policy uncertainty, but its effects, particularly on inflation, are stronger during the latter.
[115]
arXiv:2408.03051
(replaced)
[pdf, html, other]
Title:
The multivariate fractional Ornstein-Uhlenbeck process
Ranieri Dugo, Giacomo Giorgio, Paolo Pigato
Comments:
40 pages, 6 figures
Subjects:
Probability (math.PR); Methodology (stat.ME)
Starting from the notion of multivariate fractional Brownian Motion introduced in [F. Lavancier, A. Philippe, and D. Surgailis. Covariance function of vector self-similar processes. Statistics & Probability Letters, 2009] we define a multivariate version of the fractional Ornstein-Uhlenbeck process. This multivariate Gaussian process is stationary, ergodic and allows for different Hurst exponents on each component. We characterize its correlation matrix and its short and long time asymptotics. Besides the marginal parameters, the cross correlation between one-dimensional marginal components is ruled by two parameters. We consider the problem of their inference, proposing two types of estimator, constructed from discrete observations of the process. We establish their asymptotic theory, in one case in the long time asymptotic setting, in the other case in the infill and long time asymptotic setting. The limit behavior can be asymptotically Gaussian or non-Gaussian, depending on the values of the Hurst exponents of the marginal components. The technical core of the paper relies on the analysis of asymptotic properties of functionals of Gaussian processes, that we establish using Malliavin calculus and Stein's method. We provide numerical experiments that support our theoretical analysis and also suggest a conjecture on the application of one of these estimators to the multivariate fractional Brownian Motion.
[116]
arXiv:2410.07443
(replaced)
[pdf, html, other]
Title:
On the Lower Confidence Band for the Optimal Welfare in Policy Learning
Kirill Ponomarev, Vira Semenova
Subjects:
Econometrics (econ.EM); Statistics Theory (math.ST)
We study inference on the optimal welfare in a policy learning problem and propose reporting a lower confidence band (LCB). A natural approach to constructing an LCB is to invert a one-sided t-test based on an efficient estimator for the optimal welfare. However, we show that for an empirically relevant class of DGPs, such an LCB can be first-order dominated by an LCB based on a welfare estimate for a suitable suboptimal treatment policy. We show that such first-order dominance is possible if and only if the optimal treatment policy is not ``well-separated'' from the rest, in the sense of the commonly imposed margin condition. When this condition fails, standard debiased inference methods are not applicable. We show that uniformly valid and easy-to-compute LCBs can be constructed analytically by inverting moment-inequality tests with the maximum and quasi-likelihood-ratio test statistics. As an empirical illustration, we revisit the National JTPA study and find that the proposed LCBs achieve reliable coverage and competitive length.
[117]
arXiv:2412.14222
(replaced)
[pdf, html, other]
Title:
A Survey on Large Language Model-based Agents for Statistics and Data Science
Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, Jian Huang
Subjects:
Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Other Statistics (stat.OT)
In recent years, data science agents powered by Large Language Models (LLMs), known as "data agents," have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.
[118]
arXiv:2501.06376
(replaced)
[pdf, html, other]
Title:
Robustness in the Face of Partial Identifiability in Reward Learning
Filippo Lazzati, Alberto Maria Metelli
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
In Reward Learning (ReL), we are given feedback on an unknown target reward, and the goal is to use this information to recover it in order to carry out some downstream application, e.g., planning. When the feedback is not informative enough, the target reward is only partially identifiable, i.e., there exists a set of rewards, called the feasible set, that are equally plausible candidates for the target reward. In these cases, the ReL algorithm might recover a reward function different from the target reward, possibly leading to a failure in the application. In this paper, we introduce a general ReL framework that permits to quantify the drop in "performance" suffered in the considered application because of identifiability issues. Building on this, we propose a robust approach to address the identifiability problem in a principled way, by maximizing the "performance" with respect to the worst-case reward in the feasible set. We then develop Rob-ReL, a ReL algorithm that applies this robust approach to the subset of ReL problems aimed at assessing a preference between two policies, and we provide theoretical guarantees on sample and iteration complexity for Rob-ReL. We conclude with a proof-of-concept experiment to illustrate the considered setting.
[119]
arXiv:2502.14479
(replaced)
[pdf, other]
Title:
Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework
Arno Botha, Tanja Verster, Roland Breedt
Comments:
35 pages, 9503 words, 11 figures
Subjects:
Risk Management (q-fin.RM); Statistical Finance (q-fin.ST); Applications (stat.AP)
The lifetime behaviour of loans is notoriously difficult to model, which can compromise a bank's financial reserves against future losses, if modelled poorly. Therefore, we present a data-driven comparative study amongst three techniques in modelling a series of default risk estimates over the lifetime of each loan, i.e., its term-structure. The behaviour of loans can be described using a nonstationary and time-dependent semi-Markov model, though we model its elements using a multistate regression-based approach. As such, the transition probabilities are explicitly modelled as a function of a rich set of input variables, including macroeconomic and loan-level inputs. Our modelling techniques are deliberately chosen in ascending order of complexity: 1) a Markov chain; 2) beta regression; and 3) multinomial logistic regression. Using residential mortgage data, our results show that each successive model outperforms the previous, likely as a result of greater sophistication. This finding required devising a novel suite of simple model diagnostics, which can itself be reused in assessing sampling representativeness and the performance of other modelling techniques. These contributions surely advance the current practice within banking when conducting multistate modelling. Consequently, we believe that the estimation of loss reserves will be more timeous and accurate under IFRS 9.
[120]
arXiv:2503.19605
(replaced)
[pdf, html, other]
Title:
Lean Formalization of Generalization Error Bound by Rademacher Complexity
Sho Sonoda, Kazumi Kasaura, Yuma Mizuno, Kei Tsukamoto, Naoto Onda
Comments:
major updated
Subjects:
Machine Learning (cs.LG); Computation and Language (cs.CL); Statistics Theory (math.ST)
We formalize the generalization error bound using the Rademacher complexity for the Lean 4 theorem prover based on the probability theory in the Mathlib 4 library. Generalization error quantifies the gap between a learning machine's performance on given training data versus unseen test data, and the Rademacher complexity is a powerful tool to upper-bound the generalization error of a variety of modern learning problems. Previous studies have only formalized extremely simple cases such as bounds by parameter counts and analyses for very simple models (decision stumps). Formalizing the Rademacher complexity bound, also known as the uniform law of large numbers, requires substantial development and is achieved for the first time in this study. In the course of development, we formalize the Rademacher complexity and its unique arguments such as symmetrization, and clarify the topological assumptions on hypothesis classes under which the bound holds. As an application, we also present the formalization of generalization error bound for $L^2$-regularization models.
[121]
arXiv:2504.03228
(replaced)
[pdf, html, other]
Title:
Weak instrumental variables due to nonlinearities in panel data: A Super Learner Control Function estimator
Monika Avila Marquez
Subjects:
Econometrics (econ.EM); Machine Learning (stat.ML)
A triangular structural panel data model with additive separable individual-specific effects is used to model the causal effect of a covariate on an outcome variable when there are unobservable confounders with some of them time-invariant. In this setup, a linear reduced-form equation might be problematic when the conditional mean of the endogenous covariate and the instrumental variables is nonlinear. The reason is that ignoring the nonlinearity could lead to weak instruments As a solution, we propose a triangular simultaneous equation model for panel data with additive separable individual-specific fixed effects composed of a linear structural equation with a nonlinear reduced form equation. The parameter of interest is the structural parameter of the endogenous variable. The identification of this parameter is obtained under the assumption of available exclusion restrictions and using a control function approach. Estimating the parameter of interest is done using an estimator that we call Super Learner Control Function estimator (SLCFE). The estimation procedure is composed of two main steps and sample splitting. We estimate the control function using a super learner using sample splitting. In the following step, we use the estimated control function to control for endogeneity in the structural equation. Sample splitting is done across the individual dimension. We perform a Monte Carlo simulation to test the performance of the estimators proposed. We conclude that the Super Learner Control Function Estimators significantly outperform Within 2SLS estimators.
[122]
arXiv:2504.08224
(replaced)
[pdf, html, other]
Title:
All Optical Echo State Network Reservoir Computing
Ishwar S Kaushik, Peter J Ehlers, Daniel Soh
Comments:
14 pages, 11 figures
Subjects:
Optics (physics.optics); Machine Learning (cs.LG); Machine Learning (stat.ML)
We propose an innovative design for an all-optical Echo State Network (ESN), an advanced type of reservoir computer known for its universal computational capabilities. Our design enables fully optical implementation of arbitrary ESNs, featuring flexibility in optical matrix multiplication and nonlinear activation. Leveraging the nonlinear characteristics of stimulated Brillouin scattering (SBS), the architecture efficiently realizes measurement-free nonlinear activation. The approach significantly reduces computational overhead and energy consumption compared to traditional software-based methods. Comprehensive simulations validate the system's memory capacity, nonlinear processing strength, and polynomial algebra capabilities, showcasing performance comparable to software ESNs across key benchmark tasks. Our design establishes a feasible, scalable, and universally applicable framework for optical reservoir computing, suitable for diverse machine learning applications.
[123]
arXiv:2508.10663
(replaced)
[pdf, html, other]
Title:
Higher-order Gini indices: An axiomatic approach
Xia Han, Ruodu Wang, Qinyu Wu
Subjects:
Mathematical Finance (q-fin.MF); Econometrics (econ.EM); Statistics Theory (math.ST)
Via an axiomatic approach, we characterize the family of n-th order Gini deviation, defined as the expected range over n independent draws from a distribution, to quantify joint dispersion across multiple observations. This family extends the classical Gini deviation, which relies solely on pairwise comparisons. The normalized version is called a high-order Gini coefficient. The generalized indices grow increasingly sensitive to tail inequality as n increases, offering a more nuanced view of distributional extremes. The higher-order Gini deviations admit a Choquet integral representation, inheriting the desirable properties of coherent deviation measures. Furthermore, we show that both the n-th order Gini deviation and the n-th order Gini coefficient are statistically n-observation elicitable, allowing for direct computation through empirical risk minimization. Data analysis using World Inequality Database data reveals that higher-order Gini coefficients capture disparities that the classical Gini coefficient may fail to reflect, particularly in cases of extreme income or wealth concentration.
[124]
arXiv:2508.11974
(replaced)
[pdf, html, other]
Title:
Binomial maps: stochastically evolving iterated integer maps for finite populations
Snehal M. Shekatkar
Comments:
6 pages, 4 figures
Subjects:
Populations and Evolution (q-bio.PE); Chaotic Dynamics (nlin.CD); Applications (stat.AP)
Many models of population dynamics are formulated as deterministic iterated maps although real populations are stochastic. This is justifiable in the limit of large population sizes, as the stochastic fluctuations are negligible then. However, this also makes it challenging to use the same models for small populations where finite size effects like demographic noise and extinction cannot be ignored. Moreover, adding noise to the equations does not solve this problem as it can only represent the environmental stochasticity. An approach, sometimes used in ecological literature, but surprisingly uncommon in dynamical systems community, is \emph{Binomial maps}, which allow stochastic evolution of deterministic iterated map models of population. Here we present their formulation in a way so as to make their connection to the agent-based models explicit, and demonstrate it for the Logistic and Ricker maps. We also show that the Binomial maps are not completely equivalent to their deterministic counterparts, and derive sufficient conditions under which the equivalence holds. This approach enables rigorous finite-population analysis within familiar map-based models, bridging the deterministic map models and stochastic agent-based models.
[125]
arXiv:2509.08708
(replaced)
[pdf, other]
Title:
Quantifying model prediction sensitivity to model-form uncertainty
Teresa Portone, Rebekah D. White, Joseph L. Hart
Subjects:
Computational Engineering, Finance, and Science (cs.CE); Computational Physics (physics.comp-ph); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)
Model-form uncertainty (MFU) in assumptions made during physics-based model development is widely considered a significant source of uncertainty; however, there are limited approaches that can quantify MFU in predictions extrapolating beyond available data. As a result, it is challenging to know how important MFU is in practice, especially relative to other sources of uncertainty in a model, making it difficult to prioritize resources and efforts to drive down error in model predictions. To address these challenges, we present a novel method to quantify the importance of uncertainties associated with model assumptions. We combine parameterized modifications to assumptions (called MFU representations) with grouped variance-based sensitivity analysis to measure the importance of assumptions. We demonstrate how, in contrast to existing methods addressing MFU, our approach can be applied without access to calibration data. However, if calibration data is available, we demonstrate how it can be used to inform the MFU representation, and how variance-based sensitivity analysis can be meaningfully applied even in the presence of dependence between parameters (a common byproduct of calibration).
[126]
arXiv:2509.08926
(replaced)
[pdf, html, other]
Title:
Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures
Waqar Ahmad, Evan Murphy, Vladimir A. Krylov
Subjects:
Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)
Object re-identification (Re-ID) methods are highly sensitive to label noise, which typically leads to significant performance degradation. We address this challenge by reframing Re-ID as a supervised image similarity task and adopting a Siamese network architecture trained to capture discriminative pairwise relationships. Central to our approach is a novel statistical outlier detection (OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier Detection), which models the distribution of cosine similarities between embedding pairs using a two-component Beta distribution mixture model. We establish a novel identifiability result for mixtures of two Beta distributions, ensuring that our learning task is well-posed. The proposed OD step complements the Re-ID architecture combining binary cross-entropy, contrastive, and cosine embedding losses that jointly optimize feature-level similarity learning. We demonstrate the effectiveness of Beta-SOD in de-noising and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance compared to the state-of-the-art methods across various noise levels (10-30\%), demonstrating both robustness and broad applicability in noisy Re-ID scenarios. The implementation of Beta-SOD is available at: this http URL
[127]
arXiv:2509.09070
(replaced)
[pdf, html, other]
Title:
STRIDE: Subset-Free Functional Decomposition for XAI in Tabular Settings
Chaeyun Ko
Comments:
Major revision for submission to ICLR 2026. Substantially revised abstract, introduction, and discussion. Added new 'component surgery' analysis and updated benchmark results for clarity. (12 pages, 2 figures)
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Most explainable AI (XAI) frameworks are limited in their expressiveness, summarizing complex feature effects as single scalar values \phi_i. This approach answers "what" features are important but fails to reveal "how" they interact. Furthermore, methods that attempt to capture interactions, like those based on Shapley values, often face an exponential computational cost. We present STRIDE, a scalable framework that addresses both limitations by reframing explanation as a subset-enumeration-free, orthogonal "functional decomposition" in a Reproducing Kernel Hilbert Space (RKHS). In the tabular setups we study, STRIDE analytically computes functional components f_S(x_S) via a recursive kernel-centering procedure. The approach is model-agnostic and theoretically grounded with results on orthogonality and L^2 convergence. In tabular benchmarks (10 datasets, median over 10 seeds), STRIDE attains a 3.0 times median speedup over TreeSHAP and a mean R^2=0.93 for reconstruction. We also introduce "component surgery", a diagnostic that isolates a learned interaction and quantifies its contribution; on California Housing, removing a single interaction reduces test R^2 from 0.019 to 0.027.
[128]
arXiv:2509.09362
(replaced)
[pdf, html, other]
Title:
Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation
Hanfei Zhou, Lei Shi
Subjects:
Numerical Analysis (math.NA); Machine Learning (cs.LG); Machine Learning (stat.ML)
A key challenge in scientific machine learning is solving partial differential equations (PDEs) on complex domains, where the curved geometry complicates the approximation of functions and their derivatives required by differential operators. This paper establishes the first simultaneous approximation theory for deep neural networks on manifolds. We prove that a constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property that plays a crucial role in controlling generalization error--can approximate any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for $k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero parameters, a rate that overcomes the curse of dimensionality by depending only on the intrinsic dimension $d$. These results readily extend to functions in H√∂lder-Zygmund spaces. We complement this result with a matching lower bound, proving our construction is nearly optimal by showing the required number of parameters matches up to a logarithmic factor. Our proof of the lower bound introduces novel estimates for the Vapnik-Chervonenkis dimension and pseudo-dimension of the network's high-order derivative classes. These complexity bounds provide a theoretical cornerstone for learning PDEs on manifolds involving derivatives. Our analysis reveals that the network architecture leverages a sparse structure to efficiently exploit the manifold's low-dimensional geometry.
Total of 128 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack