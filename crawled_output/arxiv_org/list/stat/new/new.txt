Statistics
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
stat
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Statistics
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Thursday, 18 September 2025
Total of 54 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 17 of 17 entries)
[1]
arXiv:2509.13384
[pdf, html, other]
Title:
A tree-based Polynomial Chaos expansion for surrogate modeling and sensitivity analysis of complex numerical models
Faten Ben Said (CERMICS, EDF R\&amp;D LNHE), Aurélien Alfonsi (CERMICS, MATHRISK), Anne Dutfoy (EDF R\&amp;D PERICLES), Cédric Goeury (EDF R\&amp;D LNHE, LHSV), Magali Jodeau (EDF R\&amp;D LNHE, LHSV), Julien Reygner (CERMICS, RT-UQ), Fabrice Zaoui (EDF R\&amp;D LNHE)
Subjects:
Methodology (stat.ME)
This paper introduces Tree-based Polynomial Chaos Expansion (Tree-PCE), a novel surrogate modeling technique designed to efficiently approximate complex numerical models exhibiting nonlinearities and discontinuities. Tree-PCE combines the expressive power of Polynomial Chaos Expansion (PCE) with an adaptive partitioning strategy inspired by regression trees. By recursively dividing the input space into hyperrectangular subdomains and fitting localized PCEs, Tree-PCE constructs a piecewise polynomial surrogate that improves both accuracy and computational efficiency. The method is particularly well-suited for global sensitivity analysis, enabling direct computation of Sobol' indices from local expansion coefficients and introducing a new class of sensitivity indices derived from the tree structure itself. Numerical experiments on synthetic and real-world models, including a 2D morphodynamic case, demonstrate that Tree-PCE offers a favorable balance between accuracy and complexity, especially in the presence of discontinuities. While its performance depends on the compromise between the number of subdomains and the degree of local polynomials, this trade-off can be explored using automated hyperparameter optimization frameworks. This opens promising perspectives for systematically identifying optimal configurations and enhancing the robustness of surrogate modeling in complex systems.
[2]
arXiv:2509.13538
[pdf, html, other]
Title:
Selective and marginal selective inference for exceptional groups
Peter Hoff, Surya Tokdar
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME)
Statistical analyses of multipopulation studies often use the data to select a particular population as the target of inference. For example, a confidence interval may be constructed for a population only in the event that its sample mean is larger than that of the other populations. We show that for the normal means model, confidence interval procedures that maintain strict coverage control conditional on such a selection event will have infinite expected width. For applications where such selective coverage control is of interest, this result motivates the development of procedures with finite expected width and approximate selective coverage control over a range of plausible parameter values. To this end, we develop selection-adjusted empirical Bayes confidence procedures that use information from the data to approximate an oracle confidence procedure that has exact selective coverage control and finite expected width. In numerical comparisons of the oracle and empirical Bayes procedures to procedures that only guarantee selective coverage control marginally over selection events, we find that improved selective coverage control comes at the cost of increased expected interval width.
[3]
arXiv:2509.13606
[pdf, html, other]
Title:
Robust, sub-Gaussian mean estimators in metric spaces
Daniel Bartl, Gabor Lugosi, Roberto Imbuzeiro Oliveira, Zoraida F. Rico
Comments:
42 pages plus 10-page-long appendix (total of 52 pages)
Subjects:
Statistics Theory (math.ST); Probability (math.PR)
Estimating the mean of a random vector from i.i.d. data has received considerable attention, and the optimal accuracy one may achieve with a given confidence is fairly well understood by now. When the data take values in more general metric spaces, an appropriate extension of the notion of the mean is the Fréchet mean. While asymptotic properties of the most natural Fréchet mean estimator (the empirical Fréchet mean) have been thoroughly researched, non-asymptotic performance bounds have only been studied recently.
The aim of this paper is to study the performance of estimators of the Fréchet mean in general metric spaces under possibly heavy-tailed and contaminated data. In such cases, the empirical Fréchet mean is a poor estimator. We propose a general estimator based on high-dimensional extensions of trimmed means and prove general performance bounds. Unlike all previously established bounds, ours generalize the optimal bounds known for Euclidean data. The main message of the bounds is that, much like in the Euclidean case, the optimal accuracy is governed by two "variance" terms: a "global variance" term that is independent of the prescribed confidence, and a potentially much smaller, confidence-dependent "local variance" term.
We apply our results for metric spaces with curvature bounded from below, such as Wasserstein spaces, and for uniformly convex Banach spaces.
[4]
arXiv:2509.13685
[pdf, html, other]
Title:
Variable Selection for Additive Global Fréchet Regression
Haoyi Yang, Satarupa Bhattacharjee, Lingzhou Xue, Bing Li
Comments:
35 pages
Subjects:
Statistics Theory (math.ST); Methodology (stat.ME)
We present a novel framework for variable selection in Fréchet regression with responses in general metric spaces, a setting increasingly relevant for analyzing non-Euclidean data such as probability distributions and covariance matrices. Building on the concept of (weak) Fréchet conditional means, we develop an additive regression model that represents the metric-based discrepancy of the response as a sum of covariate-specific nonlinear functions in reproducing kernel Hilbert spaces (RKHS). To address the absence of linear structure in the response space, we transform the response via squared distances, enabling an interpretable and tractable additive decomposition. Variable selection is performed using Elastic Net regularization, extended to the RKHS setting, and further refined through a local linear approximation scheme that incorporates folded concave penalties such as the SCAD. We establish theoretical guarantees, including variable selection consistency and the strong oracle property, under minimal assumptions tailored to metric-space-valued responses. Simulations and applications to distributional and matrix-valued data demonstrate the scalability, interpretability, and practical effectiveness of the proposed approach. This work provides a principled foundation for statistical learning with random object data.
[5]
arXiv:2509.13778
[pdf, html, other]
Title:
Imputation-Powered Inference
Sarah Zhao, Emmanuel Candès
Subjects:
Methodology (stat.ME); Machine Learning (stat.ML)
Modern multi-modal and multi-site data frequently suffer from blockwise missingness, where subsets of features are missing for groups of individuals, creating complex patterns that challenge standard inference methods. Existing approaches have critical limitations: complete-case analysis discards informative data and is potentially biased; doubly robust estimators for non-monotone missingness-where the missingness patterns are not nested subsets of one another-can be theoretically efficient but lack closed-form solutions and often fail to scale; and blackbox imputation can leverage partially observed data to improve efficiency but provides no inferential guarantees when misspecified. To address the limitations of these existing methods, we propose imputation-powered inference (IPI), a model-lean framework that combines the flexibility of blackbox imputation with bias correction using fully observed data, drawing on ideas from prediction-powered inference and semiparametric inference. IPI enables valid and efficient M-estimation under missing completely at random (MCAR) blockwise missingness and improves subpopulation inference under a weaker assumption we formalize as first-moment MCAR, for which we also provide practical diagnostics. Simulation studies and a clinical application demonstrate that IPI may substantially improve subpopulation efficiency relative to complete-case analysis, while maintaining statistical validity in settings where both doubly robust estimators and naive imputation fail to achieve nominal coverage.
[6]
arXiv:2509.13886
[pdf, html, other]
Title:
Three Distributional Approaches for PM10 Assessment in Northern Italy
Marco F. De Sanctis, Andrea Gilardi, Giacomo Milan, Laura M. Sangalli, Francesca Ieva, Piercesare Secchi
Subjects:
Applications (stat.AP); Methodology (stat.ME)
We propose three spatial methods for estimating the full probability distribution of PM10 concentrations, with the ultimate goal of assessing air quality in Northern Italy. Moving beyond spatial averages and simple indicators, we adopt a distributional perspective to capture the complex variability of pollutant concentrations across space. The first proposed approach predicts class-based compositions via Fixed Rank Kriging; the second estimates multiple, non-crossing quantiles through a spatial regression with differential regularization; the third directly reconstructs full probability densities leveraging on both Fixed Rank Kriging and multiple quantiles spatial regression within a Simplicial Principal Component Analysis framework. These approaches are applied to daily PM10 measurements, collected from 2018 to 2022 in Northern Italy, to estimate spatially continuous distributions and to identify regions at risk of regulatory exceedance. The three approaches exhibit localized differences, revealing how modeling assumptions may influence the prediction of fine-scale pollutant concentration patterns. Nevertheless, they consistently agree on the broader spatial patterns of pollution. This general agreement supports the robustness of a distributional approach, which offers a comprehensive and policy-relevant framework for assessing air quality and regulatory exceedance risks.
[7]
arXiv:2509.13944
[pdf, html, other]
Title:
Bridging Control Variates and Regression Adjustment in A/B Testing: From Design-Based to Model-Based Frameworks
Yu Zhang, Bokui Wan, Yongli Qin
Subjects:
Methodology (stat.ME); Probability (math.PR)
A B testing serves as the gold standard for large scale, data driven decision making in online businesses. To mitigate metric variability and enhance testing sensitivity, control variates and regression adjustment have emerged as prominent variance reduction techniques, leveraging pre experiment data to improve estimator performance. Over the past decade, these methods have spawned numerous derivatives, yet their theoretical connections and comparative properties remain underexplored. In this paper, we conduct a comprehensive analysis of their statistical properties, establish a formal bridge between the two frameworks in practical implementations, and extend the investigation from design based to model-based frameworks. Through simulation studies and real world experiments at ByteDance, we validate our theoretical insights across both frameworks. Our work aims to provide rigorous guidance for practitioners in online controlled experiments, addressing critical considerations of internal and external validity. The recommended method control variates with group specific coefficient estimates has been fully implemented and deployed on ByteDance's experimental platform.
[8]
arXiv:2509.13945
[pdf, html, other]
Title:
Ensembled Direct Multi Step forecasting methodology with comparison on macroeconomic and financial data
Tomasz M. Łapiński, Krzysztof Ziółkowski
Comments:
40 pages, 7 figures
Subjects:
Methodology (stat.ME)
Accurate forecasts of macroeconomic and financial data, such as GDP, CPI, unemployment rates, and stock indices, are crucial for the success of countries, businesses, and investors, resulting in a constant demand for reliable forecasting models. This research introduces a novel methodology for time series forecasting that combines Ensemble technique with a Direct Multi-Step (DMS) forecasting procedure. This Ensembled Direct Multi-Step (EDMS) approach not only leverages the strengths of both techniques but also capitalizes on their synergy. The ensemble models were selected based on performance, complexity, and computational resource requirements, encompassing a full spectrum of model complexities, from simple Linear and Polynomial Regression to medium-complexity ETS and complex LSTM models. Ensembling is carried out using weights derived from each model's performance. The DMS procedure limits retraining to one- and five-year forecasts for economic data and one- and five-month forecasts for financial data. The standard Iterative Multi-Step (IMS) procedure is employed for other horizons, effectively reducing computational demands while maintaining satisfactory results. The proposed methodology is benchmarked against the Ensemble technique conventionally applied to IMS-generated forecasts, utilizing several publicly available macroeconomic datasets, including GDP, CPI, and employment figures across selected countries, and common financial indices data. Results demonstrate a significant performance improvement with the EDMS methodology, averaging a 33.32% enhancement across the analysed datasets, and sometimes reaching improvement above 60%.
[9]
arXiv:2509.13970
[pdf, html, other]
Title:
Optimal Transport Based Testing in Factorial Design
Michel Groppe, Linus Niemöller, Shayan Hundrieser, David Ventzke, Anna Blob, Sarah Köster, Axel Munk
Comments:
45 pages
Subjects:
Statistics Theory (math.ST)
We introduce a general framework for testing statistical hypotheses for probability measures supported on finite spaces, which is based on optimal transport (OT). These tests are inspired by the analysis of variance (ANOVA) and its nonparametric counterparts. They allow for testing linear relationships in factorial designs between discrete probability measures and are based on pairwise comparisons of the OT distance and corresponding barycenters. To this end, we derive under the null hypotheses and (local) alternatives the asymptotic distribution of empirical OT costs and the empirical OT barycenter cost functional as the optimal value of linear programs with random objective function. In particular, we extend existing techniques for probability to signed measures and show directional Hadamard differentiability and the validity of the functional delta method. We discuss computational issues, permutation and bootstrap tests, and back up our findings with simulations. We illustrate our methodology on two datasets from cellular biophysics and biometric identification.
[10]
arXiv:2509.13971
[pdf, html, other]
Title:
Time-smoothed inverse probability weighted estimation of effects of generalized time-varying treatment strategies on repeated outcomes truncated by death
Sean McGrath, Takuya Kawahara, Joshua Petimar, Sheryl L. Rifas-Shiman, Iván Díaz, Jason P. Block, Jessica G. Young
Subjects:
Methodology (stat.ME)
Researchers are often interested in estimating effects of generalized time-varying treatment strategies on the mean of an outcome at one or more selected follow-up times of interest. For example, the Medications and Weight Gain in PCORnet (MedWeight) study aimed to estimate effects of adhering to flexible medication regimes on future weight change using electronic health records (EHR) data. This problem presents several methodological challenges that have not been jointly addressed in the prior literature. First, this setting involves treatment strategies that vary over time and depend dynamically and non-deterministically on measured confounder history. Second, the outcome is repeatedly, non-monotonically, informatively, and sparsely measured in the data source. Third, some individuals die during follow-up, rendering the outcome of interest undefined at the follow-up time of interest. In this article, we pose a range of inverse probability weighted (IPW) estimators targeting effects of generalized time-varying treatment strategies in truncation by death settings that allow time-smoothing for precision gain. We conducted simulation studies that confirm precision gains of the time-smoothed IPW approaches over more conventional IPW approaches that do not leverage the repeated outcome measurements. We illustrate an application of the IPW approaches to estimate comparative effects of adhering to flexible antidepressant medication strategies on future weight change. The methods are implemented in the accompanying R package, smoothedIPW.
[11]
arXiv:2509.14028
[pdf, other]
Title:
Sample Size Calculations for the Development of Risk Prediction Models that Account for Performance Variability
Menelaos Pavlou, Rumana Z. Omar, Gareth Ambler
Comments:
29 pages, 5 figures, 2 tables (plus supplementary material)
Subjects:
Methodology (stat.ME); Applications (stat.AP); Computation (stat.CO)
Existing approaches to sample size calculations for developing clinical prediction models have focused on ensuring that the expected value of a chosen performance measure meets a pre-specified target. For example, to limit model-overfitting, the sample size is commonly chosen such that the expected calibration slope (CS) is 0.9, close to 1 for a perfectly calibrated model. In practice, due to sampling variability, model performance can vary considerably across different development samples of the recommended size. If this variability is high, the probability of obtaining a model with performance close to the target for a given measure may be unacceptably low. To address this, we propose an adapted approach to sample size calculations that explicitly incorporates performance variability by targeting the probability of acceptable performance (PrAP). For example, in the context of calibration, we may define a model as acceptably calibrated if CS falls in a pre-defined range, e.g. between 0.85 and 1.15. Then we choose the required sample size to ensure that PrAP(CS)=80%. For binary outcomes we implemented our approach for CS within a simulation-based framework via the R package `samplesizedev'. Additionally, for CS specifically, we have proposed an equivalent analytical calculation which is computationally efficient. While we focused on CS, the simulation-based framework is flexible and can be easily extended to accommodate other performance measures and types of outcomes. When adhering to existing recommendations, we found that performance variability increased substantially as the number of predictors, p, decreased. Consequently, PrAP(CS) was often low. For example, with 5 predictors, PrAP(CS) was around 50%. Our adapted approach resulted in considerably larger sample sizes, especially for p<10. Applying shrinkage tends to improve PrAP(CS).
[12]
arXiv:2509.14039
[pdf, html, other]
Title:
On the Rate of Gaussian Approximation for Linear Regression Problems
Marat Khusainov, Marina Sheshukova, Alain Durmus, Sergey Samsonov
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Optimization and Control (math.OC)
In this paper, we consider the problem of Gaussian approximation for the online linear regression task. We derive the corresponding rates for the setting of a constant learning rate and study the explicit dependence of the convergence rate upon the problem dimension $d$ and quantities related to the design matrix. When the number of iterations $n$ is known in advance, our results yield the rate of normal approximation of order $\sqrt{\log{n}/n}$, provided that the sample size $n$ is large enough.
[13]
arXiv:2509.14183
[pdf, html, other]
Title:
Index Date Imputation For Survival Outcomes for Externally Controlled Trials
Q. Le Coent, G. L. Rosner, M-C. Wang, C. Hu
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Externally controlled trials (ECTs) compare outcomes between a single-arm trial and external controls drawn from sources such as historical trials, registries, or observational studies. In survival analysis, a major challenge arises when the time origin (index date) differs across groups, for example, when treatment initiation occurs after a delay in the single-arm trial but is undefined in the external controls. This misalignment can bias treatment effect estimates and distort causal interpretation. We propose a novel statistical method, Index Date Imputation (IDI), that imputes comparable index dates for external control patients using the estimated distribution of treatment initiation times from the single-arm cohort. To address population-level confounding, IDI is combined with propensity score weighting or matching, yielding balanced and temporally aligned cohorts for survival comparison. We detail diagnostics for covariate balance and truncation bias, and evaluate performance via extensive simulations. Applying IDI to a randomized oncology trial, we demonstrate that the method recovers the known treatment effect despite artificial index date misalignment. IDI provides a principled framework for time-to-event analyses in ECTs and is broadly applicable in oncology and rare disease settings.
[14]
arXiv:2509.14188
[pdf, html, other]
Title:
Covariate-adjusted Group Sequential Comparisons of Restricted Mean Survival Times
Peter Zhang, Brent Logan, Michael Martens
Subjects:
Methodology (stat.ME)
The restricted mean survival time (RMST) is the mean survival time in the study population followed up to a specific time point, and is simply the area under the survival curve up to the specific time point. The difference between two RMSTs quantifies the group difference in a time scale by measuring the integrated difference between survival curves in the two treatment groups. This paper develops a group sequential (GS) test of comparing two RMSTs up to a restriction time point under a stratified proportional hazards model with stratum representing treatment status. This covariate-adjusted GS test does not require the assumption of a constant hazard ratio over time between the two treatment groups and is valid whether or not the proportional hazards assumption holds for the treatment effect. We establish the large-sample properties of the covariate-adjusted GS test and show that the proposed test statistics sequentially computed at different interim analysis times possess an independent increments covariance structure. Using currently available methodology, this joint independent increments structure allows us to calculate sequential stopping boundaries for preserving the desired type I error probability and maintaining the required statistical power. We evaluate the small-sample performance of the proposed GS test via a simulation study and illustrate its real-world application through analysis of a clinical trial dataset from the BMT CTN 1101 study.
[15]
arXiv:2509.14213
[pdf, html, other]
Title:
PoPStat-COVID19: Leveraging Population Pyramids to Quantify Demographic Vulnerability to COVID-19
Buddhi Wijenayake, Athulya Ratnayake, Lelumi Edirisinghe, Uditha Wijeratne, Tharaka Fonseka, Roshan Godaliyadda, Samath Dharmaratne, Parakrama Ekanayake, Vijitha Herath, Insoha Alwis, Supun Manathunga
Comments:
14 pages, 4 Figures, 25th ICTer Conference
Subjects:
Applications (stat.AP); Information Theory (cs.IT)
Understanding how population age structure shapes COVID-19 burden is crucial for pandemic preparedness, yet common summary measures such as median age ignore key distributional features like skewness, bimodality, and the proportional weight of high-risk cohorts. We extend the PoPStat framework, originally devised to link entire population pyramids with cause-specific mortality by applying it to COVID-19. Using 2019 United Nations World Population Prospects age-sex distributions together with cumulative cases and deaths per million recorded up to 5 May 2023 by Our World in Data, we calculate PoPDivergence (the Kullback-Leibler divergence from an optimised reference pyramid) for 180+ countries and derive PoPStat-COVID19 as the Pearson correlation between that divergence and log-transformed incidence or mortality. Optimisation selects Malta's old-skewed pyramid as the reference, yielding strong negative correlations for cases (r=-0.86, p<0.001, R^2=0.74) and deaths (r=-0.82, p<0.001, R^2=0.67). Sensitivity tests across twenty additional, similarly old-skewed references confirm that these associations are robust to reference choice. Benchmarking against eight standard indicators like gross domestic product per capita, Gini index, Human Development Index, life expectancy at birth, median age, population density, Socio-demographic Index, and Universal Health Coverage Index shows that PoPStat-COVID19 surpasses GDP per capita, median age, population density, and several other traditional measures, and outperforms every comparator for fatality burden. PoPStat-COVID19 therefore provides a concise, distribution-aware scalar for quantifying demographic vulnerability to COVID-19.
[16]
arXiv:2509.14218
[pdf, html, other]
Title:
Adaptive Off-Policy Inference for M-Estimators Under Model Misspecification
James Leiner, Robin Dunn, Aaditya Ramdas
Comments:
36 pages, 6 figures
Subjects:
Methodology (stat.ME); Statistics Theory (math.ST); Machine Learning (stat.ML); Other Statistics (stat.OT)
When data are collected adaptively, such as in bandit algorithms, classical statistical approaches such as ordinary least squares and $M$-estimation will often fail to achieve asymptotic normality. Although recent lines of work have modified the classical approaches to ensure valid inference on adaptively collected data, most of these works assume that the model is correctly specified. We propose a method that provides valid inference for M-estimators that use adaptively collected bandit data with a (possibly) misspecified working model. A key ingredient in our approach is the use of flexible machine learning approaches to stabilize the variance induced by adaptive data collection. A major novelty is that our procedure enables the construction of valid confidence sets even in settings where treatment policies are unstable and non-converging, such as when there is no unique optimal arm and standard bandit algorithms are used. Empirical results on semi-synthetic datasets constructed from the Osteoarthritis Initiative demonstrate that the method maintains type I error control, while existing methods for inference in adaptive settings do not cover in the misspecified case.
[17]
arXiv:2509.14229
[pdf, html, other]
Title:
Spacing Test for Fused Lasso
Rieko Tasaka, Tatsuya Kimura, Joe Suzuki
Subjects:
Statistics Theory (math.ST); Machine Learning (cs.LG)
This study addresses the unresolved problem of selecting the regularization parameter in the fused lasso. In particular, we extend the framework of the Spacing Test proposed by Tibshirani et al. to the fused lasso, providing a theoretical foundation for post-selection inference by characterizing the selection event as a polyhedral constraint. Based on the analysis of the solution path of the fused lasso using a LARS-type algorithm, we derive exact conditional $p$-values for the selected change-points. Our method broadens the applicability of the Spacing Test from the standard lasso to fused penalty structures. Furthermore, through numerical experiments comparing the proposed method with sequential versions of AIC and BIC as well as cross-validation, we demonstrate that the proposed approach properly controls the type I error while achieving high detection power. This work offers a theoretically sound and computationally practical solution for parameter selection and post-selection inference in structured signal estimation problems. Keywords: Fused Lasso, Regularization parameter selection, Spacing Test for Lasso, Selective inference, Change-point detection
Cross submissions (showing 8 of 8 entries)
[18]
arXiv:2509.13388
(cross-list from cs.CV)
[pdf, html, other]
Title:
Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji
Yadvendra Gurjar, Ruoni Wan, Ehsan Farahbakhsh, Rohitash Chandra
Subjects:
Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Applications (stat.AP)
As a developing country, Fiji is facing rapid urbanisation, which is visible in the massive development projects that include housing, roads, and civil works. In this study, we present machine learning and remote sensing frameworks to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The ultimate goal of this study is to provide technical support in land cover/land use modelling and change detection. We used Landsat-8 satellite image for the study region and created our training dataset with labels for supervised machine learning. We used Google Earth Engine and unsupervised machine learning via k-means clustering to generate the land cover map. We used convolutional neural networks to classify the selected regions' land cover types. We present a visualisation of change detection, highlighting urban area changes over time to monitor changes in the map.
[19]
arXiv:2509.13548
(cross-list from cs.SD)
[pdf, html, other]
Title:
Field of View Enhanced Signal Dependent Binauralization with Mixture of Experts Framework for Continuous Source Motion
Manan Mittal, Thomas Deppisch, Joseph Forrer, Chris Le Sueur, Zamir Ben-Hur, David Lou Along, Daniel D.E. Wong
Comments:
5 pages, 3 figures
Subjects:
Sound (cs.SD); Machine Learning (stat.ML)
We propose a novel mixture of experts framework for field-of-view enhancement in binaural signal matching. Our approach enables dynamic spatial audio rendering that adapts to continuous talker motion, allowing users to emphasize or suppress sounds from selected directions while preserving natural binaural cues. Unlike traditional methods that rely on explicit direction-of-arrival estimation or operate in the Ambisonics domain, our signal-dependent framework combines multiple binaural filters in an online manner using implicit localization. This allows for real-time tracking and enhancement of moving sound sources, supporting applications such as speech focus, noise reduction, and world-locked audio in augmented and virtual reality. The method is agnostic to array geometry offering a flexible solution for spatial audio capture and personalized playback in next-generation consumer audio devices.
[20]
arXiv:2509.13805
(cross-list from cs.LG)
[pdf, html, other]
Title:
Towards a Physics Foundation Model
Florian Wiesner, Matthias Wessling, Stephen Baek
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Foundation models have revolutionized natural language processing through a ``train once, deploy anywhere'' paradigm, where a single pre-trained model adapts to countless downstream tasks without retraining. Access to a Physics Foundation Model (PFM) would be transformative -- democratizing access to high-fidelity simulations, accelerating scientific discovery, and eliminating the need for specialized solver development. Yet current physics-aware machine learning approaches remain fundamentally limited to single, narrow domains and require retraining for each new system. We present the General Physics Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that demonstrates foundation model capabilities are achievable for physics. Our key insight is that transformers can learn to infer governing dynamics from context, enabling a single model to simulate fluid-solid interactions, shock waves, thermal convection, and multi-phase dynamics without being told the underlying equations. GPhyT achieves three critical breakthroughs: (1) superior performance across multiple physics domains, outperforming specialized architectures by up to 29x, (2) zero-shot generalization to entirely unseen physical systems through in-context learning, and (3) stable long-term predictions through 50-timestep rollouts. By establishing that a single model can learn generalizable physical principles from data alone, this work opens the path toward a universal PFM that could transform computational science and engineering.
[21]
arXiv:2509.13923
(cross-list from q-fin.ST)
[pdf, other]
Title:
Holdout cross-validation for large non-Gaussian covariance matrix estimation using Weingarten calculus
Lamia Lamrani, Benoît Collins, Jean-Philippe Bouchaud
Subjects:
Statistical Finance (q-fin.ST); Statistics Theory (math.ST); Risk Management (q-fin.RM); Machine Learning (stat.ML)
Cross-validation is one of the most widely used methods for model selection and evaluation; its efficiency for large covariance matrix estimation appears robust in practice, but little is known about the theoretical behavior of its error. In this paper, we derive the expected Frobenius error of the holdout method, a particular cross-validation procedure that involves a single train and test split, for a generic rotationally invariant multiplicative noise model, therefore extending previous results to non-Gaussian data distributions. Our approach involves using the Weingarten calculus and the Ledoit-Péché formula to derive the oracle eigenvalues in the high-dimensional limit. When the population covariance matrix follows an inverse Wishart distribution, we approximate the expected holdout error, first with a linear shrinkage, then with a quadratic shrinkage to approximate the oracle eigenvalues. Under the linear approximation, we find that the optimal train-test split ratio is proportional to the square root of the matrix dimension. Then we compute Monte Carlo simulations of the holdout error for different distributions of the norm of the noise, such as the Gaussian, Student, and Laplace distributions and observe that the quadratic approximation yields a substantial improvement, especially around the optimal train-test split ratio. We also observe that a higher fourth-order moment of the Euclidean norm of the noise vector sharpens the holdout error curve near the optimal split and lowers the ideal train-test ratio, making the choice of the train-test ratio more important when performing the holdout method.
[22]
arXiv:2509.13950
(cross-list from math.NA)
[pdf, html, other]
Title:
Hierarchical Importance Sampling for Estimating Occupation Time for SDE Solutions
Eya Ben Amar, Nadhir Ben Rached, Raul Tempone
Subjects:
Numerical Analysis (math.NA); Computation (stat.CO)
This study considers the estimation of the complementary cumulative distribution function of the occupation time (i.e., the time spent below a threshold) for a process governed by a stochastic differential equation. The focus is on the right tail, where the underlying event becomes rare, and using variance reduction techniques is essential to obtain computationally efficient estimates. Building on recent developments that relate importance sampling (IS) to stochastic optimal control, this work develops an optimal single level IS (SLIS) estimator based on the solution of an auxiliary Hamilton Jacobi Bellman (HJB) partial differential equation (PDE). The cost of solving the HJB-PDE is incorporated into the total computational work, and an optimized trade off between preprocessing and sampling is proposed to minimize the overall cost. The SLIS approach is extended to the multilevel setting to enhance efficiency, yielding a multilevel IS (MLIS) estimator. A necessary and sufficient condition under which the MLIS method outperforms the SLIS method is established, and a common likelihood MLIS formulation is introduced that satisfies this condition under appropriate regularity assumptions. The classical multilevel Monte Carlo complexity theory can be extended to accommodate settings where the single-level variance varies with the discretization level. As a special case, the variance-decay behavior observed in the IS framework stems from the zero variance property of the optimal control. Notably, the total work complexity of MLIS can be better than an order of two. Numerical experiments in the context of fade duration estimation demonstrate the benefits of the proposed approach and validate these theoretical results.
[23]
arXiv:2509.14167
(cross-list from cs.LG)
[pdf, html, other]
Title:
Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework
Md Rezwan Jaher, Abul Mukid Mohammad Mukaddes, A. B. M. Abdul Malek
Comments:
43 pages, 10 figures (including supplementary material)
Subjects:
Machine Learning (cs.LG); Quantitative Methods (q-bio.QM); Applications (stat.AP); Methodology (stat.ME)
Many critical healthcare decisions are challenged by the inability to measure key underlying parameters. Glaucoma, a leading cause of irreversible blindness driven by elevated intraocular pressure (IOP), provides a stark example. The primary determinant of IOP, a tissue property called trabecular meshwork permeability, cannot be measured in vivo, forcing clinicians to depend on indirect surrogates. This clinical challenge is compounded by a broader computational one: developing predictive models for such ill-posed inverse problems is hindered by a lack of ground-truth data and prohibitive cost of large-scale, high-fidelity simulations. We address both challenges with an end-to-end framework to noninvasively estimate unmeasurable variables from sparse, routine data. Our approach combines a multi-stage artificial intelligence architecture to functionally separate the problem; a novel data generation strategy we term PCDS that obviates the need for hundreds of thousands of costly simulations, reducing the effective computational time from years to hours; and a Bayesian engine to quantify predictive uncertainty. Our framework deconstructs a single IOP measurement into its fundamental components from routine inputs only, yielding estimates for the unmeasurable tissue permeability and a patient's outflow facility. Our noninvasively estimated outflow facility achieved excellent agreement with state-of-the-art tonography with precision comparable to direct physical instruments. Furthermore, the newly derived permeability biomarker demonstrates high accuracy in stratifying clinical cohorts by disease risk, highlighting its diagnostic potential. More broadly, our framework establishes a generalizable blueprint for solving similar inverse problems in other data-scarce, computationally-intensive domains.
[24]
arXiv:2509.14186
(cross-list from eess.SP)
[pdf, html, other]
Title:
Quickest Change Detection with Cost-Constrained Experiment Design
Patrick Vincent N. Lubenia, Taposh Banerjee
Subjects:
Signal Processing (eess.SP); Systems and Control (eess.SY); Statistics Theory (math.ST); Methodology (stat.ME)
In the classical quickest change detection problem, an observer performs only one experiment to monitor a stochastic process. This paper considers the case where, at each observation time, the decision-maker needs to choose between multiple experiments with different information qualities and costs. The goal is to minimize the worst-case average detection delay subject to false alarm and cost constraints. An algorithm called the 2E-CUSUM Algorithm has been developed to achieve this goal for the two-experiment case. Extensions to multiple-experiment designs are also studied, and 2E-CUSUM is extended accordingly. Data efficiency, where the observer has the choice not to perform an experiment, is explored as well. The proposed algorithms are analyzed and shown to be asymptotically optimal.
[25]
arXiv:2509.14225
(cross-list from cs.LG)
[pdf, html, other]
Title:
Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics
Benjamin Sterling, Yousef El-Laham, Mónica F. Bugallo
Comments:
5 pages, 2 figures, 1 table
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
Recent advances in generative artificial intelligence applications have raised new data security concerns. This paper focuses on defending diffusion models against membership inference attacks. This type of attack occurs when the attacker can determine if a certain data point was used to train the model. Although diffusion models are intrinsically more resistant to membership inference attacks than other generative models, they are still susceptible. The defense proposed here utilizes critically-damped higher-order Langevin dynamics, which introduces several auxiliary variables and a joint diffusion process along these variables. The idea is that the presence of auxiliary variables mixes external randomness that helps to corrupt sensitive input data earlier on in the diffusion process. This concept is theoretically investigated and validated on a toy dataset and a speech dataset using the Area Under the Receiver Operating Characteristic (AUROC) curves and the FID metric.
Replacement submissions (showing 29 of 29 entries)
[26]
arXiv:2001.10488
(replaced)
[pdf, other]
Title:
Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications
Nassim Nicholas Taleb
Comments:
Third Revised Edition, 2025
Subjects:
Other Statistics (stat.OT); Risk Management (q-fin.RM); Applications (stat.AP); Methodology (stat.ME)
(The third edition corrects minor typos and adds 3 chapters synthesized from published papers plus an appendix on maximum entropy distributions.) The monograph investigates the misapplication of conventional statistical techniques to fat tailed distributions and looks for remedies, when possible.
Switching from thin tailed to fat tailed distributions requires more than "changing the color of the dress". Traditional asymptotics deal mainly with either n=1 or $n=\infty$, and the real world is in between, under of the "laws of the medium numbers" --which vary widely across specific distributions. Both the law of large numbers and the generalized central limit mechanisms operate in highly idiosyncratic ways outside the standard Gaussian or Levy-Stable basins of convergence.
A few examples:
+ The sample mean is rarely in line with the population mean, with effect on "naive empiricism", but can be sometimes be estimated via parametric methods.
+ The "empirical distribution" is rarely empirical.
+ Parameter uncertainty has compounding effects on statistical metrics.
+ Dimension reduction (principal components) fails.
+ Inequality estimators (GINI or quantile contributions) are not additive and produce wrong results.
+ Many "biases" found in psychology become entirely rational under more sophisticated probability distributions
+ Most of the failures of financial economics, econometrics, and behavioral economics can be attributed to using the wrong distributions.
This book, the first volume of the Technical Incerto, weaves a narrative around published journal articles.
[27]
arXiv:2009.00148
(replaced)
[pdf, html, other]
Title:
Design and Analysis of Switchback Experiments
Iavor Bojinov, David Simchi-Levi, Jinglong Zhao
Comments:
Fixed a typo in definition (3). This typo was purely writing and did not change our results or proof
Subjects:
Methodology (stat.ME); Applications (stat.AP)
Switchback experiments, where a firm sequentially exposes an experimental unit to random treatments, are among the most prevalent designs used in the technology sector, with applications ranging from ride-hailing platforms to online marketplaces. Although practitioners have widely adopted this technique, the derivation of the optimal design has been elusive, hindering practitioners from drawing valid causal conclusions with enough statistical power. We address this limitation by deriving the optimal design of switchback experiments under a range of different assumptions on the order of the carryover effect -- the length of time a treatment persists in impacting the outcome. We cast the optimal experimental design problem as a minimax discrete optimization problem, identify the worst-case adversarial strategy, establish structural results, and solve the reduced problem via a continuous relaxation. For switchback experiments conducted under the optimal design, we provide two approaches for performing inference. The first provides exact randomization based p-values, and the second uses a new finite population central limit theorem to conduct conservative hypothesis tests and build confidence intervals. We further provide theoretical results when the order of the carryover effect is misspecified and provide a data-driven procedure to identify the order of the carryover effect. We conduct extensive simulations to study the numerical performance and empirical properties of our results, and conclude with practical suggestions.
[28]
arXiv:2304.11200
(replaced)
[pdf, html, other]
Title:
A Plug-and-Play Method with Inpainting Network for Bayesian Uncertainty Quantification in Imaging
Xiaoyu Wang, Michael Tang, Audrey Repetti
Subjects:
Methodology (stat.ME)
We contribute to an uncertainty quantification problem in imaging that evaluates a hypothesis test questioning the existence of local "artefacts" appearing in the maximum a posteriori (MAP) estimate (obtained from standard numerical tools). Such a method, called Bayesian uncertainty quantification by optimization (BUQO), was introduced a few years ago as an efficient and scalable alternative to sampling methods when per-pixel error-bars are not needed. BUQO formulates a hypothesis test for probing the existence of local structures in the MAP estimate as a minimization problem, that can be solved efficiently with standard optimization algorithms. In this context, BUQO requires a "mathematical" definition of the "local artefact". This definition can be interpreted as an inpainting of the structure. However, only simple hand-crafted techniques have been proposed so far due to the complexity of the problem. In this work, we propose a data-driven alternative to BUQO where the inpainting procedure in the algorithm is performed using a convolutional inpainting neural network (NN). This results in a plug-and-play algorithm, based on the primal-dual Condat-Vu iterations,where the inpainting procedure is performed with a NN. The proposed approach is assessed on two image reconstruction problems inspired by medicine. We specifically perform simulations on two Fourier undersampling problems (discrete and non-uniform) encountered in magnetic resonance imaging, as well as a computed tomography problem using the Radon measurement operator.
[29]
arXiv:2306.06932
(replaced)
[pdf, other]
Title:
Whittaker--Henderson smoothing revisited: A modern statistical framework for practical use
Guillaume Biessy (LPSM (UMR\_8001))
Comments:
ASTIN Bulletin, In press
Subjects:
Methodology (stat.ME)
Introduced over a century ago, Whittaker-Henderson smoothing remains widely used by actuaries in constructing one-dimensional and two-dimensional experience tables for mortality, disability and other life insurance risks. In this paper, we reinterpret this smoothing technique within a modern statistical framework and address six practically relevant questions about its use. First, we adopt a Bayesian perspective on this method to construct credible intervals. Second, in the context of survival analysis, we clarify how to choose the observation and weight vectors by linking the smoothing technique to a maximum likelihood estimator. Third, we improve accuracy by relaxing the method's reliance on an implicit normal approximation. Fourth, we select the smoothing parameters by maximizing a marginal likelihood function. Fifth, we improve computational efficiency when dealing with numerous observation points and consequently parameters. Finally, we develop an extrapolation procedure that ensures consistency between estimated and predicted values through constraints.
[30]
arXiv:2311.05649
(replaced)
[pdf, html, other]
Title:
Bayesian Image-on-Image Regression via Deep Kernel Learning based Gaussian Processes
Guoxuan Ma, Bangyao Zhao, Hasan Abu-Amara, Jian Kang
Subjects:
Applications (stat.AP); Methodology (stat.ME)
In neuroimaging studies, it becomes increasingly important to study associations between different imaging modalities using image-on-image regression (IIR), which faces challenges in interpretation, statistical inference, and prediction. Our motivating problem is how to predict task-evoked fMRI activity using resting-state fMRI data in the Human Connectome Project (HCP). The main difficulty lies in effectively combining different types of imaging predictors with varying resolutions and spatial domains in IIR. To address these issues, we develop Bayesian Image-on-image Regression via Deep Kernel Learning Gaussian Processes (BIRD-GP) and develop efficient posterior computation methods through Stein variational gradient descent. We demonstrate the advantages of BIRD-GP over state-of-the-art IIR methods using simulations. For HCP data analysis using BIRD-GP, we combine the voxel-wise fALFF maps and region-wise connectivity matrices to predict fMRI contrast maps for language and social recognition tasks. We show that fALFF is less predictive than the connectivity matrix for both tasks, but combining both yields improved results. Angular Gyrus Right emerges as the most predictable region for the language task (75.9% predictable voxels), while Superior Parietal Gyrus Right tops for the social recognition task (48.9% predictable voxels). Additionally, we identify features from the resting-state fMRI data that are important for task fMRI prediction.
[31]
arXiv:2312.01723
(replaced)
[pdf, html, other]
Title:
Group Sequential Design for Non-Proportional Hazards: Logrank, Weighted Logrank, and MaxCombo Methods
Yujie Zhao, Yilong Zhang, Larry Leon, Keaven M. Anderson
Subjects:
Methodology (stat.ME)
Non-proportional hazards (NPH) are often observed in clinical trials with time-to-event endpoints. A common example is a long-term clinical trial with a delayed treatment effect in immunotherapy for cancer. When designing clinical trials with time-to-event endpoints, it is crucial to consider NPH scenarios to gain a complete understanding of design operating characteristics. In this paper, we focus on group sequential design for three NPH methods: the average hazard ratio, the weighted logrank test, and the MaxCombo combination test. For each of these approaches, we provide analytic forms of design characteristics that facilitate sample size calculation and bound derivation for group sequential designs. Examples are provided to illustrate the proposed methods. To facilitate statisticians in designing and comparing group sequential designs under NPH, we have implemented the group sequential design methodology in the gsDesign2 R package at this https URL.
[32]
arXiv:2403.09928
(replaced)
[pdf, html, other]
Title:
Identification and estimation of mediational effects of longitudinal modified treatment policies
Brian Gilbert, Katherine L. Hoffman, Nicholas Williams, Kara E. Rudolph, Edward J. Schenck, Iván Díaz
Comments:
revised interpretation of estimand in application, additional simulation, minor changes throughout
Subjects:
Methodology (stat.ME)
We demonstrate a comprehensive semiparametric approach to causal mediation analysis, addressing the complexities inherent in settings with longitudinal and continuous treatments, confounders, and mediators. Our methodology utilizes a nonparametric structural equation model and a cross-fitted sequential regression technique based on doubly robust pseudo-outcomes, yielding an efficient, asymptotically normal estimator without relying on restrictive parametric modeling assumptions. We are motivated by a recent scientific controversy regarding the effects of invasive mechanical ventilation (IMV) on the survival of COVID-19 patients, considering acute kidney injury (AKI) as a mediating factor. We highlight the possibility of "inconsistent mediation," in which the direct and indirect effects of the exposure operate in opposite directions. We discuss the significance of mediation analysis for scientific understanding and its potential utility in treatment decisions.
[33]
arXiv:2405.17094
(replaced)
[pdf, html, other]
Title:
Dual Feature Reduction for the Sparse-group Lasso and its Adaptive Variant
Fabio Feser, Marina Evangelou
Comments:
32 pages, 20 figures, 20 tables
Journal-ref:
International Conference on Machine Learning 42 (2025)
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
The sparse-group lasso performs both variable and group selection, simultaneously using the strengths of the lasso and group lasso. It has found widespread use in genetics, a field that regularly involves the analysis of high-dimensional data, due to its sparse-group penalty, which allows it to utilize grouping information. However, the sparse-group lasso can be computationally expensive, due to the added shrinkage complexity, and its additional hyperparameter that needs tuning. This paper presents a novel feature reduction method, Dual Feature Reduction (DFR), that uses strong screening rules for the sparse-group lasso and the adaptive sparse-group lasso to reduce their input space before optimization, without affecting solution optimality. DFR applies two layers of screening through the application of dual norms and subdifferentials. Through synthetic and real data studies, it is shown that DFR drastically reduces the computational cost under many different scenarios.
[34]
arXiv:2406.19597
(replaced)
[pdf, html, other]
Title:
What's the Weight? Estimating Controlled Outcome Differences in Complex Surveys for Health Disparities Research
Stephen Salerno, Emily K. Roberts, Belinda L. Needham, Tyler H. McCormick, Fan Li, Bhramar Mukherjee, Xu Shi
Subjects:
Methodology (stat.ME)
In this work, we are motivated by the problem of estimating racial disparities in health outcomes, specifically the average controlled difference (ACD) in telomere length between Black and White individuals, using data from the National Health and Nutrition Examination Survey (NHANES). To do so, we build a propensity for race to properly adjust for other social determinants while characterizing the controlled effect of race on telomere length. Propensity score methods are broadly employed with observational data as a tool to achieve covariate balance, but how to implement them in complex surveys is less studied - in particular, when the survey weights depend on the group variable under comparison (as the NHANES sampling scheme depends on self-reported race). We propose identification formulas to properly estimate the ACD in outcomes between Black and White individuals, with appropriate weighting for both covariate imbalance across the two racial groups and generalizability. Via extensive simulation, we show that our proposed methods outperform traditional analytic approaches in terms of bias, mean squared error, and coverage when estimating the ACD for our setting of interest. In our data, we find that evidence of racial differences in telomere length between Black and White individuals attenuates after accounting for confounding by socioeconomic factors and utilizing appropriate propensity score and survey weighting techniques. Software to implement these methods and code to reproduce our results can be found in the R package svycdiff, available through the Comprehensive R Archive Network (CRAN) at this http URL, or in a development version on GitHub at this http URL.
[35]
arXiv:2410.02727
(replaced)
[pdf, html, other]
Title:
Regression Discontinuity Designs Under Interference
Elena Dal Torrione, Tiziano Arduini, Laura Forastiere
Subjects:
Methodology (stat.ME)
We extend the continuity-based framework to Regression Discontinuity Designs (RDDs) to identify and estimate causal effects in the presence of interference when units are connected through a network. In this setting, assignment to an "effective treatment," which comprises the individual treatment and a summary of the treatment of interfering units (e.g., friends, classmates), is determined by the unit's score and the scores of other interfering units, leading to a multiscore RDD with potentially complex, multidimensional boundaries. We characterize these boundaries and derive generalized continuity assumptions to identify the proposed causal estimands, i.e., point and boundary causal effects. Additionally, we develop a distance-based nonparametric estimator, derive its asymptotic properties under restrictions on the network degree distribution, and introduce a novel variance estimator that accounts for network correlation. Finally, we apply our methodology to the PROGRESA/Oportunidades dataset to estimate the direct and indirect effects of receiving cash transfers on children's school attendance.
[36]
arXiv:2410.21419
(replaced)
[pdf, html, other]
Title:
High-Dimensional Gaussian Process Regression with Soft Kernel Interpolation
Chris Camaño, Daniel Huang
Comments:
12 pages, 6 Figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
We introduce Soft Kernel Interpolation (SoftKI), a method that combines aspects of Structured Kernel Interpolation (SKI) and variational inducing point methods, to achieve scalable Gaussian Process (GP) regression on high-dimensional datasets. SoftKI approximates a kernel via softmax interpolation from a smaller number of interpolation points learned by optimizing a combination of the SoftKI marginal log-likelihood (MLL), and when needed, an approximate MLL for improved numerical stability. Consequently, it can overcome the dimensionality scaling challenges that SKI faces when interpolating from a dense and static lattice while retaining the flexibility of variational methods to adapt inducing points to the dataset. We demonstrate the effectiveness of SoftKI across various examples and show that it is competitive with other approximated GP methods when the data dimensionality is modest (around 10).
[37]
arXiv:2502.04118
(replaced)
[pdf, html, other]
Title:
Maximum Likelihood Estimation of the Parameters of Matrix Variate Symmetric Laplace Distribution
Pooja Yadav, Tanuja Srivastava
Subjects:
Statistics Theory (math.ST)
This paper considers an extension of the multivariate symmetric Laplace distribution to matrix variate case. The symmetric Laplace distribution is a scale mixture of normal distribution. The maximum likelihood estimators (MLE) of the parameters of multivariate and matrix variate symmetric Laplace distribution are proposed, which are not explicitly obtainable, as the density function involves the modified Bessel function of the third kind. Thus, the EM algorithm is applied to find the maximum likelihood estimators. The parameters and their maximum likelihood estimators of matrix variate symmetric Laplace distribution are defined up to a positive multiplicative constant with their Kronecker product uniquely defined. The condition for the existence of the MLE is given, and the stability of the estimators is discussed. The empirical bias and the dispersion of the Kronecker product of the estimators for different sample sizes are discussed using simulated data.
[38]
arXiv:2502.16831
(replaced)
[pdf, html, other]
Title:
Minimum Copula Divergence for Robust Estimation
Shinto Eguchi, Shogo Kato
Subjects:
Methodology (stat.ME)
This paper introduces a robust estimation framework based solely on the copula function. We begin by introducing a family of divergence measures tailored for copulas, including the \(\alpha\)-, \(\beta\)-, and \(\gamma\)-copula divergences, which quantify the discrepancy between a parametric copula model and an empirical copula derived from data independently of marginal specifications. Using these divergence measures, we propose the minimum copula divergence estimator (MCDE), an estimation method that minimizes the divergence between the model and the empirical copula. The framework proves particularly effective in addressing model misspecifications and analyzing heavy-tailed data, where traditional methods such as the maximum likelihood estimator (MLE) may fail. Theoretical results show that common copula families, including Archimedean and elliptical copulas, satisfy conditions ensuring the boundedness of divergence-based estimators, thereby guaranteeing the robustness of MCDE, especially in the presence of extreme observations. Numerical examples further underscore MCDE's ability to adapt to varying dependence structures, ensuring its utility in real-world scenarios.
[39]
arXiv:2503.15186
(replaced)
[pdf, html, other]
Title:
Optimal Data Splitting for Holdout Cross-Validation in Large Covariance Matrix Estimation
Lamia Lamrani, Christian Bongiorno, Marc Potters
Subjects:
Statistics Theory (math.ST); Portfolio Management (q-fin.PM); Risk Management (q-fin.RM); Applications (stat.AP)
Cross-validation is a statistical tool that can be used to improve large covariance matrix estimation. Although its efficiency is observed in practical applications and a convergence result towards the error of the non linear shrinkage is available in the high-dimensional regime, formal proofs that take into account the finite sample size effects are currently lacking. To carry on analytical analysis, we focus on the holdout method, a single iteration of cross-validation, rather than the traditional $k$-fold approach. We derive a closed-form expression for the expected estimation error when the population matrix follows a white inverse Wishart distribution, and we observe the optimal train-test split scales as the square root of the matrix dimension. For general population matrices, we connected the error to the variance of eigenvalues distribution, but approximations are necessary. In this framework and in the high-dimensional asymptotic regime, both the holdout and $k$-fold cross-validation methods converge to the optimal estimator when the train-test ratio scales with the square root of the matrix dimension which is coherent with the existing theory.
[40]
arXiv:2504.13520
(replaced)
[pdf, html, other]
Title:
Bayesian Model Averaging in Causal Instrumental Variable Models
Gregor Steiner, Mark Steel
Subjects:
Methodology (stat.ME); Econometrics (econ.EM); Statistics Theory (math.ST)
Instrumental variables are a popular tool to infer causal effects under unobserved confounding, but choosing suitable instruments is challenging in practice. We propose gIVBMA, a Bayesian model averaging procedure that addresses this challenge by averaging across different sets of instrumental variables and covariates in a structural equation model. Our approach extends previous work through a scale-invariant prior structure and accommodates non-Gaussian outcomes and treatments, offering greater flexibility than existing methods. The computational strategy uses conditional Bayes factors to update models separately for the outcome and treatments. We prove that this model selection procedure is consistent. By explicitly accounting for model uncertainty, gIVBMA allows instruments and covariates to switch roles and provides robustness against invalid instruments. In simulation experiments, gIVBMA outperforms current state-of-the-art methods. We demonstrate its usefulness in two empirical applications: the effects of malaria and institutions on income per capita and the returns to schooling. A software implementation of gIVBMA is available in Julia.
[41]
arXiv:2505.00283
(replaced)
[pdf, html, other]
Title:
High Dimensional Ensemble Kalman Filter
Shouxia Wang, Hao-Xuan Sun, Song Xi Chen
Subjects:
Methodology (stat.ME)
The Ensemble Kalman Filter (EnKF), as a fundamental data assimilation approach, has been widely used in many fields of the sciences and engineering. When the state variable is of high dimensional accompanied with high resolution observations of physical models, some key theoretical aspects of the EnKF are open for investigation. This paper proposes several high dimensional EnKF (HD-EnKF) methods equipped with consistent estimators for the important forecast error covariance and Kalman Gain matrices. It then studies the theoretical properties of the EnKF under both fixed and high dimensional state variables, which provides one-step and multiple-step mean square errors of the analysis states to the underlying oracle states offered by the Kalman Filter and gives the much needed insight to the roles played by the forecast error covariance on the accuracy of the EnKF. The accuracy of the data assimilation under the misspecified physical model is also considered. Numerical studies on the Lorenz-96 and the Shallow Water Equation models illustrate that the proposed HD-EnKF algorithms outperform the standard EnKF and widely used inflation methods.
[42]
arXiv:2505.08698
(replaced)
[pdf, html, other]
Title:
Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data
Antonio Álvarez-López, Marcos Matabuena
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Dynamical Systems (math.DS); Applications (stat.AP); Methodology (stat.ME)
Modeling the dynamics of probability distributions from time-dependent data samples is a fundamental problem in many fields, including digital health. The goal is to analyze how the distribution of a biomarker, such as glucose, changes over time and how these changes may reflect the progression of chronic diseases such as diabetes. We introduce a probabilistic model based on a Gaussian mixture that captures the evolution of a continuous-time stochastic process. Our approach combines a nonparametric estimate of the distribution, obtained with Maximum Mean Discrepancy (MMD), and a Neural Ordinary Differential Equation (Neural ODE) that governs the temporal evolution of the mixture weights. The model is highly interpretable, detects subtle distribution shifts, and remains computationally efficient. We illustrate the broad utility of our approach in a 26-week clinical trial that treats all continuous glucose monitoring (CGM) time series as the primary outcome. This method enables rigorous longitudinal comparisons between the treatment and control arms and yields characterizations that conventional summary-based clinical trials analytical methods typically do not capture.
[43]
arXiv:2506.20523
(replaced)
[pdf, html, other]
Title:
Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment and Balanced Power
Daniel Molitor, Samantha Gold
Comments:
14 pages, 5 figures
Subjects:
Methodology (stat.ME); Econometrics (econ.EM); Computation (stat.CO)
Adaptive experiments such as multi-armed bandits offer efficiency gains over traditional randomized experiments but pose two major challenges: invalid inference on the Average Treatment Effect (ATE) due to adaptive sampling and low statistical power for sub-optimal treatments. We address both issues by extending the Mixture Adaptive Design framework (arXiv:2311.05794). First, we propose MADCovar, a covariate-adjusted ATE estimator that is unbiased and preserves anytime-valid inference guarantees while substantially improving ATE precision. Second, we introduce MADMod, which dynamically reallocates samples to underpowered arms, enabling more balanced statistical power across treatments without sacrificing valid inference. Both methods retain MAD's core advantage of constructing asymptotic confidence sequences (CSs) that allow researchers to continuously monitor ATE estimates and stop data collection once a desired precision or significance criterion is met. Empirically, we validate both methods using simulations and real-world data. In simulations, MADCovar reduces CS width by up to $60\%$ relative to MAD. In a large-scale political RCT with $\approx32,000$ participants, MADCovar achieves similar precision gains. MADMod improves statistical power and inferential precision across all treatment arms, particularly for suboptimal treatments. Simulations show that MADMod sharply reduces Type II error while preserving the efficiency benefits of adaptive allocation. Together, MADCovar and MADMod make adaptive experiments more practical, reliable, and efficient for applied researchers across many domains. Our proposed methods are implemented through an open-source software package.
[44]
arXiv:2507.02884
(replaced)
[pdf, html, other]
Title:
Random time-shift approximation enables hierarchical Bayesian inference of mechanistic within-host viral dynamics models on large datasets
Dylan J. Morris, Lauren Kennedy, Andrew J. Black
Comments:
37 pages, 9 figures
Subjects:
Applications (stat.AP); Probability (math.PR)
Mechanistic mathematical models of within-host viral dynamics are tools for understanding how a virus' biology and its interaction with the immune system shape the infectivity of a host. The biology of the process is encoded by the structure and parameters of the model that can be inferred statistically by fitting to viral load data. The main drawback of mechanistic models is that this inference is computationally expensive because the model must be repeatedly solved. This limits the size of the datasets that can be considered or the complexity of the models fitted. In this paper we develop a much cheaper inference method by implementing a novel approximation of the model dynamics that uses a combination of random and deterministic processes. This approximation also properly accounts for process noise early in the infection when cell and virion numbers are small, which is important for the viral dynamics but often overlooked. Our method runs on a consumer laptop and is fast enough to facilitate a full hierarchical Bayesian treatment of the problem with sharing of information to allow for individual level parameter differences. We apply our method to simulated datasets and a reanalysis of COVID-19 monitoring data in an National Basketball Association cohort of 163 individuals.
[45]
arXiv:2509.05775
(replaced)
[pdf, html, other]
Title:
Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery
Zilong Wang, Turgay Ayer, Shihao Yang
Comments:
Pre-print for camera ready version for IEEE EMBS BHI 2025. This work has been submitted to the IEEE for possible publication
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG)
Estimating heterogeneous treatment effects is critical in domains such as personalized medicine, resource allocation, and policy evaluation. A central challenge lies in identifying subpopulations that respond differently to interventions, thereby enabling more targeted and effective decision-making. While clustering methods are well-studied in unsupervised learning, their integration with causal inference remains limited. We propose a novel framework that clusters individuals based on estimated treatment effects using a learned kernel derived from causal forests, revealing latent subgroup structures. Our approach consists of two main steps. First, we estimate debiased Conditional Average Treatment Effects (CATEs) using orthogonalized learners via the Robinson decomposition, yielding a kernel matrix that encodes sample-level similarities in treatment responsiveness. Second, we apply kernelized clustering to this matrix to uncover distinct, treatment-sensitive subpopulations and compute cluster-level average CATEs. We present this kernelized clustering step as a form of regularization within the residual-on-residual regression framework. Through extensive experiments on semi-synthetic and real-world datasets, supported by ablation studies and exploratory analyses, we demonstrate the effectiveness of our method in capturing meaningful treatment effect heterogeneity.
[46]
arXiv:2509.11070
(replaced)
[pdf, html, other]
Title:
A Kernel-based Stochastic Approximation Framework for Nonlinear Operator Learning
Jia-Qi Yang, Lei Shi
Comments:
34 pages, 3 figures
Subjects:
Machine Learning (stat.ML); Machine Learning (cs.LG); Functional Analysis (math.FA); Numerical Analysis (math.NA); Statistics Theory (math.ST)
We develop a stochastic approximation framework for learning nonlinear operators between infinite-dimensional spaces utilizing general Mercer operator-valued kernels. Our framework encompasses two key classes: (i) compact kernels, which admit discrete spectral decompositions, and (ii) diagonal kernels of the form $K(x,x')=k(x,x')T$, where $k$ is a scalar-valued kernel and $T$ is a positive operator on the output space. This broad setting induces expressive vector-valued reproducing kernel Hilbert spaces (RKHSs) that generalize the classical $K=kI$ paradigm, thereby enabling rich structural modeling with rigorous theoretical guarantees. To address target operators lying outside the RKHS, we introduce vector-valued interpolation spaces to precisely quantify misspecification error. Within this framework, we establish dimension-free polynomial convergence rates, demonstrating that nonlinear operator learning can overcome the curse of dimensionality. The use of general operator-valued kernels further allows us to derive rates for intrinsically nonlinear operator learning, going beyond the linear-type behavior inherent in diagonal constructions of $K=kI$. Importantly, this framework accommodates a wide range of operator learning tasks, ranging from integral operators such as Fredholm operators to architectures based on encoder-decoder representations. Moreover, we validate its effectiveness through numerical experiments on the two-dimensional Navier-Stokes equations.
[47]
arXiv:1911.01067
(replaced)
[pdf, html, other]
Title:
Blind Network Revenue Management and Bandits with Knapsacks under Limited Switches
David Simchi-Levi, Yunzong Xu, Jinglong Zhao
Subjects:
Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC); Machine Learning (stat.ML)
This paper studies the impact of limited switches on resource-constrained dynamic pricing with demand learning. We focus on the classical price-based blind network revenue management problem and extend our results to the bandits with knapsacks problem. In both settings, a decision maker faces stochastic and distributionally unknown demand, and must allocate finite initial inventory across multiple resources over time. In addition to standard resource constraints, we impose a switching constraint that limits the number of action changes over the time horizon. We establish matching upper and lower bounds on the optimal regret and develop computationally efficient limited-switch algorithms that achieve it. We show that the optimal regret rate is fully characterized by a piecewise-constant function of the switching budget, which further depends on the number of resource constraints. Our results highlight the fundamental role of resource constraints in shaping the statistical complexity of online learning under limited switches. Extensive simulations demonstrate that our algorithms maintain strong cumulative reward performance while significantly reducing the number of switches.
[48]
arXiv:2406.12945
(replaced)
[pdf, other]
Title:
Tabular Data Generation Models: An In-Depth Survey and Performance Benchmarks with Extensive Tuning
G. Charbel N. Kindji (LACODAM), Lina Maria Rojas-Barahona, Elisa Fromont (LACODAM), Tanguy Urvoy
Subjects:
Machine Learning (cs.LG); Machine Learning (stat.ML)
The ability to train generative models that produce realistic, safe and useful tabular data is essential for data privacy, imputation, oversampling, explainability or simulation. However, generating tabular data is not straightforward due to its heterogeneity, non-smooth distributions, complex dependencies and imbalanced categorical features. Although diverse methods have been proposed in the literature, there is a need for a unified evaluation, under the same conditions, on a variety of datasets. This study addresses this need by fully considering the optimization of: hyperparameters, feature encodings, and architectures. We investigate the impact of dataset-specific tuning on five recent model families for tabular data generation through an extensive benchmark on 16 datasets. These datasets vary in terms of size (an average of 80,000 rows), data types, and domains. We also propose a reduced search space for each model that allows for quick optimization, achieving nearly equivalent performance at a significantly lower cost. Our benchmark demonstrates that, for most models, large-scale dataset-specific tuning substantially improves performance compared to the original configurations. Furthermore, we confirm that diffusion-based models generally outperform other models on tabular data. However, this advantage is not significant when the entire tuning and training process is restricted to the same GPU budget.
[49]
arXiv:2407.01613
(replaced)
[pdf, html, other]
Title:
Self-adaptive weights based on balanced residual decay rate for physics-informed neural networks and deep operator networks
Wenqian Chen, Amanda A. Howard, Panos Stinis
Comments:
13 figures, 4 tables
Journal-ref:
J. Comput. Phys., 542 (2025) 114226
Subjects:
Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Physics-informed deep learning has emerged as a promising alternative for solving partial differential equations. However, for complex problems, training these networks can still be challenging, often resulting in unsatisfactory accuracy and efficiency. In this work, we demonstrate that the failure of plain physics-informed neural networks arises from the significant discrepancy in the convergence rate of residuals at different training points, where the slowest convergence rate dominates the overall solution convergence. Based on these observations, we propose a pointwise adaptive weighting method that balances the residual decay rate across different training points. The performance of our proposed adaptive weighting method is compared with current state-of-the-art adaptive weighting methods on benchmark problems for both physics-informed neural networks and physics-informed deep operator networks. Through extensive numerical results we demonstrate that our proposed approach of balanced residual decay rates offers several advantages, including bounded weights, high prediction accuracy, fast convergence rate, low training uncertainty, low computational cost, and ease of hyperparameter tuning.
[50]
arXiv:2411.19572
(replaced)
[pdf, html, other]
Title:
Canonical correlation analysis of stochastic trends via functional approximation
Massimo Franchi, Iliyan Georgiev, Paolo Paruolo
Subjects:
Econometrics (econ.EM); Methodology (stat.ME)
This paper proposes a novel approach for semiparametric inference on the number $s$ of common trends and their loading matrix $\psi$ in $I(1)/I(0)$ systems. It combines functional approximation of limits of random walks and canonical correlations analysis, performed between the $p$ observed time series of length $T$ and the first $K$ discretized elements of an $L^2$ basis. Tests and selection criteria on $s$, and estimators and tests on $\psi$ are proposed; their properties are discussed as $T$ and $K$ diverge sequentially for fixed $p$ and $s$. It is found that tests on $s$ are asymptotically pivotal, selection criteria of $s$ are consistent, estimators of $\psi$ are $T$-consistent, mixed-Gaussian and efficient, so that Wald tests on $\psi$ are asymptotically Normal or $\chi^2$. The paper also discusses asymptotically pivotal misspecification tests for checking model assumptions. The approach can be coherently applied to subsets or aggregations of variables in a given panel. Monte Carlo simulations show that these tools have reasonable performance for $T\geq 10 p$ and $p\leq 300$. An empirical analysis of 20 exchange rates illustrates the methods.
[51]
arXiv:2505.02308
(replaced)
[pdf, html, other]
Title:
Enabling Local Neural Operators to perform Equation-Free System-Level Analysis
Gianluca Fabiani, Hannes Vandecasteele, Somdatta Goswami, Constantinos Siettos, Ioannis G. Kevrekidis
Comments:
35 pages, 13 figures
Subjects:
Machine Learning (cs.LG); Dynamical Systems (math.DS); Numerical Analysis (math.NA); Machine Learning (stat.ML)
Neural Operators (NOs) provide a powerful framework for computations involving physical laws that can be modelled by (integro-) partial differential equations (PDEs), directly learning maps between infinite-dimensional function spaces that bypass both the explicit equation identification and their subsequent numerical solving. Still, NOs have so far primarily been employed to explore the dynamical behavior as surrogates of brute-force temporal simulations/predictions. Their potential for systematic rigorous numerical system-level tasks, such as fixed-point, stability, and bifurcation analysis - crucial for predicting irreversible transitions in real-world phenomena - remains largely unexplored. Toward this aim, inspired by the Equation-Free multiscale framework, we propose and implement a framework that integrates (local) NOs with advanced iterative numerical methods in the Krylov subspace, so as to perform efficient system-level stability and bifurcation analysis of large-scale dynamical systems. Beyond fixed point, stability, and bifurcation analysis enabled by local in time NOs, we also demonstrate the usefulness of local in space as well as in space-time ("patch") NOs in accelerating the computer-aided analysis of spatiotemporal dynamics. We illustrate our framework via three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes multiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE, which features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN) model, consisting of two coupled PDEs that exhibit both Hopf and saddle-node bifurcations.
[52]
arXiv:2507.17582
(replaced)
[pdf, html, other]
Title:
Physics-informed, boundary-constrained Gaussian process regression for the reconstruction of fluid flow fields
Adrian Padilla-Segarra, Pascal Noble, Olivier Roustant, Éric Savin
Comments:
22 pages, 11 figures
Subjects:
Fluid Dynamics (physics.flu-dyn); Machine Learning (stat.ML)
Gaussian process regression techniques have been used in fluid mechanics for the reconstruction of flow fields from a reduction-of-dimension perspective. A main ingredient in this setting is the construction of adapted covariance functions, or kernels, to obtain such estimates. In this paper, we present a general method for constraining a prescribed Gaussian process on an arbitrary compact set. The kernel of the pre-defined process must be at least continuous and may include other information about the studied phenomenon. This general boundary-constraining framework can be implemented with high flexibility for a broad range of engineering applications. From this, we derive physics-informed kernels for simulating two-dimensional velocity fields of an incompressible (divergence-free) flow around aerodynamic profiles. These kernels allow to define Gaussian process priors satisfying the incompressibility condition and the prescribed boundary conditions along the profile in a continuous manner. We describe an adapted numerical method for the boundary-constraining procedure parameterized by a measure on the compact set. The relevance of the methodology and performances are illustrated by numerical simulations of flows around a cylinder and a NACA 0412 airfoil profile, for which no observation at the boundary is needed at all.
[53]
arXiv:2509.02996
(replaced)
[pdf, html, other]
Title:
Group-averaged Markov chains: mixing improvement
Michael C.H. Choi, Youjia Wang
Comments:
68 pages
Subjects:
Probability (math.PR); Information Theory (cs.IT); Group Theory (math.GR); Computation (stat.CO)
For Markov kernels $P$ on a general state space $\mathcal{X}$, we introduce a new class of averaged Markov kernels $P_{da}(G,\nu)$ of $P$ induced by a group $G$ that acts on $\mathcal{X}$ and a probability measure $\nu$ on $G \times G$. Notable special cases are the group-orbit average $\overline{P}$, left-average $P_{la}$, right-average $P_{ra}$ and the independent-double-average $(P_{la})_{ra}$. For $\pi$-stationary $P$ in which $\pi$ is invariant with respect to $G$, we show that in general $P_{da}$ enjoys favorable convergence properties than $P$ based on metrics such as spectral gap or asymptotic variance, and within the family of $P_{da}$ the most preferable kernel is in general $(P_{la})_{ra}$. We demonstrate that $P_{la}, P_{ra}, (P_{la})_{ra}$ are comparable in terms of mixing times, which supports the use of $P_{la}, P_{ra}$ in practice as computationally cheaper alternatives over $(P_{la})_{ra}$. These averaged kernels also admit natural geometric interpretations: they emerge as unique projections of $P$ onto specific $G$-invariant structures under the Kullback-Leibler divergence or the Hilbert-Schmidt norm and satisfy Pythagorean identities. On the other hand, in the general case if $\pi$ is not invariant with respect to $G$, we propose and study a technique that we call state-dependent averaging of Markov kernels which generalizes the earlier results to this setting. As examples and applications, this averaging perspective not only allows us to recast state-of-the-art Markov chain samplers such as Hamiltonian Monte Carlo or piecewise-deterministic Markov processes as specific cases of $P_{da}$, but also enables improvements to existing samplers such as Metropolis-Hastings, achieving rapid mixing in some toy models or when $\pi$ is the discrete uniform distribution.
[54]
arXiv:2509.08560
(replaced)
[pdf, html, other]
Title:
A transport approach to the cutoff phenomenon
Francesco Pedrotti, Justin Salez
Comments:
Typos fixed
Subjects:
Probability (math.PR); Analysis of PDEs (math.AP); Statistics Theory (math.ST); Machine Learning (stat.ML)
Substantial progress has recently been made in the understanding of the cutoff phenomenon for Markov processes, using an information-theoretic statistics known as varentropy [Sal23; Sal24; Sal25a; PS25]. In the present paper, we propose an alternative approach which bypasses the use of varentropy and exploits instead a new W-TV transport inequality, combined with a classical parabolic regularization estimate [BGL01; OV01]. While currently restricted to non-negatively curved processes on smooth spaces, our argument no longer requires the chain rule, nor any approximate version thereof. As applications, we recover the main result of [Sal25a] establishing cutoff for the log-concave Langevin dynamics, and extend the conclusion to a widely-used discrete-time sampling algorithm known as the Proximal Sampler.
Total of 54 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack