Quantitative Finance
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
q-fin
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Quantitative Finance
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Thursday, 18 September 2025
Total of 19 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 9 of 9 entries)
[1]
arXiv:2509.13374
[pdf, html, other]
Title:
Valuation of Exotic Options and Counterparty Games Based on Conditional Diffusion
Helin Zhao, Junchi Shen
Comments:
28 pages, 12 figures
Subjects:
Pricing of Securities (q-fin.PR); Machine Learning (cs.LG); Risk Management (q-fin.RM)
This paper addresses the challenges of pricing exotic options and structured products, which traditional models often fail to handle due to their inability to capture real-world market phenomena like fat-tailed distributions and volatility clustering. We introduce a Diffusion-Conditional Probability Model (DDPM) to generate more realistic price paths. Our method incorporates a composite loss function with financial-specific features, and we propose a P-Q dynamic game framework for evaluating the model's economic value through adversarial backtesting. Static validation shows our P-model effectively matches market mean and volatility. In dynamic games, it demonstrates significantly higher profitability than a traditional Monte Carlo-based model for European and Asian options. However, the model shows limitations in pricing products highly sensitive to extreme events, such as snowballs and accumulators, because it tends to underestimate tail risks. The study concludes that diffusion models hold significant potential for enhancing pricing accuracy, though further research is needed to improve their ability to model extreme market risks.
[2]
arXiv:2509.13578
[pdf, html, other]
Title:
In-between Transatlantic (Monetary) Disturbances
Santiago Camara, Jeanne Aublin
Subjects:
General Economics (econ.GN)
This paper studies the spillovers of European Central Bank (ECB) interest rate shocks into the Canadian economy and compares them with those of the U.S. Federal Reserve (Fed). We combine a VAR model and local projection regressions with identification strategies that explicitly purge information effects around policy announcements. We find that an ECB rate hike leads to a depreciation of the Canadian dollar and a sharp contraction in economic activity. The main transmission channel is international trade: ECB shocks trigger a decline in oil prices and exports, while leaving domestic financial conditions largely unaffected. By contrast, Fed shocks tighten Canadian financial conditions significantly, with more limited effects on trade flows. These findings show that Canada is exposed to foreign monetary policy both directly and indirectly, through its integration in global financial and trade markets.
[3]
arXiv:2509.13623
[pdf, html, other]
Title:
Deep Learning in the Sequence Space
Marlon Azinovic-Yang, Jan Žemlička
Subjects:
General Economics (econ.GN)
We develop a deep learning algorithm for approximating functional rational expectations equilibria of dynamic stochastic economies in the sequence space. We use deep neural networks to parameterize equilibrium objects of the economy as a function of truncated histories of exogenous shocks. We train the neural networks to fulfill all equilibrium conditions along simulated paths of the economy. To illustrate the performance of our method, we solve three economies of increasing complexity: the stochastic growth model, a high-dimensional overlapping generations economy with multiple sources of aggregate risk, and finally an economy where households and firms face uninsurable idiosyncratic risk, shocks to aggregate productivity, and shocks to idiosyncratic and aggregate volatility. Furthermore, we show how to design practical neural policy function architectures that guarantee monotonicity of the predicted policies, facilitating the use of the endogenous grid method to simplify parts of our algorithm.
[4]
arXiv:2509.13887
[pdf, html, other]
Title:
Can the decoy effect increase cooperation in networks? An experiment
Claudia Cerrone, Francesco Feri, Anita Gantner, Paolo Pin
Subjects:
General Economics (econ.GN)
This paper investigates whether the decoy effect - specifically the attraction effect - can foster cooperation in social networks. In a lab experiment, we show that introducing a dominated option increases the selection of the target choice, especially in early decisions. The effect is stronger in individual settings but persists in networks despite free-riding incentives, with variation depending on the decision-maker's strategic position.
[5]
arXiv:2509.13923
[pdf, other]
Title:
Holdout cross-validation for large non-Gaussian covariance matrix estimation using Weingarten calculus
Lamia Lamrani, Benoît Collins, Jean-Philippe Bouchaud
Subjects:
Statistical Finance (q-fin.ST); Statistics Theory (math.ST); Risk Management (q-fin.RM); Machine Learning (stat.ML)
Cross-validation is one of the most widely used methods for model selection and evaluation; its efficiency for large covariance matrix estimation appears robust in practice, but little is known about the theoretical behavior of its error. In this paper, we derive the expected Frobenius error of the holdout method, a particular cross-validation procedure that involves a single train and test split, for a generic rotationally invariant multiplicative noise model, therefore extending previous results to non-Gaussian data distributions. Our approach involves using the Weingarten calculus and the Ledoit-Péché formula to derive the oracle eigenvalues in the high-dimensional limit. When the population covariance matrix follows an inverse Wishart distribution, we approximate the expected holdout error, first with a linear shrinkage, then with a quadratic shrinkage to approximate the oracle eigenvalues. Under the linear approximation, we find that the optimal train-test split ratio is proportional to the square root of the matrix dimension. Then we compute Monte Carlo simulations of the holdout error for different distributions of the norm of the noise, such as the Gaussian, Student, and Laplace distributions and observe that the quadratic approximation yields a substantial improvement, especially around the optimal train-test split ratio. We also observe that a higher fourth-order moment of the Euclidean norm of the noise vector sharpens the holdout error curve near the optimal split and lowers the ideal train-test ratio, making the choice of the train-test ratio more important when performing the holdout method.
[6]
arXiv:2509.14057
[pdf, html, other]
Title:
Machines are more productive than humans until they aren't, and vice versa
Riccardo Zanardelli
Subjects:
General Economics (econ.GN); Artificial Intelligence (cs.AI)
With the growth of artificial skills, organizations may increasingly confront with the problem of optimizing skill policy decisions guided by economic principles. This paper addresses the underlying complexity of this challenge by developing an in-silico framework based on Monte Carlo simulations grounded in empirical realism to analyze the economic impact of human and machine skills, individually or jointly deployed, in the execution of tasks presenting varying levels of complexity. Our results provide quantitative support for the established notions that automation tends to be the most economically-effective strategy for tasks characterized by low-to-medium generalization difficulty, while automation struggles to match the economic utility of human skills in more complex scenarios. Critically, our simulations highlight that combining human and machine skills can be the most effective strategy when a high level of generalization is required, but only if genuine augmentation is achieved. In contrast, when failing to realize this synergy, the human-machine policy is severely penalized by the inherent costs of its dual skill structure, causing it to destroy value and becoming the worst choice from an economic perspective. The takeaway for decision-makers is unambiguous: simply allocating human and machine skills to a task is insufficient, and a human-machine skill policy is neither a silver-bullet solution nor a low-risk compromise. Rather, it is a critical opportunity to boost competitiveness that demands a strong organizational commitment to enabling augmentation. Also, our findings show that improving the cost-effectiveness of machine skills over time, while useful, does not replace the fundamental need to focus on achieving augmentation.
[7]
arXiv:2509.14080
[pdf, html, other]
Title:
Dynamic Inverse Optimization under Drift and Shocks: Theory, Regret Bounds, and Applications
JINHO CHA (Gwinnett Technical College)
Comments:
36 pages, 8 figures, submitted to journal of Computational Management Science in Sep. 13. 2025
Subjects:
Computational Finance (q-fin.CP)
The growing prevalence of drift and shocks in modern decision environments exposes a gap between classical optimization theory and real-world practice. Standard models assume fixed objectives, yet organizations from hospitals to power grids routinely adapt to shifting priorities, noisy data, and abrupt disruptions. To address this gap, this study develops a dynamic inverse optimization framework that recovers hidden, time-varying preferences from observed allocation trajectories. The framework unifies identifiability analysis with regret guarantees conditions are established for existence and uniqueness of recovered parameters, and sharp static and dynamic regret bounds are derived to characterize responsiveness to gradual drift and sudden shocks. Methodologically, a drift-aware estimator grounded in convex analysis and online learning theory is introduced, with finite-sample guarantees on recovery accuracy. Computational experiments in healthcare, energy, logistics, and finance reveal heterogeneous recovery patterns, ranging from rapid resilience to persistent vulnerability. Overall, dynamic inverse optimization emerges as both a theoretical contribution and a broadly applicable diagnostic tool for benchmarking resilience, uncovering hidden behavioral shifts, and guiding policy interventions in complex stochastic systems.
[8]
arXiv:2509.14102
[pdf, html, other]
Title:
Incentivizing High Quality Entrants When Creators Are Strategic
Felicia Nguyen
Subjects:
General Economics (econ.GN)
We study how a platform should design early exposure and rewards when creators strategically choose quality before release. A short testing window with a pass/fail bar induces a pass probability, the slope of which is the key sufficient statistic for incentives. We derive three main results. First, a closed-form ``implementability bounty'' can perfectly align creator and platform objectives, correcting for incomplete revenue sharing. Second, front-loading guaranteed impressions is the most effective way to strengthen incentives for a given attention budget. Third, when impression and cash budgets are constrained, the optimal policy follows an equal-marginal-value rule based on the prize spread and certain exposure. We map realistic ranking engines (e.g., Thompson sampling) into the model's parameters and provide telemetry-based estimators. The framework is simple to operationalize and offers a direct, managerially interpretable solution for platforms to solve the creator cold-start problem and cultivate high-quality supply.
[9]
arXiv:2509.14116
[pdf, html, other]
Title:
Minimum pricing or volumetric taxation? Quantity, quality and competition effects of price regulations in alcohol markets
Celine Bonnet, Fabrice Etile, Sebastien Lecocq
Comments:
Main Text: 52 pages; 11 Tables
Subjects:
General Economics (econ.GN)
Reforming alcohol price regulations in wine-producing countries is challenging, as current price regulations reflect the alignment of cultural preferences with economic interests rather than public health concerns. We evaluate and compare the impact of counterfactual alcohol pricing policies on consumer behaviors, firms, and markets in France. We develop a micro-founded partial equilibrium model that accounts for consumer preferences over purchase volumes across alcohol categories and over product quality within categories, and for firms' strategic price-setting. After calibration on household scanner data, we compare the impacts of replacing current taxes by ethanol-based volumetric taxes with a minimum unit price (MUP) policy of 0.50 Euro per standard drink. The results show that the MUP in addition to the current tax outperforms a tax reform in reducing ethanol purchases (-15% vs. -10% for progressive taxation), especially among heavy drinking households (-17%). The MUP increases the profits of small and medium wine firms (+39%) while decreasing the profits of large manufacturers and retailers (-39%) and maintaining tax revenues stable. The results support the MUP as a targeted strategy to reduce harmful consumption while benefiting small and medium wine producers. This study provides ex-ante evidence that is crucial for alcohol pricing policies in wine-producing countries.
Cross submissions (showing 1 of 1 entries)
[10]
arXiv:2509.13323
(cross-list from cs.HC)
[pdf, html, other]
Title:
AI Behavioral Science
Matthew O. Jackson, Qiaozhu Me, Stephanie W. Wang, Yutong Xie, Walter Yuan, Seth Benzell, Erik Brynjolfsson, Colin F. Camerer, James Evans, Brian Jabarian, Jon Kleinberg, Juanjuan Meng, Sendhil Mullainathan, Asuman Ozdaglar, Thomas Pfeiffer, Moshe Tennenholtz, Robb Willer, Diyi Yang, Teng Ye
Subjects:
Human-Computer Interaction (cs.HC); General Economics (econ.GN)
We discuss the three main areas comprising the new and emerging field of "AI Behavioral Science". This includes not only how AI can enhance research in the behavioral sciences, but also how the behavioral sciences can be used to study and better design AI and to understand how the world will change as AI and humans interact in increasingly layered and complex ways.
Replacement submissions (showing 9 of 9 entries)
[11]
arXiv:2211.04763
(replaced)
[pdf, html, other]
Title:
Jihad over Centuries
Masahiro Kubo, Shunsuke Tsuda
Subjects:
General Economics (econ.GN)
This paper examines the origins of Islamist insurgencies, or jihad, through the lens of past prosperity, decline, and cultural revival in West Africa. Using shrinking water sources as an instrument, we show that trans-Saharan cities once-thriving under pre-colonial Islamic states but now deserted have become contemporary hotspots of jihadist violence. We argue that military power asymmetries between Islamic states and colonizers during historical jihad shaped the persistence of jihadist ideology, fueling today's resurgence especially in areas that lacked intense armed resistance against colonial invasions. Extensive qualitative evidence, a dynamic model of conflict, and individual-level surveys examining ideologies support this mechanism.
[12]
arXiv:2401.13694
(replaced)
[pdf, other]
Title:
The Arrival of Fast Internet and Employment in Africa: Comment
David Roodman
Subjects:
General Economics (econ.GN)
Hjort and Poulsen (2019) frames the staggered arrival of submarine Internet cables on the shores of Africa circa 2010 as a difference-in-differences natural experiment. The paper finds positive impacts of broadband on individual- and firm-level employment--with a bias toward skilled employment--and on nighttime light emissions. These results largely are not robust to alternative geocoding of survey locations, to correcting for a satellite changeover at end-2009, and to revisiting a definition of the treated zone that has no clear technological basis, is narrower than the spatial resolution of nearly all the data sources, and is empirically suboptimal as a representation of the geography of broadband.
[13]
arXiv:2410.22706
(replaced)
[pdf, html, other]
Title:
Graph Signal Processing for Global Stock Market Realized Volatility Forecasting
Zhengyang Chi, Junbin Gao, Chao Wang
Subjects:
General Finance (q-fin.GN)
This paper introduces an innovative realized volatility (RV) forecasting framework that extends the conventional Heterogeneous autoregressive (HAR) model via integrating Graph Signal Processing (GSP). The study first evaluates various constructions of volatility-interrelationship networks by analyzing how the associated graph signal energy tracks global financial market volatility. Volatility spillovers are subsequently embedded into the proposed framework, which employs the graph Fourier transform (GFT) and its inverse to effectively capture global stock market dynamics in both the spectral and spatial domains. The framework not only provides a global context for modeling the volatility interrelationships, but also captures the nonlinearity and directionality of the volatility spillover effect. The empirical study using RV data of $24$ global stock market indices compares short-, mid- and long-term RV forecasts with various HAR-type benchmarks and a graph neural network-based HAR model. The proposed model consistently outperforms all comparators, demonstrating the effectiveness of integrating GSP into the HAR model for RV forecasting.
[14]
arXiv:2507.06422
(replaced)
[pdf, html, other]
Title:
Trial Length, Pricing, and Rationally Inattentive Customers
F. Nguyen
Subjects:
General Economics (econ.GN)
The "free trial" followed by automatic renewal is a dominant business model in the digital economy. Standard models explain trials as a mechanism for consumers to learn their valuation for a product. We propose a complementary theory based on the rational inattention framework. Consumers know their valuation but face a cognitive cost to remember to cancel an unwanted subscription. We model this using a Shannon entropy-based cost of information processing, where a consumer's baseline attention level decays with the length of the trial period. This creates a novel trade-off for a monopolist firm: a longer trial increases "inattentive revenue" from consumers who fail to cancel, but it also lowers ex-ante consumer utility, making the initial offer less attractive. We show that this trade-off leads to an interior optimal trial length, even for products where value-learning is instantaneous. Our model, under standard assumptions about demand elasticity and the distribution of consumer valuations, generates sharp, testable predictions about the relationship between contract terms. We find that the optimal renewal price and trial length are complements: firms offering longer trials will also set higher post-trial prices. We analyze the impact of policies aimed at curbing consumer exploitation, such as "click-to-cancel" regulations. We show that such policies, by making attention effectively cheaper, lead firms to reduce trial lengths. The effect on price depends directly on the elasticity of demand from loyal subscribers. We also extend the model to include paid trials, showing that introductory prices and trial lengths act as strategic substitutes. Our framework provides a micro-founded explanation for common features of subscription contracts and offers a new lens through which to evaluate consumer protection policies in digital markets.
[15]
arXiv:2507.20410
(replaced)
[pdf, other]
Title:
Beyond pay: AI skills reward more job benefits
Alejandra Mira, Matthew Bone, Fabian Stephany
Comments:
42 pages, 10 figures, 6 tables
Subjects:
General Economics (econ.GN)
This study investigates the non-monetary rewards associated with artificial intelligence (AI) skills in the U.S. labour market. Using a dataset of approximately ten million online job vacancies from 2018 to 2024, we identify AI roles-positions requiring at least one AI-related skill-and examine the extent to which these roles offer non-monetary benefits such as tuition assistance, paid leave, health and well-being perks, parental leave, workplace culture enhancements, and remote work options. While previous research has documented substantial wage premiums for AI-related roles due to growing demand and limited talent supply, our study asks whether this demand also translates into enhanced non-monetary compensation. We find that AI roles are significantly more likely to offer such perks, even after controlling for education requirements, industry, and occupation type. It is twice as likely for an AI role to offer parental leave and almost three times more likely to provide remote working options. Moreover, the highest-paying AI roles tend to bundle these benefits, suggesting a compound premium where salary increases coincide with expanded non-monetary rewards. AI roles offering parental leave or health benefits show salaries that are, on average, 12% to 20% higher than AI roles without this benefit. This pattern is particularly pronounced in years and occupations experiencing the highest AI-related demand, pointing to a demand-driven dynamic. Our findings underscore the strong pull of AI talent in the labor market and challenge narratives of technological displacement, highlighting instead how employers compete for scarce talent through both financial and non-financial incentives.
[16]
arXiv:2508.20075
(replaced)
[pdf, html, other]
Title:
Predicting Qualification Thresholds in UEFA's incomplete round-robin tournaments
David Winkelmann, Rouven Michels, Christian Deutscher
Subjects:
General Economics (econ.GN)
For the 2024/25 season, the Union of European Football Associations (UEFA) introduced an incomplete round-robin format in the Champions League, Europa League, and Conference League, replacing the traditional group stage with a single league table of all 36 teams. Under this structure, the top eight teams advance directly to the round of 16, while those ranked 9th-24th compete in a play-off round. Simulation-based analyses, such as those by commercial data analyst Opta, provide indicative point thresholds for qualification but reveal deviations when compared with actual outcomes in the first season. To overcome these discrepancies, we employ a bivariate Dixon-Coles model that accounts for the lower frequency of draws observed in the 2024/25 UCL season, with team strengths proxied by Elo ratings. This framework enables the simulation of match outcomes and the estimation of qualification thresholds for both direct advancement and play-off participation. Our results provide scientific guidance for clubs and managers, supporting strategic decision-making under uncertainty regarding their progression prospects in the new UEFA club competition formats.
[17]
arXiv:2509.12874
(replaced)
[pdf, html, other]
Title:
Income Disaster, Role of Income Support, and Optimal Retirement
Tae Ung Gang, Seyoung Park, Yong Hyun Shin
Comments:
12 pages
Subjects:
Portfolio Management (q-fin.PM)
This paper investigates the interactions among consumption/savings, investment, and retirement choices with income disaster. We consider low-income people who are exposed to income disaster so that they retire involuntarily when income disaster occurs. The government provides extra income support to low-income retirees who suffer from significant income gaps. We demonstrate that the decision to enter retirement in the event of income disaster depends crucially on the level of income support. In particular, we quantitatively identify a certain income support level below which the optimal decision is to delay retirement. This implies that availability of the government's extra income support can be particularly important for the low-income people to achieve optimal retirement with income disaster.
[18]
arXiv:2001.10488
(replaced)
[pdf, other]
Title:
Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications
Nassim Nicholas Taleb
Comments:
Third Revised Edition, 2025
Subjects:
Other Statistics (stat.OT); Risk Management (q-fin.RM); Applications (stat.AP); Methodology (stat.ME)
(The third edition corrects minor typos and adds 3 chapters synthesized from published papers plus an appendix on maximum entropy distributions.) The monograph investigates the misapplication of conventional statistical techniques to fat tailed distributions and looks for remedies, when possible.
Switching from thin tailed to fat tailed distributions requires more than "changing the color of the dress". Traditional asymptotics deal mainly with either n=1 or $n=\infty$, and the real world is in between, under of the "laws of the medium numbers" --which vary widely across specific distributions. Both the law of large numbers and the generalized central limit mechanisms operate in highly idiosyncratic ways outside the standard Gaussian or Levy-Stable basins of convergence.
A few examples:
+ The sample mean is rarely in line with the population mean, with effect on "naive empiricism", but can be sometimes be estimated via parametric methods.
+ The "empirical distribution" is rarely empirical.
+ Parameter uncertainty has compounding effects on statistical metrics.
+ Dimension reduction (principal components) fails.
+ Inequality estimators (GINI or quantile contributions) are not additive and produce wrong results.
+ Many "biases" found in psychology become entirely rational under more sophisticated probability distributions
+ Most of the failures of financial economics, econometrics, and behavioral economics can be attributed to using the wrong distributions.
This book, the first volume of the Technical Incerto, weaves a narrative around published journal articles.
[19]
arXiv:2503.15186
(replaced)
[pdf, html, other]
Title:
Optimal Data Splitting for Holdout Cross-Validation in Large Covariance Matrix Estimation
Lamia Lamrani, Christian Bongiorno, Marc Potters
Subjects:
Statistics Theory (math.ST); Portfolio Management (q-fin.PM); Risk Management (q-fin.RM); Applications (stat.AP)
Cross-validation is a statistical tool that can be used to improve large covariance matrix estimation. Although its efficiency is observed in practical applications and a convergence result towards the error of the non linear shrinkage is available in the high-dimensional regime, formal proofs that take into account the finite sample size effects are currently lacking. To carry on analytical analysis, we focus on the holdout method, a single iteration of cross-validation, rather than the traditional $k$-fold approach. We derive a closed-form expression for the expected estimation error when the population matrix follows a white inverse Wishart distribution, and we observe the optimal train-test split scales as the square root of the matrix dimension. For general population matrices, we connected the error to the variance of eigenvalues distribution, but approximations are necessary. In this framework and in the high-dimensional asymptotic regime, both the holdout and $k$-fold cross-validation methods converge to the optimal estimator when the train-test ratio scales with the square root of the matrix dimension which is coherent with the existing theory.
Total of 19 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack