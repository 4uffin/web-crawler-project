Quantitative Finance
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
q-fin
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Quantitative Finance
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Friday, 19 September 2025
Total of 11 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 6 of 6 entries)
[1]
arXiv:2509.14385
[pdf, html, other]
Title:
Adaptive and Regime-Aware RL for Portfolio Optimization
Gabriel Nixon Raj
Subjects:
Portfolio Management (q-fin.PM); Computational Finance (q-fin.CP)
This study proposes a regime-aware reinforcement learning framework for long-horizon portfolio optimization. Moving beyond traditional feedforward and GARCH-based models, we design realistic environments where agents dynamically reallocate capital in response to latent macroeconomic regime shifts. Agents receive hybrid observations and are trained using constrained reward functions that incorporate volatility penalties, capital resets, and tail-risk shocks. We benchmark multiple architectures, including PPO, LSTM-based PPO, and Transformer PPO, against classical baselines such as equal-weight and Sharpe-optimized portfolios. Our agents demonstrate robust performance under financial stress. While Transformer PPO achieves the highest risk-adjusted returns, LSTM variants offer a favorable trade-off between interpretability and training cost. The framework promotes regime-adaptive, explainable reinforcement learning for dynamic asset allocation.
[2]
arXiv:2509.14401
[pdf, html, other]
Title:
Predictive Performance of LSTM Networks on Sectoral Stocks in an Emerging Market: A Case Study of the Pakistan Stock Exchange
Ahad Yaqoob, Syed M. Abdullah
Comments:
11 pages, 10 figures. Ancillary files to reproduce results available at this https URL Random seeder is set to 42. This paper is also to be submitted to Journal of Machine Learning Research (JMLR) after feedback and response
Subjects:
Trading and Market Microstructure (q-fin.TR)
The application of deep learning models for stock price forecasting in emerging markets remains underexplored despite their potential to capture complex temporal dependencies. This study develops and evaluates a Long Short-Term Memory (LSTM) network model for predicting the closing prices of ten major stocks across diverse sectors of the Pakistan Stock Exchange (PSX). Utilizing historical OHLCV data and an extensive set of engineered technical indicators, we trained and validated the model on a multi-year dataset. Our results demonstrate strong predictive performance ($R^2 > 0.87$) for stocks in stable, high-liquidity sectors such as power generation, cement, and fertilizers. Conversely, stocks characterized by high volatility, low liquidity, or sensitivity to external shocks (e.g., global oil prices) presented significant forecasting challenges. The study provides a replicable framework for LSTM-based forecasting in data-scarce emerging markets and discusses implications for investors and future research.
[3]
arXiv:2509.14422
[pdf, other]
Title:
Aggregating Epigenetic Clocks to Study Human Capital Formation
Giorgia Menta, Pietro Biroli, Divya Mehta, Conchita D'Ambrosio, Deborah Cobb-Clark
Subjects:
General Economics (econ.GN)
Epigenetics is the study of how people's behavior and environments influence the way their genes are expressed, even though their DNA sequence is itself unchanged. By aggregating age-related epigenetic markers, epigenetic 'clocks' have become the leading tool for studying biological aging. We make an important contribution by developing a novel, integrated measure of epigenetic aging--the Multi EpiGenetic Age (MEGA) clock--which combines several existing epigenetic clocks to reduce measurement error and improve estimation efficiency. We use the MEGA clock in three empirical contexts to show that: i) accelerated epigenetic aging in adolescence is associated with worse educational, mental-health, and labor market outcomes in early adulthood; ii) exposure to child maltreatment before adolescence is associated with half a year higher epigenetic aging; and iii) that entering school one year later accelerates epigenetic aging by age seven, particularly among disadvantaged children. The MEGA clock is robust to alternative methods for constructing it, providing a flexible and interpretable approach for incorporating epigenetic data into a wide variety of settings.
[4]
arXiv:2509.14529
[pdf, html, other]
Title:
Unbiased Rough Integrators and No Free Lunch in Rough-Path-Based Market Models
Tomoyuki Ichiba, Qijin Shi
Comments:
50 pages, 0 figures
Subjects:
Mathematical Finance (q-fin.MF); Probability (math.PR)
Built to generalise classical stochastic calculus, rough path theory provides a natural and pathwise framework to model continuous non-semimartingale assets. This paper investigates the ultimate capacity of this framework to support frictionless continuous No-Free-Lunch markets à la Kreps-Yan. We establish a ``Rough Kreps-Yan" theorem, which links our No Controlled Free Lunch (NCFL) condition to the unbiasedness of the driver of the price process as a rough integrator. The central work of this paper is a complete classification of these unbiased rough integrators with respect to different classes of controlled paths as integrands. As the set of admissible trading strategies is enlarged to include Markovian-type and signature-type portfolios, the only admissible random rough paths must be infinitesimally close to the Itô rough path lift of a standard Brownian motion, up to a time change. In particular, Gaussianity is no longer a model assumption, but rather a no-arbitrage market consequence. Notably, simple strategies do not appear in the theory, and if they are then reintroduced, the rough noise is further enforced to be the Itô rough path of Brownian motion itself. Ultimately, this implies that continuous frictionless markets based on rough path theory are inevitably constrained to the semimartingale paradigm, providing a definitive answer on the limits of this approach. Our framework covers $\alpha-$Hölder continuous rough paths for $\alpha>0$ arbitrarily small.
[5]
arXiv:2509.14645
[pdf, html, other]
Title:
Are Final Market Prices Sufficient for Information Aggregation? Evidence from Last-Minute Dynamics in Parimutuel Betting
Hiroaki Hanyu, Shunsuke Ishii, Suguru Otani, Kazuhiro Teramoto
Comments:
18 pages with 8 pages appendix
Subjects:
General Economics (econ.GN)
Most betting market models employ static frameworks that condition decisions on final odds. Using a unique dataset of interim odds from Japanese horse racing, this study examines the validity of such static analyses by asking whether there is a systematic relationship between expected returns and the trajectory of odds. We find that returns are negatively related to last-minute changes in odds, and that these late movements attenuate the favorite-longshot bias by weakening the correlation between final odds and returns. These patterns suggest that informed bettors place wagers at the final stage based on private signals, leaving surprises in final odds.
[6]
arXiv:2509.14795
[pdf, html, other]
Title:
Paradoxes of the public sector productivity measurement
Timo Kuosmanen, Xun Zhou
Subjects:
General Economics (econ.GN)
This paper critically investigates standard total factor productivity (TFP) measurement in the public sector, where output information is often incomplete or distorted. The analysis reveals fundamental paradoxes under three common output measurement conventions. When cost-based value added is used as the aggregate output, measured TFP may paradoxically decline as a result of genuine productivity-enhancing changes such as technical progress and improved allocative and scale efficiencies, as well as reductions in real input prices. We show that the same problems carry over to the situation where the aggregate output is constructed as the cost-share weighted index of outputs. In the case of distorted output prices, measured TFP may move independently of any productivity changes and instead reflect shifts in pricing mechanisms. Using empirical illustrations from the United Kingdom and Finland, we demonstrate that such distortions are not merely theoretical but are embedded in widely used public productivity statistics. We argue that public sector TFP measurement requires a shift away from cost-based aggregation of outputs and toward non-market valuation methods grounded in economic theory.
Cross submissions (showing 1 of 1 entries)
[7]
arXiv:2509.14532
(cross-list from cs.CY)
[pdf, html, other]
Title:
Leveraging Artificial Intelligence as a Strategic Growth Catalyst for Small and Medium-sized Enterprises
Oluwatosin Agbaakin (Indiana University Indianapolis)
Comments:
14 pages, 2 figures. A review and strategic framework for AI adoption in SMEs
Subjects:
Computers and Society (cs.CY); Artificial Intelligence (cs.AI); General Economics (econ.GN)
Artificial Intelligence (AI) has transitioned from a futuristic concept reserved for large corporations to a present-day, accessible, and essential growth lever for Small and Medium-sized Enterprises (SMEs). For entrepreneurs and business leaders, strategic AI adoption is no longer an option but an imperative for competitiveness, operational efficiency, and long-term survival. This report provides a comprehensive framework for SME leaders to navigate this technological shift, offering the foundational knowledge, business case, practical applications, and strategic guidance necessary to harness the power of AI. The quantitative evidence supporting AI adoption is compelling; 91% of SMEs using AI report that it directly boosts their revenue. Beyond top-line growth, AI drives profound operational efficiencies, with studies showing it can reduce operational costs by up to 30% and save businesses more than 20 hours of valuable time each month. This transformation is occurring within the context of a seismic economic shift; the global AI market is projected to surge from $233.46 Billion in 2024 to an astonishing $1.77 Trillion by 2032. This paper demystifies the core concepts of AI, presents a business case based on market data, details practical applications, and lays out a phased, actionable adoption strategy.
Replacement submissions (showing 4 of 4 entries)
[8]
arXiv:2403.15243
(replaced)
[pdf, html, other]
Title:
Robust Utility Optimization via a GAN Approach
Florian Krach, Josef Teichmann, Hanna Wutte
Subjects:
Computational Finance (q-fin.CP); Machine Learning (cs.LG); Mathematical Finance (q-fin.MF); Portfolio Management (q-fin.PM)
Robust utility optimization enables an investor to deal with market uncertainty in a structured way, with the goal of maximizing the worst-case outcome. In this work, we propose a generative adversarial network (GAN) approach to (approximately) solve robust utility optimization problems in general and realistic settings. In particular, we model both the investor and the market by neural networks (NN) and train them in a mini-max zero-sum game. This approach is applicable for any continuous utility function and in realistic market settings with trading costs, where only observable information of the market can be used. A large empirical study shows the versatile usability of our method. Whenever an optimal reference strategy is available, our method performs on par with it and in the (many) settings without known optimal strategy, our method outperforms all other reference strategies. Moreover, we can conclude from our study that the trained path-dependent strategies do not outperform Markovian ones. Lastly, we uncover that our generative approach for learning optimal, (non-) robust investments under trading costs generates universally applicable alternatives to well known asymptotic strategies of idealized settings.
[9]
arXiv:2506.09760
(replaced)
[pdf, html, other]
Title:
The Additive Bachelier model with an application to the oil option market in the Covid period
Roberto Baviera, Michele Domenico Massaria
Subjects:
Mathematical Finance (q-fin.MF)
In April 2020, the Chicago Mercantile Exchange temporarily switched the pricing formula for West Texas Intermediate oil market options from the Black model to the Bachelier model. In this context, we introduce an Additive Bachelier model that provides a simple closed-form solution and a good description of the Implied volatility surface. This new Additive model exhibits several notable mathematical and financial properties. It ensures the no-arbitrage condition, a critical requirement in highly volatile markets, while also enabling a parsimonious synthesis of the volatility surface. The model features only three parameters, each one with a clear financial interpretation: the volatility term structure, vol-of-vol, and a parameter for modelling skew. The proposed model supports efficient pricing of path-dependent exotic options via Monte Carlo simulation, using a straightforward and computationally efficient approach. Its calibration process can follow a cascade calibration: first, it accurately replicates the term structures of forwards and At-The-Money volatilities observed in the market; second, it fits the smile of the volatility surface. Overall this model provides a robust and parsimonious description of the oil option market during the exceptionally volatile first period of the Covid-19 pandemic.
[10]
arXiv:2509.14057
(replaced)
[pdf, html, other]
Title:
Machines are more productive than humans until they aren't, and vice versa
Riccardo Zanardelli
Comments:
Added more detailed characterization of the simulation setup in the conclusions; added more detailed considerations in the conclusion, no changes in results; minor typos corrected; improved the example in section A.4.1, with corrections to Table 17
Subjects:
General Economics (econ.GN); Artificial Intelligence (cs.AI)
With the growth of artificial skills, organizations may increasingly confront with the problem of optimizing skill policy decisions guided by economic principles. This paper addresses the underlying complexity of this challenge by developing an in-silico framework based on Monte Carlo simulations grounded in empirical realism to analyze the economic impact of human and machine skills, individually or jointly deployed, in the execution of tasks presenting varying levels of complexity. Our results provide quantitative support for the established notions that automation tends to be the most economically-effective strategy for tasks characterized by low-to-medium generalization difficulty, while automation may struggle to match the economic utility of human skills in more complex scenarios. Critically, our simulations highlight that combining human and machine skills can be the most effective strategy when a high level of generalization is required, but only if genuine augmentation is achieved. In contrast, when failing to realize this synergy, the human-machine policy is severely penalized by the inherent costs of its dual skill structure, causing it to destroy value and becoming the worst choice from an economic perspective. The takeaway for decision-makers is unambiguous: in contexts requiring high generalization capabilities, simply allocating human and machine skills to a task is insufficient, and a human-machine skill policy is neither a silver-bullet solution nor a low-risk compromise. Rather, it is a critical opportunity to boost competitiveness that demands a strong organizational commitment to enabling augmentation. Also, our findings show that improving the cost-effectiveness of machine skills over time, while useful, does not replace the fundamental need to focus on achieving augmentation.
[11]
arXiv:2505.18297
(replaced)
[pdf, html, other]
Title:
A deep solver for backward stochastic Volterra integral equations
Kristoffer Andersson, Alessandro Gnoatto, Camilo Andrés García Trillos
Comments:
25 pages, 10 figures
Subjects:
Numerical Analysis (math.NA); Machine Learning (cs.LG); Probability (math.PR); Mathematical Finance (q-fin.MF)
We present the first deep-learning solver for backward stochastic Volterra integral equations (BSVIEs) and their fully-coupled forward-backward variants. The method trains a neural network to approximate the two solution fields in a single stage, avoiding the use of nested time-stepping cycles that limit classical algorithms. For the decoupled case we prove a non-asymptotic error bound composed of an a posteriori residual plus the familiar square root dependence on the time step. Numerical experiments confirm this rate and reveal two key properties: \emph{scalability}, in the sense that accuracy remains stable from low dimension up to 500 spatial variables while GPU batching keeps wall-clock time nearly constant; and \emph{generality}, since the same method handles coupled systems whose forward dynamics depend on the backward solution. These results open practical access to a family of high-dimensional, path-dependent problems in stochastic control and quantitative finance.
Total of 11 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack