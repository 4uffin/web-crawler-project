Quantitative Finance
Skip to main content
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
q-fin
Help | Advanced Search
All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
open search
GO
open navigation menu
quick links
Login
Help Pages
About
Quantitative Finance
New submissions
Cross-lists
Replacements
See recent articles
Showing new listings for Tuesday, 16 September 2025
Total of 39 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
New submissions (showing 20 of 20 entries)
[1]
arXiv:2509.10461
[pdf, html, other]
Title:
Momentum-integrated Multi-task Stock Recommendation with Converge-based Optimization
Hao Wang, Jingshu Peng, Yanyan Shen, Xujia Li, Lei Chen
Comments:
10 pages, 5 figures
Subjects:
Statistical Finance (q-fin.ST); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
Stock recommendation is critical in Fintech applications, which use price series and alternative information to estimate future stock performance. Although deep learning models are prevalent in stock recommendation systems, traditional time-series forecasting training often fails to capture stock trends and rankings simultaneously, which are essential consideration factors for investors. To tackle this issue, we introduce a Multi-Task Learning (MTL) framework for stock recommendation, \textbf{M}omentum-\textbf{i}ntegrated \textbf{M}ulti-task \textbf{Stoc}k \textbf{R}ecommendation with Converge-based Optimization (\textbf{MiM-StocR}). To improve the model's ability to capture short-term trends, we novelly invoke a momentum line indicator in model training. To prioritize top-performing stocks and optimize investment allocation, we propose a list-wise ranking loss function called Adaptive-k ApproxNDCG. Moreover, due to the volatility and uncertainty of the stock market, existing MTL frameworks face overfitting issues when applied to stock time series. To mitigate this issue, we introduce the Converge-based Quad-Balancing (CQB) method. We conducted extensive experiments on three stock benchmarks: SEE50, CSI 100, and CSI 300. MiM-StocR outperforms state-of-the-art MTL baselines across both ranking and profitable evaluations.
[2]
arXiv:2509.10483
[pdf, html, other]
Title:
Equity Premium Prediction: Taking into Account the Role of Long, even Asymmetric, Swings in Stock Market Behavior
Kuok Sin Un, Marcel Ausloos
Comments:
36 pages, 69 references, 7 tables, 3 figures ; as prepared for Physica A
Subjects:
Statistical Finance (q-fin.ST); Trading and Market Microstructure (q-fin.TR)
Through a novel approach, this paper shows that substantial change in stock market behavior has a statistically and economically significant impact on equity risk premium predictability both on in-sample and out-of-sample cases. In line with Auer's ''Bullish ratio'', a ''Bullish index'' is introduced to measure the changes in stock market behavior, which we describe through a ''fluctuation detrending moving average analysis'' (FDMAA) for returns. We consider 28 indicators. We find that a ''positive shock'' of the Bullish Index is closely related to strong equity risk premium predictability for forecasts based on macroeconomic variables for up to six months. In contrast, a ''negative shock'' is associated with strong equity risk premium predictability with adequate forecasts for up to nine months when based on technical indicators.
[3]
arXiv:2509.10542
[pdf, html, other]
Title:
Adaptive Temporal Fusion Transformers for Cryptocurrency Price Prediction
Arash Peik, Mohammad Ali Zare Chahooki, Amin Milani Fard, Mehdi Agha Sarram
Subjects:
Statistical Finance (q-fin.ST); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)
Precise short-term price prediction in the highly volatile cryptocurrency market is critical for informed trading strategies. Although Temporal Fusion Transformers (TFTs) have shown potential, their direct use often struggles in the face of the market's non-stationary nature and extreme volatility. This paper introduces an adaptive TFT modeling approach leveraging dynamic subseries lengths and pattern-based categorization to enhance short-term forecasting. We propose a novel segmentation method where subseries end at relative maxima, identified when the price increase from the preceding minimum surpasses a threshold, thus capturing significant upward movements, which act as key markers for the end of a growth phase, while potentially filtering the noise. Crucially, the fixed-length pattern ending each subseries determines the category assigned to the subsequent variable-length subseries, grouping typical market responses that follow similar preceding conditions. A distinct TFT model trained for each category is specialized in predicting the evolution of these subsequent subseries based on their initial steps after the preceding peak. Experimental results on ETH-USDT 10-minute data over a two-month test period demonstrate that our adaptive approach significantly outperforms baseline fixed-length TFT and LSTM models in prediction accuracy and simulated trading profitability. Our combination of adaptive segmentation and pattern-conditioned forecasting enables more robust and responsive cryptocurrency price prediction.
[4]
arXiv:2509.10548
[pdf, html, other]
Title:
The Economics and Game Theory of OSINT Frontline Photography: Risk, Attention, and the Collective Dilemma
Jonathan Teagan
Subjects:
General Economics (econ.GN)
This paper develops an economic model of the Open Source Intelligence (OSINT) attention economy in contemporary armed conflict. We conceptualize attention (e.g. social media views, followers, likes) as revenue, and time and risk spent in analysis as costs. Using utility functions and simple game theoretic setups, we show how OSINT actors (amateurs, journalists, analysts, and state operatives) allocate effort to maximize net attention benefit. We incorporate strategic behaviors such as a first mover advantage (racing to publish) and prisoner's dilemma scenarios (to share information or hold it back). In empirical case studies, especially the Ukraine conflict actors like the UAV unit Madyar's Birds and volunteer channels like Kavkazfighter, illustrate how battlefront reporting translates into digital revenue (attention) at real cost. We draw on recent literature and data (e.g., public follower counts, viral posts) to examine trends such as OSINT virality. Finally, we discuss policy implications for balancing transparency with operational security, citing calls for verification ethics and attention sustaining narratives. Our analysis bridges conflict studies and economics, highlighting OSINT as both a public good and a competitive product in today's information war.
[5]
arXiv:2509.10553
[pdf, html, other]
Title:
A Stochastic Model for Illiquid Stock Prices and its Conclusion about Correlation Measurement
Erina Nanyonga, Juma Kasozi, Fred Mayambala, Hassan W. Kayondo, Matt Davison
Comments:
21 pages
Subjects:
Statistical Finance (q-fin.ST)
This study explores the behavioral dynamics of illiquid stock prices in a listed stock market. Illiquidity, characterized by wide bid and ask spreads affects price formation by decoupling prices from standard risk and return relationships and increasing sensitivity to market sentiment. We model the prices at the Uganda Securities Exchange (USE) which is illiquid in that the prices remain constant much of the time thus complicating price modelling. We circumvent this challenge by combining the Markov model (MM) with two models; the exponential Ornstein Uhlenbeck model (XOU) and geometric Brownian motion (gBm). In the combined models, the MM was used to capture the constant prices in the stock prices while the XOU and gBm captured the stochastic price dynamics. We modelled stock prices using the combined models, as well as XOU and gBm alone. We found that USE stocks appeared to have low correlation with one another. Using theoretical analysis, simulation study and empirical analysis, we conclude that this apparent low correlation is due to illiquidity. In particular data simulated from combined MM-gBm, in which the gBm portion were highly correlated resulted in a low measured correlation when the Markov chain had a higher transition from zero state to zero state.
[6]
arXiv:2509.10586
[pdf, other]
Title:
Stabilising Lifetime PD Models under Forecast Uncertainty
Vahab Rostampour
Subjects:
Risk Management (q-fin.RM); Systems and Control (eess.SY)
Estimating lifetime probabilities of default (PDs) under IFRS~9 and CECL requires projecting point--in--time transition matrices over multiple years. A persistent weakness is that macroeconomic forecast errors compound across horizons, producing unstable and volatile PD term structures. This paper reformulates the problem in a state--space framework and shows that a direct Kalman filter leaves non--vanishing variability. We then introduce an anchored observation model, which incorporates a neutral long--run economic state into the filter. The resulting error dynamics exhibit asymptotic stochastic stability, ensuring convergence in probability of the lifetime PD term structure. Simulation on a synthetic corporate portfolio confirms that anchoring reduces forecast noise and delivers smoother, more interpretable projections.
[7]
arXiv:2509.10795
[pdf, other]
Title:
The impact on health system expenditure in Australia and OECD countries from accelerated NCD mortality decline through prevention or treatment strategies to achieve Sustainable Development Goal Target 3.4
Bibha Dhungel, Jingjing Yang, Tim Wilson, Samantha Grimshaw, Emily Bourke, Stephanie Khuu, Tony Blakely
Subjects:
General Economics (econ.GN)
Background: It is unclear what the relative impacts of prevention or treatment of NCDs are on future health system expenditure. First, we estimated expenditure in Australia for prevention vs treatment pathways to achieve SDG target 3.4. Second, we applied the method to 34 other OECD countries.
Methods: We used GBD data to estimate average annual percentage changes in disease incidence, remission, and CFRs from 1990-2021, and projected to 2030 to estimate business-as-usual (BAU) reductions in NCD mortality risk (40q30). For countries not on track to meet SDG3.4 under BAU, we modelled two intervention scenarios commencing in 2022 to achieve SDG3.4: (1) prevention via accelerated incidence reduction; (2) treatment via accelerated increases in remission and decreases in CFRs. Australian disease expenditure data were input into a PMSLT model to estimate expenditure changes from 2022 to 2040. Assuming similar expenditure patterns, the method was applied across OECD countries.
Findings: In Australia, current trends project a 25% reduction in 40q30 by 2030, short of the 33.3% SDG3.4 target. Achieving this requires a 2.53 percentage point (pp) annual acceleration in incidence decline (prevention) or 1.56pp acceleration in CFR reduction and remission increase (treatment). Prevention reduces disease expenditure by 0.72%-3.17% by 2030 and 2040; treatment initially increase expenditure by 0.16%, before reducing it by 0.98%. A treatment scenario reducing only CFRs increased expenditure initially; increasing remission alone achieved savings similar to prevention. Only Sweden, Ireland, and South Korea were on track to meet SDG3.4. Other OECD countries showed similar expenditure impacts to Australia.
Interpretation: Whether reducing NCD mortality saves money depends on pathway taken (prevention or treatment). Care is needed when linking NCD mortality reduction to health system savings.
[8]
arXiv:2509.10802
[pdf, html, other]
Title:
Why Bonds Fail Differently? Explainable Multimodal Learning for Multi-Class Default Prediction
Yi Lu, Aifan Ling, Chaoqun Wang, Yaxin Xu
Subjects:
Risk Management (q-fin.RM); Computation and Language (cs.CL); Machine Learning (cs.LG); Computational Finance (q-fin.CP)
In recent years, China's bond market has seen a surge in defaults amid regulatory reforms and macroeconomic volatility. Traditional machine learning models struggle to capture financial data's irregularity and temporal dependencies, while most deep learning models lack interpretability-critical for financial decision-making. To tackle these issues, we propose EMDLOT (Explainable Multimodal Deep Learning for Time-series), a novel framework for multi-class bond default prediction. EMDLOT integrates numerical time-series (financial/macroeconomic indicators) and unstructured textual data (bond prospectuses), uses Time-Aware LSTM to handle irregular sequences, and adopts soft clustering and multi-level attention to boost interpretability. Experiments on 1994 Chinese firms (2015-2024) show EMDLOT outperforms traditional (e.g., XGBoost) and deep learning (e.g., LSTM) benchmarks in recall, F1-score, and mAP, especially in identifying default/extended firms. Ablation studies validate each component's value, and attention analyses reveal economically intuitive default drivers. This work provides a practical tool and a trustworthy framework for transparent financial risk modeling.
[9]
arXiv:2509.10810
[pdf, other]
Title:
Attribution Locus and the Timeliness of Long-lived Asset Write-downs
Yao-Lin Chang, Chun-Yang Lin, Chi-Chun Liu, Stephen G. Ryan
Subjects:
General Finance (q-fin.GN)
We examine the relative timeliness with which write-downs of long-lived assets incorporate adverse macroeconomic and industry outcomes versus adverse firm-specific outcomes. We posit that users of financial reports are more likely to attribute adverse firm-specific outcomes to suboptimal managerial actions, which provide managers with more incentive to delay write downs. We provide evidence that, controlling for other incentives to manage earnings, firms record write-downs in the current year that are driven by adverse macroeconomic and industry outcomes during both the current year and the next year, but they record write-downs driven by adverse firm-specific outcomes only in the current year.
[10]
arXiv:2509.10933
[pdf, html, other]
Title:
Bailouts and Redistribution
Mikayel Sukiasyan
Comments:
Job market paper; May 2021
Subjects:
General Economics (econ.GN)
What is the best macroprudential regulation when households differ in their exposure to profits from the financial sector? To answer the question, I study a real business cycle model with household heterogeneity and market incompleteness. In the model, shocks are amplified in states with high leverage, leading to lower investment. I consider the problem of a Ramsey planner who can finance transfers with a distortive tax on labor and levy taxes on the balance sheet components of experts. I show that the optimal tax on capital purchases is zero and the optimal policy relies mostly on a tax on deposit issuance. The latter redistributes between agents by affecting the equilibrium rate on deposits. The welfare gains from optimal policy are due to both redistribution and insurance and are larger the more unequal the initial distribution is. A simple tax rule that targets a level of leverage can achieve most of the welfare gains from optimal policy.
[11]
arXiv:2509.11271
[pdf, html, other]
Title:
Out-of-sample gravity predictions and trade policy counterfactuals
Nicolas Apfel, Holger Breinlich, Nick Green, Dennis Novy, J.M.C. Santos Silva, Tom Zylkin
Subjects:
General Economics (econ.GN)
Gravity equations are often used to evaluate counterfactual trade policy scenarios, such as the effect of regional trade agreements on trade flows. In this paper, we argue that the suitability of gravity equations for this purpose crucially depends on their out-of-sample predictive power. We propose a methodology that compares different versions of the gravity equation, both among themselves and with machine learning-based forecast methods such as random forests and neural networks. We find that the 3-way gravity model is difficult to beat in terms of out-of-sample average predictive performance, further justifying its place as the predominant tool for applied trade policy analysis. However, when the goal is to predict individual bilateral trade flows, the 3-way model can be outperformed by an ensemble machine learning method.
[12]
arXiv:2509.11420
[pdf, other]
Title:
Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning
Yijia Xiao, Edward Sun, Tong Chen, Fang Wu, Di Luo, Wei Wang
Comments:
Tauric Research: this https URL
Subjects:
Trading and Market Microstructure (q-fin.TR); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL); Machine Learning (cs.LG)
Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades. Although reasoning LLMs have advanced in step-by-step planning and verification, their application to risk-sensitive financial decisions is underexplored. We present Trading-R1, a financially-aware model that incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions. Trading-R1 Terminal will be released at this https URL.
[13]
arXiv:2509.11501
[pdf, html, other]
Title:
The Price of Disaster: Estimating the Impact of Hurricane Harvey on the Texas Construction Labor Market
Kartik Ganesh
Comments:
16 pages, 6 figures, 4 tables
Subjects:
General Economics (econ.GN); Computation (stat.CO)
This paper estimates the effect of Hurricane Harvey on wages and employment in the construction labor industry across impacted counties in Texas. Based on data from the Quarterly Census of Employment and Wages (QCEW) for the period 2016-2019, I adopted a difference-in-differences event study approach by comparing results in 41 FEMA-designated disaster counties with a set of unaffected southern control counties. I find that Hurricane Harvey had a large and long-lasting impact on labor market outcomes in the construction industry. More precisely, average log wages in treated counties rose by around 7.2 percent compared to control counties two quarters after the hurricane and remained high for the next two years. Employment effects were more gradual, showing a statistically significant increase only after six quarters, in line with the lagged nature of large-scale reconstruction activities. These results imply that natural disasters can generate persistent labor demand shocks to local construction markets, with policy implications for disaster recovery planning and workforce mobilization.
[14]
arXiv:2509.11562
[pdf, html, other]
Title:
Human or Robot? Evidence from Last-Mile Delivery Service
Baorui Li, Xincheng Ma, Brian Rongqing Han, Daizhong Tang, Lei Fu
Subjects:
General Economics (econ.GN)
As platforms increasingly deploy robots alongside human labor in last-mile logistics, little is known about how contextual features like product attributes, environmental conditions, and psychological mechanisms shape consumer preference in real-world settings. To address this gap, this paper conducts an empirical study on consumer choice between human versus robot service, analyzing 241,517 package-level choices from Alibaba's last-mile delivery stations. We identify how product privacy sensitivity, product value, and environmental complexity affect consumer preference. Our findings reveal that consumers are significantly more likely to choose robot delivery for privacy-sensitive packages (11.49%) and high-value products (0.97% per 1% increase in value), but prefer human couriers under adverse weather conditions (1.63%). These patterns are robust to alternative specifications and controls. These results also underscore that delivery choices are shaped not only by functional considerations but also by psychological concerns, highlighting the need for context-aware service design that aligns strategies with consumer perceptions.
[15]
arXiv:2509.11579
[pdf, html, other]
Title:
Group Survival Probability under Contagion in Microlending
Héctor Jasso-Fuentes, Alejandra Quintos, Xinta Yang
Subjects:
Mathematical Finance (q-fin.MF)
In this paper we apply a probabilistic approach to analyze the impact of contagious default among investment group members. A general for- mula is given to compute the group survival probability with the presence of contagion effect. Special cases of this probability model are exam- ined. In particular, we show that if the investment group is homogeneous, defined in the paper, then including more members into the group will eventually lead to default with probability 1, contrasting with the non- contagious scenario, where the default probability increases monotonically with respect to the group size. Also, we provide an upper bound of the optimal group size under the homogeneous setup; so, one can run a linear search with finite time to locate this optimizer.
[16]
arXiv:2509.11844
[pdf, html, other]
Title:
ProteuS: A Generative Approach for Simulating Concept Drift in Financial Markets
Andrés L. Suárez-Cetrulo, Alejandro Cervantes, David Quintana
Subjects:
Statistical Finance (q-fin.ST); Machine Learning (cs.LG)
Financial markets are complex, non-stationary systems where the underlying data distributions can shift over time, a phenomenon known as regime changes, as well as concept drift in the machine learning literature. These shifts, often triggered by major economic events, pose a significant challenge for traditional statistical and machine learning models. A fundamental problem in developing and validating adaptive algorithms is the lack of a ground truth in real-world financial data, making it difficult to evaluate a model's ability to detect and recover from these drifts. This paper addresses this challenge by introducing a novel framework, named ProteuS, for generating semi-synthetic financial time series with pre-defined structural breaks. Our methodology involves fitting ARMA-GARCH models to real-world ETF data to capture distinct market regimes, and then simulating realistic, gradual, and abrupt transitions between them. The resulting datasets, which include a comprehensive set of technical indicators, provide a controlled environment with a known ground truth of regime changes. An analysis of the generated data confirms the complexity of the task, revealing significant overlap between the different market states. We aim to provide the research community with a tool for the rigorous evaluation of concept drift detection and adaptation mechanisms, paving the way for more robust financial forecasting models.
[17]
arXiv:2509.11928
[pdf, html, other]
Title:
Meta-Learning Neural Process for Implied Volatility Surfaces with SABR-induced Priors
Jirong Zhuang, Xuan Wu
Comments:
10 pages
Subjects:
Computational Finance (q-fin.CP)
Constructing the implied volatility surface (IVS) is reframed as a meta-learning problem training across trading days to learn a general process that reconstructs a full IVS from few quotes, eliminating daily recalibration. We introduce the Volatility Neural Process, an attention-based model that uses a two-stage training: pre-training on SABR-generated surfaces to encode a financial prior, followed by fine-tuning on market data. On S&P 500 options (2006-2023; out-of-sample 2019-2023), our model outperforms SABR, SSVI, Gaussian Process, and an ablation trained only on real data. Relative to the ablation, the SABR-induced prior reduces RMSE by about 40% and dominates in mid- and long-maturity regions where quotes are sparse. The learned prior suppresses large errors, providing a practical, data-efficient route to stable IVS construction with a single deployable model.
[18]
arXiv:2509.11970
[pdf, html, other]
Title:
Sentiment Feedback in Equity Markets: Asymmetries, Retail Heterogeneity, and Structural Calibration
Lucas Marques Sneller
Subjects:
Trading and Market Microstructure (q-fin.TR)
We study how sentiment shocks propagate through equity returns and investor clientele using four independent proxies with sign-aligned kappa-rho parameters. A structural calibration links a one standard deviation innovation in sentiment to a pricing impact of 1.06 basis points with persistence parameter rho = 0.940, yielding a half-life of 11.2 months. The impulse response peaks around the 12-month horizon, indicating slow-moving amplification. Cross-sectionally, a simple D10-D1 portfolio earns 4.0 basis points per month with Sharpe ratios of 0.18-0.85, consistent with tradable exposure to the sentiment factor. Three regularities emerge: (i) positive sentiment innovations transmit more strongly than negative shocks (amplification asymmetry); (ii) effects are concentrated in retail-tilted and non-optionable stocks (clientele heterogeneity); and (iii) responses are state-dependent across volatility regimes - larger on impact in high-VIX months but more persistent in low-VIX months. Baseline time-series fits are parsimonious (R2 ~ 0.001; 420 monthly observations), yet the calibrated dynamics reconcile modest impact estimates with sizable long-short payoffs. Consistent with Miller (1977), a one standard deviation sentiment shock has 1.72-8.69 basis points larger effects in low-breadth stocks across horizons of 1-12 months, is robust to institutional flows, and exhibits volatility state dependence - larger on impact but less persistent in high-VIX months, smaller on impact but more persistent in low-VIX months.
[19]
arXiv:2509.11990
[pdf, html, other]
Title:
Bootstrapping Liquidity in BTC-Denominated Prediction Markets
Fedor Shabashev
Comments:
LaTeX; pdfLaTeX; includes precompiled this http URL. 2 figures (TikZ)
Subjects:
Trading and Market Microstructure (q-fin.TR)
Prediction markets have gained adoption as on-chain mechanisms for aggregating information, with platforms such as Polymarket demonstrating demand for stablecoin-denominated markets. However, denominating in non-interest-bearing stablecoins introduces inefficiencies: participants face opportunity costs relative to the fiat risk-free rate, and Bitcoin holders in particular lose exposure to BTC appreciation when converting into stablecoins. This paper explores the case for prediction markets denominated in Bitcoin, treating BTC as a deflationary settlement asset analogous to gold under the classical gold standard. We analyse three methods of supplying liquidity to a newly created BTC-denominated prediction market: cross-market making against existing stablecoin venues, automated market making, and DeFi-based redirection of user trades. For each approach we evaluate execution mechanics, risks (slippage, exchange-rate risk, and liquidation risk), and capital efficiency. Our analysis shows that cross-market making provides the most user-friendly risk profile, though it requires active professional makers or platform-subsidised liquidity. DeFi redirection offers rapid bootstrapping and reuse of existing USDC liquidity, but exposes users to liquidation thresholds and exchange-rate volatility, reducing capital efficiency. Automated market making is simple to deploy but capital-inefficient and exposes liquidity providers to permanent loss. The results suggest that BTC-denominated prediction markets are feasible, but their success depends critically on the choice of liquidity provisioning mechanism and the trade-off between user safety and deployment convenience.
[20]
arXiv:2509.12084
[pdf, other]
Title:
Geopolitical Barriers to Globalization
Tianyu Fan, Mai Wo, Wei Xiang
Subjects:
General Economics (econ.GN)
This paper systematically estimates and quantifies how geopolitical alignment shapes global trade across three distinct eras: the Cold War, hyper-globalization, and contemporary fragmentation. We construct a novel measure of bilateral alignment using large language models to compile and analyze 833,485 political events spanning 193 countries from 1950 to 2024. Our analysis reveals that trade flows systematically track geopolitical alignment in both bilateral relationships and aggregate patterns. Using local projections within a gravity framework, we estimate that a one-standard-deviation improvement in geopolitical alignment increases bilateral trade by 20 percent over ten years. Integrating these elasticities into a quantitative general equilibrium model, we find that deteriorating geopolitical relations have reduced global trade by 7 percentage points between 1995 and 2020. Our findings provide empirical benchmarks for evaluating the costs of geopolitical fragmentation in an era of renewed great power competition.
Cross submissions (showing 4 of 4 entries)
[21]
arXiv:2509.10465
(cross-list from math.OC)
[pdf, html, other]
Title:
Bilevel subsidy-enabled mobility hub network design with perturbed utility coalitional choice-based assignment
Hai Yang, Joseph Y. J. Chow
Subjects:
Optimization and Control (math.OC); Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT); General Economics (econ.GN)
Urban mobility is undergoing rapid transformation with the emergence of new services. Mobility hubs (MHs) have been proposed as physical-digital convergence points, offering a range of public and private mobility options in close proximity. By supporting Mobility-as-a-Service, these hubs can serve as focal points where travel decisions intersect with operator strategies. We develop a bilevel MH platform design model that treats MHs as control levers. The upper level (platform) maximizes revenue or flow by setting subsidies to incentivize last-mile operators; the lower level captures joint traveler-operator decisions with a link-based Perturbed Utility Route Choice (PURC) assignment, yielding a strictly convex quadratic program. We reformulate the bilevel problem to a single-level program via the KKT conditions of the lower level and solve it with a gap-penalty method and an iterative warm-start scheme that exploits the computationally cheap lower-level problem. Numerical experiments on a toy network and a Long Island Rail Road (LIRR) case (244 nodes, 469 links, 78 ODs) show that the method attains sub-1% optimality gaps in minutes. In the base LIRR case, the model allows policymakers to quantify the social surplus value of a MH, or the value of enabling subsidy or regulating the microtransit operator's pricing. Comparing link-based subsidies to hub-based subsidies, the latter is computationally more expensive but offers an easier mechanism for comparison and control.
[22]
arXiv:2509.10977
(cross-list from cs.MA)
[pdf, html, other]
Title:
Statistical Model Checking of NetLogo Models
Marco Pangallo, Daniele Giachini, Andrea Vandin
Subjects:
Multiagent Systems (cs.MA); General Economics (econ.GN)
Agent-based models (ABMs) are gaining increasing traction in several domains, due to their ability to represent complex systems that are not easily expressible with classical mathematical models. This expressivity and richness come at a cost: ABMs can typically be analyzed only through simulation, making their analysis challenging. Specifically, when studying the output of ABMs, the analyst is often confronted with practical questions such as: (i) how many independent replications should be run? (ii) how many initial time steps should be discarded as a warm-up? (iii) after the warm-up, how long should the model run? (iv) what are the right parameter values? Analysts usually resort to rules of thumb and experimentation, which lack statistical rigor. This is mainly because addressing these points takes time, and analysts prefer to spend their limited time improving the model. In this paper, we propose a methodology, drawing on the field of Statistical Model Checking, to automate the process and provide guarantees of statistical rigor for ABMs written in NetLogo, one of the most popular ABM platforms. We discuss MultiVeStA, a tool that dramatically reduces the time and human intervention needed to run statistically rigorous checks on ABM outputs, and introduce its integration with NetLogo. Using two ABMs from the NetLogo library, we showcase MultiVeStA's analysis capabilities for NetLogo ABMs, as well as a novel application to statistically rigorous calibration. Our tool-chain makes it immediate to perform statistical checks with NetLogo models, promoting more rigorous and reliable analyses of ABM outputs.
[23]
arXiv:2509.11389
(cross-list from cs.LG)
[pdf, html, other]
Title:
Enhancing ML Models Interpretability for Credit Scoring
Sagi Schwartz, Qinling Wang, Fang Fang
Subjects:
Machine Learning (cs.LG); Risk Management (q-fin.RM)
Predicting default is essential for banks to ensure profitability and financial stability. While modern machine learning methods often outperform traditional regression techniques, their lack of transparency limits their use in regulated environments. Explainable artificial intelligence (XAI) has emerged as a solution in domains like credit scoring. However, most XAI research focuses on post-hoc interpretation of black-box models, which does not produce models lightweight or transparent enough to meet regulatory requirements, such as those for Internal Ratings-Based (IRB) models.
This paper proposes a hybrid approach: post-hoc interpretations of black-box models guide feature selection, followed by training glass-box models that maintain both predictive power and transparency.
Using the Lending Club dataset, we demonstrate that this approach achieves performance comparable to a benchmark black-box model while using only 10 features - an 88.5% reduction. In our example, SHapley Additive exPlanations (SHAP) is used for feature selection, eXtreme Gradient Boosting (XGBoost) serves as the benchmark and the base black-box model, and Explainable Boosting Machine (EBM) and Penalized Logistic Tree Regression (PLTR) are the investigated glass-box models.
We also show that model refinement using feature interaction analysis, correlation checks, and expert input can further enhance model interpretability and robustness.
[24]
arXiv:2509.11528
(cross-list from math.OC)
[pdf, html, other]
Title:
Dynamic Factor Models with Forward-Looking Views
Anas Abdelhakmi, Andrew E.B. Lim
Subjects:
Optimization and Control (math.OC); Portfolio Management (q-fin.PM); Risk Management (q-fin.RM)
Prediction models calibrated using historical data may forecast poorly if the dynamics of the present and future differ from observations in the past. For this reason, predictions can be improved if information like forward looking views about the state of the system are used to refine the forecast. We develop an approach for combining a dynamic factor model for risky asset prices calibrated on historical data, and noisy expert views of future values of the factors/covariates in the model, and study the implications for dynamic portfolio choice. By exploiting the graphical structure linking factors, asset prices, and views, we derive closed-form expressions for the dynamics of the factor and price processes after conditioning on the views. For linear factor models, the price process becomes a time-inhomogeneous affine process with a new covariate formed from the views. We establish a novel theoretical connection between the conditional factor process and a process we call a Mean-Reverting Bridge (MrB), an extension of the classical Brownian bridge. We derive the investor's optimal portfolio strategy and show that views influence both the myopic mean-variance term and the intertemporal hedge. The optimal dynamic portfolio when the long-run mean of the expected return is uncertain and learned online from data is also derived. More generally, our framework offers a generalizable approach for embedding forward-looking information about covariates in a dynamic factor model.
Replacement submissions (showing 15 of 15 entries)
[25]
arXiv:2305.09352
(replaced)
[pdf, other]
Title:
Health Impacts of Public Pawnshops in Industrializing Tokyo
Tatsuki Inoue
Comments:
63 pages, 7 tables, 3 figures, Appendicies
Subjects:
General Economics (econ.GN)
This study is the first to investigate whether pawnshops, financial institutions for low-income populations, have contributed to the decline in mortality in the early twentieth century. Using ward-level panel data from Tokyo City, this study revealed that the popularity of public pawnshops was associated with a 4% and 5% decrease in infant mortality and fetal death rates, respectively, during 1927-1935. The historical context implies that the potential channels of the relationships were improving nutrition and hygiene and covering childbirth costs. Moreover, a cost-effectiveness calculation highlighted that the establishment of public pawnshops was a cost-effective public investment for better public health. Contrarily, for-profit private pawnshops showed no significant association with health improvements.
[26]
arXiv:2308.04378
(replaced)
[pdf, html, other]
Title:
Mean-field control problems with multi-dimensional singular controls
Robert Denkert, Ulrich Horst
Subjects:
Mathematical Finance (q-fin.MF)
We consider extended mean-field control problems with multi-dimensional singular controls. A key challenge when analysing singular controls are jump costs. When controls are one-dimensional, jump costs are most naturally computed by linear interpolation. When the controls are multi-dimensional the situation is more complex, especially when the model parameters depend on an additional mean-field interaction term, in which case one needs to "jointly" and "consistently" interpolate jumps both on a distributional and a pathwise level. This is achieved by introducing the novel concept of two-layer parametrisations of stochastic processes. Two-layer parametrisations allow us to equivalently rewrite rewards in terms of continuous functions of parametrisations of the control process and to derive an explicit representation of rewards in terms of minimal jump costs. From this we derive a DPP for extended mean-field control problems with multi-dimensional singular controls. Under the additional assumption that the value function is continuous we characterise the value function as the minimal super-solution to a certain quasi-variational inequality in the Wasserstein space.
[27]
arXiv:2404.13818
(replaced)
[pdf, html, other]
Title:
Joint Liability Model with Adaptation to Climate Change
Jiayue Zhang, Ken Seng Tan, Tony S. Wirjanto, Lysa Porth
Subjects:
General Finance (q-fin.GN)
This paper extends the application of ESG score assessment methodologies from large corporations to individual farmers' production, within the context of climate change. Our proposal involves the integration of crucial agricultural sustainability variables into conventional personal credit evaluation frameworks, culminating in the formulation of a holistic sustainable credit rating referred to as the Environmental, Social, Economics (ESE) score. This ESE score is integrated into theoretical joint liability models, to gain valuable insights into optimal group sizes and individual-ESE score relationships. Additionally, we adopt a mean-variance utility function for farmers to effectively capture the risk associated with anticipated profits. Through a set of simulation exercises, the paper investigates the implications of incorporating ESE scores into credit evaluation systems, offering a nuanced comprehension of the repercussions under various climatic conditions.
[28]
arXiv:2411.02520
(replaced)
[pdf, html, other]
Title:
Short-maturity options on realized variance in local-stochastic volatility models
Dan Pirjol, Xiaoyu Wang, Lingjiong Zhu
Comments:
53 pages, 4 figures, 1 table
Subjects:
Pricing of Securities (q-fin.PR)
We derive the short-maturity asymptotics for prices of options on realized variance in local-stochastic volatility models. We consider separately the short-maturity asymptotics for out-of-the-money and in-the-money options cases. The analysis for the out-of-the-money case uses large deviations theory and the solution for the rate function involves solving a two-dimensional variational problem. In the special case when the Brownian noises in the asset price dynamics and the volatility process are uncorrelated, we solve this variational problem explicitly. For the correlated case, we obtain upper and lower bounds for the rate function, as well as an expansion around the at-the-money point. Numerical simulations of the prices of variance options in a local-stochastic volatility model with bounded local volatility are in good agreement with the asymptotic results for sufficiently small maturity. The leading-order asymptotics for at-the-money options on realized variance is dominated by fluctuations of the asset price around the spot value, and is computed in closed form.
[29]
arXiv:2411.18154
(replaced)
[pdf, html, other]
Title:
Semiclassical CEV Option Pricing Model: an Analytical Approach
Jose A. Capitán, Jose Lope-Alba, Juan J. Morales-Ruiz
Comments:
11 pages
Subjects:
Mathematical Finance (q-fin.MF)
This paper is devoted to obtain closed form solutions for the semiclassical (or WKB) approximation of the heat kernel propagator of the diffusion equation defined by the constant elasticity variance (CEV) option pricing model. One of the key points is that our calculations are based on the Van Vleck-Morette determinant instead of the Van Vleck determinant used by other authors. In fact, we compute this determinant in two different ways: by means of the solution of the classical Hamiltonian equations, and by solving the variational equations. Furthermore, the calculation reveals an exponential factor in the prefactor of the kernel not considered in previous works.
[30]
arXiv:2412.02065
(replaced)
[pdf, other]
Title:
Leveraging Large Language Models to Democratize Access to Costly Datasets for Academic Research
Julian Junyan Wang, Victor Xiaoqi Wang
Comments:
58 pagegs, 5 figures, 5 tables
Subjects:
General Finance (q-fin.GN); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); General Economics (econ.GN)
Unequal access to costly datasets essential for empirical research has long hindered researchers from disadvantaged institutions, limiting their ability to contribute to their fields and advance their careers. Recent breakthroughs in Large Language Models (LLMs) have the potential to democratize data access by automating data collection from unstructured sources. We develop and evaluate a novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation (RAG) framework to collect data from corporate disclosures. Our approach achieves human-level accuracy in collecting CEO pay ratios from approximately 10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000 10-K filings, with LLM processing times of 9 and 40 minutes respectively, each at a cost under US $10. This stands in stark contrast to the hundreds of hours needed for manual collection or the thousands of dollars required for commercial database subscriptions. To foster a more inclusive research community by empowering researchers with limited resources to explore new avenues of inquiry, we share our methodology and the resulting datasets.
[31]
arXiv:2501.02383
(replaced)
[pdf, html, other]
Title:
Strategic Investment to Mitigate Transition Risks
Jiayue Zhang, Tony S. Wirjanto, Lysa Porth, Ken Seng Tan
Subjects:
General Economics (econ.GN)
This paper investigates strategic investments needed to mitigate transition risks, particularly focusing on sectors significantly impacted by the shift to a low-carbon economy. It emphasizes the importance of tailored sector-specific strategies and the role of government interventions, such as carbon taxes and subsidies, in shaping corporate behavior. In providing a multi-period framework, this paper evaluates the economic and operational trade-offs companies face under four various decarbonization scenarios: immediate, quick, slow, and no transitions. The analysis provides practical insights for both policymakers and business leaders, demonstrating how regulatory frameworks and strategic investments can be aligned to manage transition risks while optimizing long-term sustainability effectively. The findings contribute to a deeper understanding of the economic impacts of regulatory policies and offer a comprehensive framework to navigate the complexities of transitioning to a low-carbon economy.
[32]
arXiv:2502.14479
(replaced)
[pdf, other]
Title:
Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework
Arno Botha, Tanja Verster, Roland Breedt
Comments:
35 pages, 9503 words, 11 figures
Subjects:
Risk Management (q-fin.RM); Statistical Finance (q-fin.ST); Applications (stat.AP)
The lifetime behaviour of loans is notoriously difficult to model, which can compromise a bank's financial reserves against future losses, if modelled poorly. Therefore, we present a data-driven comparative study amongst three techniques in modelling a series of default risk estimates over the lifetime of each loan, i.e., its term-structure. The behaviour of loans can be described using a nonstationary and time-dependent semi-Markov model, though we model its elements using a multistate regression-based approach. As such, the transition probabilities are explicitly modelled as a function of a rich set of input variables, including macroeconomic and loan-level inputs. Our modelling techniques are deliberately chosen in ascending order of complexity: 1) a Markov chain; 2) beta regression; and 3) multinomial logistic regression. Using residential mortgage data, our results show that each successive model outperforms the previous, likely as a result of greater sophistication. This finding required devising a novel suite of simple model diagnostics, which can itself be reused in assessing sampling representativeness and the performance of other modelling techniques. These contributions surely advance the current practice within banking when conducting multistate modelling. Consequently, we believe that the estimation of loss reserves will be more timeous and accurate under IFRS 9.
[33]
arXiv:2503.06646
(replaced)
[pdf, other]
Title:
Evaluating and Aligning Human Economic Risk Preferences in LLMs
Jiaxin Liu, Yixuan Tang, Yi Yang, Kar Yan Tam
Subjects:
General Economics (econ.GN); Computation and Language (cs.CL)
Large Language Models (LLMs) are increasingly used in decision-making scenarios that involve risk assessment, yet their alignment with human economic rationality remains unclear. In this study, we investigate whether LLMs exhibit risk preferences consistent with human expectations across different personas. Specifically, we assess whether LLM-generated responses reflect appropriate levels of risk aversion or risk-seeking behavior based on individual's persona. Our results reveal that while LLMs make reasonable decisions in simplified, personalized risk contexts, their performance declines in more complex economic decision-making tasks. To address this, we propose an alignment method designed to enhance LLM adherence to persona-specific risk preferences. Our approach improves the economic rationality of LLMs in risk-related applications, offering a step toward more human-aligned AI decision-making.
[34]
arXiv:2503.16974
(replaced)
[pdf, other]
Title:
Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks
Julian Junyan Wang, Victor Xiaoqi Wang
Comments:
76 pages, 20 tables, 12 figures
Subjects:
General Finance (q-fin.GN); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL); Machine Learning (cs.LG)
This study provides the first comprehensive assessment of consistency and reproducibility in Large Language Model (LLM) outputs in finance and accounting research. We evaluate how consistently LLMs produce outputs given identical inputs through extensive experimentation with 50 independent runs across five common tasks: classification, sentiment analysis, summarization, text generation, and prediction. Using three OpenAI models (GPT-3.5-turbo, GPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse financial source texts and data, covering MD&As, FOMC statements, finance news articles, earnings call transcripts, and financial statements. Our findings reveal substantial but task-dependent consistency, with binary classification and sentiment analysis achieving near-perfect reproducibility, while complex tasks show greater variability. More advanced models do not consistently demonstrate better consistency and reproducibility, with task-specific patterns emerging. LLMs significantly outperform expert human annotators in consistency and maintain high agreement even where human experts significantly disagree. We further find that simple aggregation strategies across 3-5 runs dramatically improve consistency. We also find that aggregation may come with an additional benefit of improved accuracy for sentiment analysis when using newer models. Simulation analysis reveals that despite measurable inconsistency in LLM outputs, downstream statistical inferences remain remarkably robust. These findings address concerns about what we term "G-hacking," the selective reporting of favorable outcomes from multiple generative AI runs, by demonstrating that such risks are relatively low for finance and accounting tasks.
[35]
arXiv:2507.16078
(replaced)
[pdf, other]
Title:
Automation, AI, and the Intergenerational Transmission of Knowledge
Enrique Ide
Subjects:
General Economics (econ.GN)
Recent advances in Artificial Intelligence (AI) have sparked expectations of unprecedented economic growth. Yet, by enabling senior workers to accomplish more tasks independently, AI may inadvertently reduce entry-level opportunities, raising concerns about how future generations will acquire essential expertise. This paper develops a model to examine how advanced automation affects the intergenerational transmission of tacit knowledge -- practical insights that resist codification and are critical for workplace success. The analysis shows that the competitive equilibrium features socially excessive automation of early-career tasks and reveals a critical trade-off: while such automation delivers immediate productivity gains, it can undermine long-term growth by hindering younger workers' acquisition of tacit skills. Back-of-the-envelope calculations suggest AI-driven entry-level automation could lower the long-run annual growth rate of U.S. per capita output by 0.05 to 0.35 percentage points, depending on its scale. The analysis further shows that AI co-pilots -- systems providing access to tacit-like expertise once obtained only through direct experience -- can partially offset these losses by assisting individuals who fail to develop adequate skills early in their careers. However, co-pilots are not always beneficial, as they may also weaken junior workers' incentives to engage in hands-on learning. These findings challenge the view that AI will automatically lead to higher economic growth, highlighting the need to safeguard -- or deliberately create -- entry-level opportunities to fully realize AI's potential.
[36]
arXiv:2508.10663
(replaced)
[pdf, html, other]
Title:
Higher-order Gini indices: An axiomatic approach
Xia Han, Ruodu Wang, Qinyu Wu
Subjects:
Mathematical Finance (q-fin.MF); Econometrics (econ.EM); Statistics Theory (math.ST)
Via an axiomatic approach, we characterize the family of n-th order Gini deviation, defined as the expected range over n independent draws from a distribution, to quantify joint dispersion across multiple observations. This family extends the classical Gini deviation, which relies solely on pairwise comparisons. The normalized version is called a high-order Gini coefficient. The generalized indices grow increasingly sensitive to tail inequality as n increases, offering a more nuanced view of distributional extremes. The higher-order Gini deviations admit a Choquet integral representation, inheriting the desirable properties of coherent deviation measures. Furthermore, we show that both the n-th order Gini deviation and the n-th order Gini coefficient are statistically n-observation elicitable, allowing for direct computation through empirical risk minimization. Data analysis using World Inequality Database data reveals that higher-order Gini coefficients capture disparities that the classical Gini coefficient may fail to reflect, particularly in cases of extreme income or wealth concentration.
[37]
arXiv:2509.04529
(replaced)
[pdf, other]
Title:
Analysis and Study of Smart Growth
Rongyan Chen, Ci Chen, Ziyang Yan
Comments:
Paper needs major revision for content and results
Subjects:
General Economics (econ.GN)
In the mid-1990s, the concept of smart growth emerged in the United States as a critical response to the phenomenon of suburban sprawl. To promote sustainable urban development, it is necessary to further investigate the principles and applications of smart growth. In this paper, we propose a Smart Growth Index (SGI) as a standard for measuring the degree of responsible urban development. Based on this index, we construct a comprehensive 3E evaluation model (covering economic prosperity, social equity, and environmental sustainability) to systematically assess the level of smart growth. For empirical analysis, we selected two medium-sized cities from different continents: Wuhu County, China, and Colima, Mexico. Using an improved entropy method, we evaluated the degree of smart growth in recent years and analyzed the contributions of various policies to sustainable urban development. Guided by the ten principles of smart growth, we further linked theoretical insights to practical challenges and formulated a development plan for both cities. To forecast long-term trends, we employed trend extrapolation based on historical data, enabling the prediction of SGI values for 2020, 2030, and 2050. The results indicate that Wuhu demonstrates greater potential for smart growth compared with Colima. We also simulated a scenario in which the population of both cities increased by 50 percent and re-evaluated the SGI. The analysis suggests that while rapid population growth tends to slow the pace of smart growth, it does not necessarily exert a negative impact on the overall trajectory of sustainable development. Finally, we conducted a study on the application of Transit-Oriented Development (TOD) theory in Wuhu County and proposed several policy recommendations aimed at enhancing the city's sustainable urban development.
[38]
arXiv:2211.07471
(replaced)
[pdf, html, other]
Title:
Optimal investment with insider information using Skorokhod & Russo-Vallois integration
Mauricio Elizalde, Carlos Escudero, Tomoyuki Ichiba
Journal-ref:
J Optim Theory Appl 207, 48 (2025)
Subjects:
Optimization and Control (math.OC); Probability (math.PR); Mathematical Finance (q-fin.MF); Portfolio Management (q-fin.PM)
We study the maximization of the logarithmic utility for an insider with different anticipating techniques. Our aim is to compare the utilization of Russo-Vallois forward and Skorokhod integrals in this context. Theoretical analysis and illustrative numerical examples showcase that the Skorokhod insider outperforms the forward insider. This remarkable observation stands in contrast to the scenario involving risk-neutral traders. Furthermore, an ordinary trader could surpass both insiders if a significant negative fluctuation in the driving stochastic process leads to a sufficiently negative final value. These findings underline the intricate interplay between anticipating stochastic calculus and nonlinear utilities, which may yield non-intuitive results from the financial viewpoint.
[39]
arXiv:2404.16169
(replaced)
[pdf, html, other]
Title:
Interpretable Machine Learning Models for Predicting the Next Targets of Activist Funds
Minwu Kim, Sidahmed Benabderrahmane, Talal Rahwan
Comments:
30 pages
Subjects:
Computational Engineering, Finance, and Science (cs.CE); Statistical Finance (q-fin.ST)
This research presents a predictive model to identify potential targets of activist investment funds--entities that acquire significant corporate stakes to influence strategic and operational decisions, ultimately enhancing shareholder value. Predicting such targets is crucial for companies aiming to mitigate intervention risks, activist funds seeking optimal investments, and investors looking to leverage potential stock price gains. Using data from the Russell 3000 index from 2016 to 2022, we evaluated 123 model configurations incorporating diverse imputation, oversampling, and machine learning techniques. Our best model achieved an AUC-ROC of 0.782, demonstrating its capability to effectively predict activist fund targets. To enhance interpretability, we employed the Shapley value method to identify key factors influencing a company's likelihood of being targeted, highlighting the dynamic mechanisms underlying activist fund target selection. These insights offer a powerful tool for proactive corporate governance and informed investment strategies, advancing understanding of the mechanisms driving activist investment decisions.
Total of 39 entries
Showing up to 2000 entries per page:
fewer
|
more
|
all
About
Help
contact arXivClick here to contact arXiv
Contact
subscribe to arXiv mailingsClick here to subscribe
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance
arXiv Operational Status
Get status notifications via
email
or slack