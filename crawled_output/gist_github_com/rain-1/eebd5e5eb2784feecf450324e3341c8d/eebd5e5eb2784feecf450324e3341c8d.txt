LLM Introduction: Learn Language Models · GitHub
Skip to content
Search Gists
Search Gists
All gists
Back to GitHub
Sign in
Sign up
Sign in
Sign up
You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.
Dismiss alert
Instantly share code, notes, and snippets.
rain-1/LLM.md
Last active
September 17, 2025 17:15
Show Gist options
Download ZIP
Star
1,652
(1,652)
You must be signed in to star a gist
Fork
161
(161)
You must be signed in to fork a gist
Embed
Embed
Embed this gist in your website.
Share
Copy sharable link for this gist.
Clone via HTTPS
Clone using the web URL.
Learn more about clone URLs
Clone this repository at &lt;script src=&quot;https://gist.github.com/rain-1/eebd5e5eb2784feecf450324e3341c8d.js&quot;&gt;&lt;/script&gt;
Save rain-1/eebd5e5eb2784feecf450324e3341c8d to your computer and use it in GitHub Desktop.
Code
Revisions
68
Stars
1,651
Forks
161
Embed
Embed
Embed this gist in your website.
Share
Copy sharable link for this gist.
Clone via HTTPS
Clone using the web URL.
Learn more about clone URLs
Clone this repository at &lt;script src=&quot;https://gist.github.com/rain-1/eebd5e5eb2784feecf450324e3341c8d.js&quot;&gt;&lt;/script&gt;
Save rain-1/eebd5e5eb2784feecf450324e3341c8d to your computer and use it in GitHub Desktop.
Download ZIP
LLM Introduction: Learn Language Models
Raw
LLM.md
Purpose
Bootstrap knowledge of LLMs ASAP. With a bias/focus to GPT.
Avoid being a link dump. Try to provide only valuable well tuned information.
Prelude
Neural network links before starting with transformers.
https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi
https://www.3blue1brown.com/topics/neural-networks
http://neuralnetworksanddeeplearning.com/
Key
🟢 = easy, 🟠 = medium, 🔴 = hard
🕰️ = long, 🙉 = low quality audio
Youtube Lessons
🟢🕰️ Łukasz Kaiser Attention is all you need; Attentional Neural Network Models This talk is from 6 years ago.
🟢🕰️ Andrej Karpathy The spelled-out intro to language modeling: building makemore: basic. bi-gram name generator model by counting, then by NN. using pytorch.
🟢🕰️ Andrej Karpathy Building makemore Part 2: MLP:
🕰️ Andrej Karpathy Building makemore Part 3: Activations & Gradients, BatchNorm):
🕰️ Andrej Karpathy Building makemore Part 4: Becoming a Backprop Ninja:
🟢 Andrej Karpathy State of GPT
🟢 Hedu AI Visual Guide to Transformer Neural Networks - (Episode 1) Position Embeddings: Tokens are embedded into a semantic space. sine/cosine position encoding explained very well.
🟢 Hedu AI Visual Guide to Transformer Neural Networks - (Episode 2) Multi-Head & Self-Attention: Clear overview of multi-head attention.
🟢 Hedu AI Visual Guide to Transformer Neural Networks - (Episode 3) Decoder’s Masked Attention: Further details on the transformer architecture.
🟠🕰️ Andrej Karpathy Andrej Karpathy - Let's build GPT: from scratch, in code, spelled out.: build up a Shakespeare gpt-2-like from scratch. starts with bi-gram and adds features one by one. pytorch.
🔴🕰️ Chris Olah CS25 I Stanford Seminar - Transformer Circuits, Induction Heads, In-Context Learning: Interpretation. Deep look into the mechanics of induction heads. Companion article
🟢 Jay Alammar The Illustrated Word2vec - A Gentle Intro to Word Embeddings in Machine Learning
🟢 Jay Alammar How GPT3 Works - Easily Explained with Animations: extremely high level basic overview.
🟢🕰️ Jay Alammar The Narrated Transformer Language Model: much deeper look at the architecture. goes into detail. Companion article.
🔥 Sebastian Raschka L19: Self-attention and transformer networks Academic style lecture series on self-attention transformers
🟢🕰️🙉 Mark Chen Transformers in Language: The development of GPT Models including GPT3 A chunk of this lecture is about applying GPT to images. Same lecture series as the Chris Olah one. Rest of the series. Papers listed in the talk:
"GPT-1": Liu et. al. Generating Wikipedia by Summarizing Long Sequences
"GPT-2": Radford et. al. Language Models are Unsupervised Multitask Learners github.com/openai/gpt-2 OpenAI: Better Language Models Fermats Library
"GPT-3": Brown et. al. Language Models are Few-Shot Learners (I think this is it, can't find the quoted text inside this paper)
🟠🕰️ Future Mojo NLP Demystified 15: Transformers From Scratch + Pre-training and Transfer Learning With BERT/GPT: Crystal clear explanation of every single detail of the transformer. Very well paced and easy to follow. Has tensorflow code. This is the culmination of a full NLP course, all of which is excellent.
Articles
🟢 Viktor Garske Transformer Models Timeline and List family tree
🟢 Jakob Uszkoreit Transformer: A Novel Neural Network Architecture for Language Understanding Google introduces the transformer model in a simple easy to understand blog post. This is in the context of translation.
🟠 Jay Mody GPT in 60 Lines of NumPy
🟠 PyTorch Language Modeling with nn.Transformer and TorchText
🟠 Sasha Rush et. al. The Annotated Transformer
🟢 Jay Alammar The Illustrated Transformer companion video above.
🟠 Jay Alammar The Illustrated GPT-2 (Visualizing Transformer Language Models)
🟢 Jay Alammar How GPT3 Works - Visualizations and Animations
🔥 Chris Olah et. al. In-context Learning and Induction Heads companion video lecture above
🟢 Finbarr Timbers Five years of GPT progress
🟠 Sebastian Raschka Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch
🟢 Jason Wei 137 emergent abilities of large language models - Includes a good list of advanced prompting strategies.
🟢 Jay Alammar and Cohere LLM University
Jean Nyandwi The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture
🔥 Eugene Yan Patterns for Building LLM-based Systems & Products
Very in-depth article on practical engineering concepts that will be useful to build software that uses an LLM as a component.
🟠 Finbarr Timbers Five years of GPT progress - Excellent technical overview of LLMs from GPT onwards.
Research Paper Lists
Sebastian Raschka Understanding Large Language Models -- A Transformative Reading List This article lists some of the most important papers in the area. This is a really good chronological list of papers.
OpenAI Research Index
Research Papers
1️⃣ (GPT-1) Radford et. al. Improving Language Understanding by Generative Pre-Training (2018) a page accompanying this paper on the OpenAI blog Improving language understanding with unsupervised learning. Source code (tidied up by thomwolf) here: huggingface.co/.../openai-gpt
2️⃣ (GPT-2) Radford et. al. Language Models are Unsupervised Multitask Learners (2019) accompanying OpenAI blog Improving language understanding with unsupervised learning. Source code here: github.com/openai/gpt-2
3️⃣ (GPT-3) Brown et. al. Language Models are Few-Shot Learners
Kaplan et. al. Scaling Laws for Neural Language Models A variety of models were trained using varying amounts of compute, data set size, and number of parameters. This enables us to predict what parameters will work well in larger future models. See also Gwern Branwen The Scaling Hypothesis
Mary Phuong et. al. Formal Algorithms for Transformers This paper gives pseudocode for various versions of the transformer (with array indexes starting at 1 for some reason). Very useful reference to have.
Philosophy of GPT
Isaac Asimov The Last Question (1956)
Justin Weinberg, Daily Nous Philosophers On GPT-3
Fernando Borretti And Yet It Understands
Ted Chiang ChatGPT Is a Blurry JPEG of the Web
Noam Chomsky The False Promise of ChatGPT
Janus Simulators This is a long post but the main point you can take from it is that LLMs act as simulators that can create many different personas to generate text. Related, easier to read and understand Janus' Simulators
Julian Togelius Is Elden Ring an existential risk to humanity? Satire. This leads into a critique of the concept of intelligence.
Josh Whiton From AI to A-Psy About Bing Sydney's reaction to prompt injection.
Usage
Chip Huyen Building LLM applications for production How to get good results from actually using an LLM.
GPT/LLM Link Collections
https://github.com/sw-yx/ai-notes/tree/main - lots of articles and podcasts
https://github.com/giuven95/chatgpt-failures - large list of examples of things it gets/got wrong
Random fun/interesting Applications
https://github.com/PrefectHQ/marvin - implement entire python functions just by describing them in a comment
https://github.com/pgosar/ChatGDB - GDB debugger commands using natural language
https://github.com/TheR1D/shell_gpt - type things like "list files" instead of "ls"
https://github.com/RomanHotsiy/commitgpt - create git commit messages
https://github.com/densmirnov/git2gpt/commits/main - create git commits from repo + prompts, mutating a codebase over time
https://www.chatpdf.com/ - Upload a PDF and discuss it.
https://www.debate-devil.com/en - devils advocate debate game
https://micahflee.com/2023/04/capturing-the-flag-with-gpt-4/ - cheating at a CTF
https://www.thisworddoesnotexist.com/ - makes up words
https://ggpt.43z.one/ - prompt injection golfing game
https://gandalf.lakera.ai/ - another prompt injection game
https://github.com/AdmTal/chat-gpt-games - conversations that are games!
ConLang + Ancient scripts stuff
Dylan Black I Taught ChatGPT to Invent a Language Gloop splog slopa slurpi
Ryszard Szopa Teaching ChatGPT to Speak my Son’s Invented Language hingadaa’ng’khuu’ngkilja’khłattama’khattama
https://medium.com/syncedreview/ai-deciphers-persian-cuneiform-tablets-from-25-centuries-ago-afc69af3f244
Controlling output
https://www.reddit.com/r/LocalLLaMA/comments/13j3747/tutorial_a_simple_way_to_get_rid_of_as_an_ai/
https://matt-rickard.com/rellm + https://matt-rickard.com/context-free-grammar-parsing-with-llms
https://github.com/microsoft/guidance
Prompt Injection
Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection https://arxiv.org/abs/2302.12173
https://llm-attacks.org/
https://poison-llm.github.io/
(Local) Model Comparisons and Rankings
If you are wondering which models are best, especially for comparing local models
https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
https://leaderboard.lmsys.org/
This page is not finished yet. I will continue adding to this.
Raw
zzz-disclosure.md
pick an emoji to represent each concept
easy
medium
hard
short
long
dont use faces
dont use the arrows for lengths
dont use the same emoji for two different things
[response lost, it suggested colored circles which was what I wanted for difficulty anyway]
pick emoji to represent concept
short =
long =
Short = ⏱️
Long = 🕰️
Copy link
whitleyhorn
commented
Mar 30, 2023
🔥
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
neoromantique
commented
Mar 30, 2023
🔥
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
hamzakat
commented
Mar 30, 2023
how should we use the materials on https://distill.pub/ ?
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
Manubi
commented
Mar 31, 2023
awesome :D
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
jcmaciel
commented
Mar 31, 2023
Wowww! Awesome, thanks for sharing!
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
SimenCTan
commented
Apr 18, 2023
👍
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
JTarakRam
commented
Jun 11, 2023
Really helpful, Thanks for Sharing !!
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
briantliao
commented
Sep 23, 2023
🔥
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
dsdanielpark
commented
Oct 4, 2023
🔥🔥
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
IvanPopJovanov
commented
Oct 17, 2023
Does anyone know of any similar compilation of resources for diffusion models?
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
choronX
commented
Jan 28, 2024
Please add to this PDFChat.. Running 2-3 RAG pipelines and building more for documents QnA.
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
phatdangminh
commented
Mar 31, 2024
Nice thanks
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
itoonx
commented
Mar 17, 2025
Thanks, That very nice
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Copy link
wd021
commented
Jul 8, 2025
•
edited
Loading
Uh oh!
There was an error while loading. Please reload this page.
www.godtierprompts.com for prompt resources
Sorry, something went wrong.
Uh oh!
There was an error while loading. Please reload this page.
Sign up for free
to join this conversation on GitHub.
Already have an account?
Sign in to comment
Footer
© 2025 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Community
Docs
Contact
Manage cookies
Do not share my personal information
You can’t perform that action at this time.