SpikingBrain 7B – More efficient than classic LLMs | Hacker NewsHacker Newsnew | past | comments | ask | show | jobs | submitloginSpikingBrain 7B – More efficient than classic LLMs (github.com/biclab)30 points by somethingsome 5 hours ago
| hide | past | favorite | 8 comments
augment_me 29 minutes ago
| next [–]
To me it sounds like sparse matrix multiplication repackaged as "event-driven spiking computation", where the spikes are simply the non-zero elements that sparse GPU kernels have always been designed to process.The supposedly dynamic/temporal nature of the model seems to be not applied for GPU execution, collapsing it into a single static computation equivalent to just applying a pre-calculated sparsity mask.Perhaps a bit cynical of me, but it feels like wrapping standard sparse computing and operator fusion in complex, biological jargon...replyGregarianChild 16 minutes ago
| parent | next [–]
The 'brain-inspired' community has always been doing this, since Carver Mead introduced the term 'neuromorphic' in the late 1980s. Reselling banalities as a new great insight. My favourite is
"Neuromorphic computing breakthrough could enable blockchain on Mars" [1]. What else can they do? After all, that community has now multiple decades of failure under it's belt. Not a single success. Failure to make progress in AI and failure to say anything in interest about the brain. To paraphrase a US president: In this world nothing can be said to be certain, except death, taxes and neuromphicists exaggerating. (Aside: I was told by someone who applied to YC with a 'neuromorphic' startup that YC said, they don't fund 'neuromorphic'. I am not sure about details ...). The whole 'brain talk' malarkey goes back way longer.
In particular psychology and related subjects,
since their origins as a specialty in the 19th century, have heavily used brain-inspired metaphors that were intended to mislead. Already in the 19th century that was criticised. See [3] for an interesting discussion.There is something interesting in this post, namely that it's based on non-Nvidia GPUs, in this case MetaX [2]. I don't know how competitive MetaX are today, but I would not bet against China in the longer term.[1] https://cointelegraph.com/news/neuromorphic-computing-breakt...[2] https://en.wikipedia.org/wiki/MetaX[3] K. S. Kendler, A history of metaphorical brain talk in psychiatry. https://www.nature.com/articles/s41380-025-03053-6replycpldcpu 1 hour ago
| prev | next [–]
>The current implementation adopts pseudo-spiking, where activations are approximated as spike-like signals at the tensor level, rather than true asynchronous event-driven spiking on neuromorphic hardware.Isn't that in essence very similar to Quantization Aware Training (QaT)?replyspwa4 1 hour ago
| parent | next [–]
Can you explain more? Why would that be the case? What is being passed from one layer to the next is not a linear value but the delay until the next spike, which is very different.replyasdfasdf1 2 hours ago
| prev | next [–]
SpikingBrain Technical Report: Spiking Brain-inspired Large Models
https://arxiv.org/abs/2509.05276replybob1029 1 hour ago
| prev | next [–]
https://news.ycombinator.com/item?id=45206420replycpldcpu 1 hour ago
| parent | next [–]
Well, it would still allow to deploy the trained model to SNN hardware, if it existed.replyimtringued 20 minutes ago
| prev [–]
In a few years China will be completely independent from Nvidia.https://en.wikipedia.org/wiki/MetaXThey have GPU manufacturers that nobody in the west has ever heard of.reply
Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search: