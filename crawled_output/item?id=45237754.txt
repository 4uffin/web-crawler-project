SpikingBrain 7B – More efficient than classic LLMs | Hacker NewsHacker Newsnew | past | comments | ask | show | jobs | submitloginSpikingBrain 7B – More efficient than classic LLMs (github.com/biclab)12 points by somethingsome 4 hours ago
| hide | past | favorite | 5 comments
cpldcpu 54 minutes ago
| next [–]
>The current implementation adopts pseudo-spiking, where activations are approximated as spike-like signals at the tensor level, rather than true asynchronous event-driven spiking on neuromorphic hardware.Isn't that in essence very similar to Quantization Aware Training (QaT)?replyspwa4 6 minutes ago
| parent | next [–]
Can you explain more? Why would that be the case? What is being passed from one layer to the next is not a linear value but the delay until the next spike, which is very different.replyasdfasdf1 1 hour ago
| prev | next [–]
SpikingBrain Technical Report: Spiking Brain-inspired Large Models
https://arxiv.org/abs/2509.05276replybob1029 39 minutes ago
| prev [–]
https://news.ycombinator.com/item?id=45206420replycpldcpu 20 minutes ago
| parent [–]
Well, it would still allow to deploy the trained model to SNN hardware, if it existed.reply
Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact
Search: