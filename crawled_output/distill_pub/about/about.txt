Distill is dedicated to making machine learning clear and dynamic
Distill
Distill
About
Prize
Submit
Machine Learning Research
Should Be Clear, Dynamic and Vivid.
Distill Is Here to Help.
A Journal
Devoted to clear explanations, native to the Web.
$10,000 Prizes
For outstanding work communicating and refining ideas.
Distill was a scientific journal which operated 2016-2021.
We are now on an indefinite hiatus.
A modern medium for presenting research
The web is a powerful medium to share new ways of thinking. Over the last few years we’ve seen many imaginative examples of such work. But traditional academic publishing remains focused on the PDF, which prevents this sort of communication.
Reactive diagrams allow for a type of communication not possible in static mediums. Hover over this diagram to see how a neural turing machine shifts its attention over its old memory values to create new values.
New ways of thinking enable new discoveries
New notations, visualizations, and mental models can deepen our understanding. By nurturing the development of such new ways of thinking, Distill will enable new discoveries.
An interactive playground for t-SNE dimensionality reduction helps readers develop an intuition for technique and where it is best applied.
Machine learning needs more transparency
Machine learning will fundamentally change how humans and computers interact.
It’s important to make those techniques transparent, so we can understand and safely control how they work. Distill will provide a platform for vividly illustrating these ideas.
An inspectable RNN handwriting model allows readers to feed in examples and see activations in real time.
Legitimacy for non-traditional research artifacts
Many researchers want to do work that is not easily contained within a PDF, but can’t get the support they need to pursue it. We, as the research community, are failing them by not treating these wonderful, non-traditional research artifacts as “real” academic contributions.
Non-traditional contributions often don’t get credit unless authors wrap them with proxy papers. Unfortunately, this multiplies effort and divides attention.
Clear writing benefits everyone
When we rush papers out the door to meet conference deadlines, something suffers — often it is the readability and clarity of our communication. This can add severe drag to the entire community as our readers struggle to understand our ideas. We think this ”research debt″ can be avoided.
The people behind Distill
Editors
Shan Carter
ObservableHQ
Chris Olah
Steering Committee
Yoshua Bengio
Université de Montréal
Mike Bostock
ObservableHQ
Amanda Cox
The New York Times
Ian Goodfellow
Apple
Andrej Karpathy
Tesla
Shakir Mohamed
DeepMind
Michael Nielsen
Fernanda Viégas
Google Big Picture Group
Staff
Ludwig Schubert
Nick Cammarata
OpenAI
Janelle Tam
YCombinator
Distill
is dedicated to clear explanations of machine learning
About
Submit
Prize
Archive
RSS
GitHub
Twitter
ISSN 2476-0757