Multimodal Neurons in Artificial Neural Networks
Distill
About
Prize
Submit
Multimodal Neurons in Artificial Neural Networks
Authors
Affiliations
Gabriel Goh
OpenAI
Nick Cammarata †
OpenAI
Chelsea Voss †
OpenAI
Shan Carter
Observable
Michael Petrov
OpenAI
Ludwig Schubert
Alec Radford
OpenAI
Chris Olah
Published
March 4, 2021
DOI
10.23915/distill.00030
Acknowledgments
We are deeply grateful to Sandhini Agarwal, Daniela Amodei, Dario Amodei,
Tom Brown, Jeff Clune, Steve Dowling, Gretchen Krueger, Brice Menard,
Reiichiro Nakano, Aditya Ramesh, Pranav Shyam, Ilya Sutskever and Martin
Wattenberg.
Author Contributions
Gabriel Goh: Research lead. Gabriel Goh first discovered multimodal
neurons, sketched out the project direction and paper outline, and did
much of the conceptual and engineering work that allowed the team to
investigate the models in a scalable way. This included developing tools
for understanding how concepts were built up and decomposed (that were
applied to emotion neurons), developing zero-shot neuron search (that
allowed easy discoverability of neurons), and working with Michael Petrov
on porting CLIP to microscope. Subsequently developed faceted feature
visualization, and text feature visualization.
Chris Olah: Worked with Gabe on the overall framing of the article,
actively mentored each member of the team through their work providing
both high and low level contributions to their sections, and contributed
to the text of much of the article, setting the stylistic tone. He worked
with Gabe on understanding the neuroscience literature and better
understanding the relevant neuroscience literature. Additionally, he wrote
the sections on region neurons and developed diversity feature
visualization which Gabe used to create faceted feature visualization
Alec Radford: Developed CLIP. First observed that CLIP was learning
to read. Advised Gabriel Goh on project direction on a weekly basis. Upon
the discovery that CLIP was using text to classify images, proposed
typographical adversarial attacks as a promising research direction.
Shan Carter: Worked on initial investigation of CLIP with Gabriel
Goh. Did multimodal activation atlases to understand the space of
multimodal representations and geometry, and neuron atlases, which
potentially helped the arrangement and display of neurons. Provided much
useful advice on the visual presentation of ideas, and helped with many
aspects of visual design.
Michael Petrov: Worked on the initial investigation of multimodal
neurons by implementing and scaling dataset examples. Discovered, with
Gabriel Goh, the original “Spider-Man” multimodal neuron in the dataset
examples, and many more multimodal neurons. Assisted a lot in the
engineering of Microscope both early on, and at the end, including helping
Gabriel Goh with the difficult technical challenges of porting microscope
to a different backend.
Chelsea Voss†: Performed investigation of the typographical attacks
phenomena, both via linear probes and zero-shot, confirming that the
attacks were indeed real and state of the art. Proposed and successfully
found “in-the-wild” attacks in the zero-shot classifier. Subsequently
wrote the section “typographical attacks”. Upon completion of this part of
the project, investigated responses of neurons to rendered text on
dictionary words. Also assisted with the organization of neurons into
neuron cards.
Nick Cammarata†: Drew the connection between multimodal neurons in
neural networks and multimodal neurons in the brain, which became the
overall framing of the article. Created the conditional probability plots
(regional, Trump, mental health), labeling more than 1500 images,
discovered that negative pre-ReLU activations are often interpretable, and
discovered that neurons sometimes contain a distinct regime change between
medium and strong activations. Wrote the identity section and the emotion
sections, building off Gabriel’s discovery of emotion neurons and
discovering that “complex” emotions can be broken down into simpler ones.
Edited the overall text of the article and built infrastructure allowing
the team to collaborate in Markdown with embeddable components.
Ludwig Schubert: Helped with general infrastructure.
† equal contributors
Discussion and Review
Review 1 - Anonymous
Review 2 - Anonymous
Review 3 - Anonymous
ReferencesInvariant visual representation by single neurons in the human brain
[PDF]Quiroga, R.Q., Reddy, L., Kreiman, G., Koch, C. and Fried, I., 2005. Nature, Vol 435(7045), pp. 1102--1107. Nature Publishing Group.Explicit encoding of multimodal percepts by single neurons in the human brain Quiroga, R.Q., Kraskov, A., Koch, C. and Fried, I., 2009. Current Biology, Vol 19(15), pp. 1308--1313. Elsevier.Learning Transferable Visual Models From Natural Language Supervision
[link]Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G. and Sutskever, I., 2021. Deep Residual Learning for Image Recognition
[PDF]He, K., Zhang, X., Ren, S. and Sun, J., 2015. CoRR, Vol abs/1512.03385. Attention is all you need Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I., 2017. Advances in neural information processing systems, pp. 5998--6008. Improved deep metric learning with multi-class n-pair loss objective Sohn, K., 2016. Advances in neural information processing systems, pp. 1857--1865. Contrastive multiview coding Tian, Y., Krishnan, D. and Isola, P., 2019. arXiv preprint arXiv:1906.05849. Linear algebraic structure of word senses, with applications to polysemy Arora, S., Li, Y., Liang, Y., Ma, T. and Risteski, A., 2018. Transactions of the Association for Computational Linguistics, Vol 6, pp. 483--495. MIT Press.Visualizing and understanding recurrent networks
[PDF]Karpathy, A., Johnson, J. and Fei-Fei, L., 2015. arXiv preprint arXiv:1506.02078. Object detectors emerge in deep scene cnns
[PDF]Zhou, B., Khosla, A., Lapedriza, A., Oliva, A. and Torralba, A., 2014. arXiv preprint arXiv:1412.6856. Network Dissection: Quantifying Interpretability of Deep Visual Representations
[PDF]Bau, D., Zhou, B., Khosla, A., Oliva, A. and Torralba, A., 2017. Computer Vision and Pattern Recognition. Zoom In: An Introduction to Circuits Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M. and Carter, S., 2020. Distill, Vol 5(3), pp. e00024--001. Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks
[PDF]Nguyen, A., Yosinski, J. and Clune, J., 2016. arXiv preprint arXiv:1602.03616. Sparse but not ‘grandmother-cell’ coding in the medial temporal lobe Quiroga, R.Q., Kreiman, G., Koch, C. and Fried, I., 2008. Trends in cognitive sciences, Vol 12(3), pp. 87--91. Elsevier.Concept cells: the building blocks of declarative memory functions Quiroga, R.Q., 2012. Nature Reviews Neuroscience, Vol 13(8), pp. 587--597. Nature Publishing Group.Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements Barrett, L.F., Adolphs, R., Marsella, S., Martinez, A.M. and Pollak, S.D., 2019. Psychological science in the public interest, Vol 20(1), pp. 1--68. Sage Publications Sage CA: Los Angeles, CA.Geographical evaluation of word embeddings
[PDF]Konkol, M., Brychc{\'\i}n, T., Nykl, M. and Hercig, T., 2017. Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 224--232. Using Artificial Intelligence to Augment Human Intelligence
[link]Carter, S. and Nielsen, M., 2017. Distill.
DOI: 10.23915/distill.00009Visualizing Representations: Deep Learning and Human Beings
[link]Olah, C., 2015. Natural language processing (almost) from scratch Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K. and Kuksa, P., 2011. Journal of machine learning research, Vol 12(ARTICLE), pp. 2493--2537. Linguistic regularities in continuous space word representations Mikolov, T., Yih, W. and Zweig, G., 2013. Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pp. 746--751. Man is to computer programmer as woman is to homemaker? debiasing word embeddings Bolukbasi, T., Chang, K., Zou, J.Y., Saligrama, V. and Kalai, A.T., 2016. Advances in neural information processing systems, pp. 4349--4357. Intriguing properties of neural networks
[PDF]Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and Fergus, R., 2013. arXiv preprint arXiv:1312.6199. Visualizing higher-layer features of a deep network
[PDF]Erhan, D., Bengio, Y., Courville, A. and Vincent, P., 2009. University of Montreal, Vol 1341, pp. 3. Feature Visualization
[link]Olah, C., Mordvintsev, A. and Schubert, L., 2017. Distill.
DOI: 10.23915/distill.00007How does the brain solve visual object recognition? DiCarlo, J.J., Zoccolan, D. and Rust, N.C., 2012. Neuron, Vol 73(3), pp. 415--434. Elsevier.Imagenet: A large-scale hierarchical image database Deng, J., Dong, W., Socher, R., Li, L., Li, K. and Fei-Fei, L., 2009. 2009 IEEE conference on computer vision and pattern recognition, pp. 248--255. BREEDS: Benchmarks for Subpopulation Shift Santurkar, S., Tsipras, D. and Madry, A., 2020. arXiv preprint arXiv:2008.04859. Global Weighted Average Pooling Bridges Pixel-level Localization and Image-level Classification
[PDF]Qiu, S., 2018. CoRR, Vol abs/1809.08264. Separating style and content with bilinear models Tenenbaum, J.B. and Freeman, W.T., 2000. Neural computation, Vol 12(6), pp. 1247--1283. MIT Press.The feeling wheel: A tool for expanding awareness of emotions and increasing spontaneity and intimacy Willcox, G., 1982. Transactional Analysis Journal, Vol 12(4), pp. 274--276. SAGE Publications Sage CA: Los Angeles, CA.Activation atlas Carter, S., Armstrong, Z., Schubert, L., Johnson, I. and Olah, C., 2019. Distill, Vol 4(3), pp. e15. Adversarial Patch
[PDF]Brown, T., Mané, D., Roy, A., Abadi, M. and Gilmer, J., 2017. arXiv preprint arXiv:1712.09665. Synthesizing Robust Adversarial Examples
[PDF]Athalye, A., Engstrom, L., Ilyas, A. and Kwok, K., 2017. arXiv preprint arXiv:1707.07397. Studies of interference in serial verbal reactions. Stroop, J.R., 1935. Journal of experimental psychology, Vol 18(6), pp. 643. Psychological Review Company.Curve Detectors Cammarata, N., Goh, G., Carter, S., Schubert, L., Petrov, M. and Olah, C., 2020. Distill, Vol 5(6), pp. e00024--003. An overview of early vision in inceptionv1 Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M. and Carter, S., 2020. Distill, Vol 5(4), pp. e00024--002. Deep inside convolutional networks: Visualising image classification models and saliency maps
[PDF]Simonyan, K., Vedaldi, A. and Zisserman, A., 2013. arXiv preprint arXiv:1312.6034. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images
[PDF]Nguyen, A., Yosinski, J. and Clune, J., 2015. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427--436.
DOI: 10.1109/cvpr.2015.7298640Inceptionism: Going deeper into neural networks
[HTML]Mordvintsev, A., Olah, C. and Tyka, M., 2015. Google Research Blog. Plug & play generative networks: Conditional iterative generation of images in latent space
[PDF]Nguyen, A., Clune, J., Bengio, Y., Dosovitskiy, A. and Yosinski, J., 2016. arXiv preprint arXiv:1612.00005. Sun database: Large-scale scene recognition from abbey to zoo Xiao, J., Hays, J., Ehinger, K.A., Oliva, A. and Torralba, A., 2010. 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 3485--3492. The pascal visual object classes (voc) challenge Everingham, M., Van Gool, L., Williams, C.K., Winn, J. and Zisserman, A., 2010. International journal of computer vision, Vol 88(2), pp. 303--338. Springer.Fairface: Face attribute dataset for balanced race, gender, and age Kärkkäinen, K. and Joo, J., 2019. arXiv preprint arXiv:1908.04913. A style-based generator architecture for generative adversarial networks Karras, T., Laine, S. and Aila, T., 2019. Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4401--4410.
Updates and Corrections
If you see mistakes or want to suggest changes, please create an issue on GitHub.
Reuse
Diagrams and text are licensed under Creative Commons Attribution CC-BY 4.0 with the source available on GitHub, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”.
Citation
For attribution in academic contexts, please cite this work as
Goh, et al., "Multimodal Neurons in Artificial Neural Networks", Distill, 2021.
BibTeX citation
@article{goh2021multimodal,
author = {Goh, Gabriel and †, Nick Cammarata and †, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
title = {Multimodal Neurons in Artificial Neural Networks},
journal = {Distill},
year = {2021},
note = {https://distill.pub/2021/multimodal-neurons},
doi = {10.23915/distill.00030}
}
Distill
is dedicated to clear explanations of machine learning
About
Submit
Prize
Archive
RSS
GitHub
Twitter
ISSN 2476-0757